####################################################### arnold al final
library(igraph)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
setwd("D:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")

network1 <-'base'
network1 <- 'ieee39 line.txt'
network1 <- 'ieee57 line.txt'
network1 <- 'ieee39 line.txt'
network1 <- 'it_bas.txt'
#network1 <- 'ieee118.txt'
#network1 <- 'power494.txt'
#network1 <- 'Germany.txt'
#network1 <- 'mafia1.txt'
#network1 <- 'jazz.txt'
#network1 <- 'Spain.txt'
#network1 <- 'it_bas.txt'
#network1 <- 'SENTRONCAL_mod2.txt'
#network1 <- 'euroroad.txt'
#network1 <- 'usaairpotx.txt'
network1 <- 'ieee57 line.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'it_bas.txt'
network1 <- 'ieee118.txt'
network1 <- 'Germany.txt'
#network1 <- "Crime_Gcc1.txt"
network1 <- "barcelona.txt"
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'Germany.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- "barcelona.txt"
network1 <- 'it_bas.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'Germany.txt'
network1 <- 'mafia1.txt'
network1 <- 'Germany.txt'

ff<-scan(network1, what=list(0,0,0))

# Define network data
f1<-ff[[1]] #Nodo desde
f2<-ff[[2]] #Nodo hast
f3<-ff[[3]] #reactance
g <-graph(t(cbind(f1,f2)), directed=FALSE)
#if(network1 == 'mafia1.txt'){
g <- simplify(delete_vertices(g, which(components(g)$membership!=which.max(components(g)$csize))))
#}

# Load graph and layout
##########
#g <- make_graph("Zachary")
#network1 <- 'Zachary'


set.seed(42)
layout_orig <- layout_with_kk(g) #layout_with_fr(g)

#g <- make_graph("Zachary")
#original_coords <- layout_orig #layout_with_kk(g)

windows()

set.seed(42)
plot(g,layout=layout_orig,main="base",vertex.size=1)
jpeg(filename= paste(network1,"base.jpeg"), units = "px", res= 600, height= 3000, width= 3000) 

 plot_network(G1, tempo1[nrow(tempo1),], layout_G1, paste("G1 at end",keval))
if(jpeg==1)dev.off()

#layout_norm <- layout_orig / max(layout_orig)
layout_norm <- (layout_orig- min(layout_orig))/ (max(layout_orig)-min(layout_orig))
original_coords <- layout_norm  
windows()
coords_orig <-layout_norm 

plot(g,layout=layout_orig,main="base norm",vertex.size=1)

jpeg(filename= paste(network1,"base.jpeg"), units = "px", res= 600, height= 3000, width= 3000) 
plot(g,layout=layout_orig,vertex.size=1)
dev.off()

center_orig <- colMeans(layout_norm)
dist_center_orig <- sqrt(rowSums((layout_norm - center_orig)^2))

noise= 0.1
angle = pi / 3
 scale = 1.3
shift = 1

# Compute topological features
features <- list(
  Degree = degree(g),
  Betweenness = betweenness(g),
  Closeness = closeness(g),
  Eigenvector = eigen_centrality(g)$vector
)

# Arnold Map functions
arnold_map <- function(x, y, mod = 1) {
  x_new <- (x + y) %% mod
  y_new <- (x + 2 * y) %% mod
  return(c(x_new, y_new))
}

apply_arnold <- function(coords, iterations = 1, mod = 1) {
  coords_out <- coords
  for (i in 1:iterations) {
    coords_out <- t(apply(coords_out, 1, function(p) arnold_map(p[1], p[2], mod)))
  }
  return(coords_out)
}

# Other distortions
apply_jitter <- function(coords, noise = noise) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}


apply_jitter <- function(coords) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}

apply_affine <- function(coords, angle =angle, scale = scale) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

apply_affine <- function(coords) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

apply_circular_swap <- function(coords) {
  center <- colMeans(coords)
  radius <- sqrt(rowSums((coords - center)^2))
  angle <- atan2(coords[,2] - center[2], coords[,1] - center[1])
  angle <- sample(angle)
  cbind(radius * cos(angle), radius * sin(angle)) + center
}

apply_shift <- function(coords ) {
  n <- nrow(coords)
  coords[((1:n - shift - 1) %% n) + 1, ]
}

# Compute robustness
compute_robustness <- function(method_name, layout_dist, feature_list, dist_center_orig, iteration = NA) {
  center_dist <- colMeans(layout_dist)
  dist_center_dist <- sqrt(rowSums((layout_dist - center_dist)^2))

  df <- lapply(names(feature_list), function(name) {
    x <- feature_list[[name]]
    cor_orig <- cor(x, -dist_center_orig)
    cor_dist <- cor(x, -dist_center_dist)
    robustness <- 1 - abs(cor_orig - cor_dist)
    data.frame(
      Method = method_name,
      Iteration = iteration,
      Feature = name,
      Cor_Original = round(cor_orig, 4),
      Cor_Distorted = round(cor_dist, 4),
      Robustness = round(robustness, 4)
    )
  })

  do.call(rbind, df)
}

# Parameters
n_iterations <- 10#5
chosen_iter <- 5#3


# Distorted layouts for fixed methods
distorted_layouts <- list(
  Jitter = apply_jitter(layout_norm),
  Affine = apply_affine(layout_norm),
  Circular = apply_circular_swap(layout_norm)
)

distorted_layouts <- list(
  Jitter = apply_jitter(layout_norm),
  Affine = apply_affine(layout_norm),
  Circular = apply_circular_swap(layout_norm),
  Shift = apply_shift(layout_norm)  # You can change shift value here
)

Arnold <- apply_arnold(layout_norm, iterations = chosen_iter)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift
)

# Rename Arnold to include iteration number
#names(layout_list)[2] <- paste0("Arnold-", #chosen_iter)

res_mean <-numeric(n_iterations )

# Store all results
robustness_results <- list()

# Arnold iterations
for (i in 1:n_iterations) {
  layout_arnold <- apply_arnold(layout_norm, iterations = i)
  res <- compute_robustness("Arnold", layout_arnold, features, dist_center_orig, iteration = i)
  robustness_results[[paste0("Arnold_", i)]] <- res
res_mean [i] <- mean(abs(res$Robustness))

}
res_mean
chosen_iter <- which.min(res_mean)
chosen_iter

# Other methods (without iterations)
for (method in names(distorted_layouts)) {
  res <- compute_robustness(method, distorted_layouts[[method]], features, dist_center_orig)
  robustness_results[[method]] <- res
}

# Combine all results
robustness_df <- do.call(rbind, robustness_results)

# === 1. Plot: Robustness across Arnold iterations ===
windows()
arnold_it <- ggplot(filter(robustness_df, Method == "Arnold"), aes(x = Iteration, y = Robustness, color = Feature)) +
  geom_line(size = 1.1) +
  geom_point(size = 2.5) +
  ylim(-0.5, 1) +
  labs(title = paste(network1,"Robustness of Features Across Arnold Iterations"),
       x = "Arnold Iteration", y = "Robustness Score") +
  theme_minimal()
 print(arnold_it)

###############
ggsave(
  filename = paste(network1,"arnold.pdf"),
  plot = arnold_it,
  device = "pdf",
  width = 8,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 6,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"arnold.jpeg"),
  plot = arnold_it,
  device = "jpeg",
  width = 8,  # Adjust width
  height = 6,  # Adjust height
  dpi = 600,
  quality = 90 # Optional: JPEG quality
)
#################
# === 2. Plot: Barplot of all methods (select 1 Arnold iteration) ===
robust_subset <- filter(robustness_df, is.na(Iteration) | Iteration == chosen_iter)
robust_subset$Method <- ifelse(is.na(robust_subset$Iteration),
                               robust_subset$Method,
                               paste0("Arnold-", robust_subset$Iteration))
windows()
barplot <- ggplot(robust_subset, aes(x = Method, y = Robustness, fill = Feature)) +
  geom_bar(stat = "identity", position = "dodge") +
  ylim(-0.5, 1) +
  labs(title = paste(network1,"Robustness by Method (Arnold Iteration =", chosen_iter, ")"),
       y = "Robustness Score", x = "Distortion Method") +
  theme_minimal()
print(barplot)
###############
ggsave(
  filename = paste(network1,"barplot.pdf"),
  plot = barplot,
  device = "pdf",
  width = 8,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 6,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"barplot"), "\n")

ggsave(
  filename = paste(network1,"barplot.jpeg"),
  plot = barplot,
  device = "jpeg",
  width = 8,  # Adjust width
  height = 6,  # Adjust height
  dpi = 600,
  quality = 90 # Optional: JPEG quality
)

#################
# === 3. Plot: Network layouts with edges ===
plot_network_layout <- function(layout_matrix, title) {
  layout_df <- as.data.frame(layout_matrix)
  colnames(layout_df) <- c("x", "y")
  layout_df$node <- as.character(V(g))

  # Edges
  edges <- get.edgelist(g)
  edge_df <- data.frame(
    x = layout_matrix[edges[,1], 1],
    y = layout_matrix[edges[,1], 2],
    xend = layout_matrix[edges[,2], 1],
    yend = layout_matrix[edges[,2], 2]
  )
#windows()
  ggplot() +
    geom_segment(data = edge_df, aes(x = x, y = y, xend = xend, yend = yend), color = "gray80") +
    geom_point(data = layout_df, aes(x = x, y = y), color = "steelblue", size = 3) +
    geom_text(data = layout_df, aes(x = x, y = y, label = node), size = 2.8, vjust = -0.7) +
    theme_void() +
    labs(title = title)
}

# Layouts to display


Arnold <- apply_arnold(layout_norm, iterations = chosen_iter)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular
)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift
)

# Rename Arnold to include iteration number
names(layout_list)[2] <- paste0("Arnold-", chosen_iter)

layout_plots <- lapply(names(layout_list), function(name) {
  plot_network_layout(layout_list[[name]], name)
})

windows()
combined_plot<-do.call(grid.arrange, c(layout_plots, ncol = 2))
#print(combined_plot)

ggsave(
  filename = paste(network1,"combined.pdf"),
  plot = combined_plot,
  device = "pdf",
  width = 12,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 9,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"combined.jpeg"),
  plot = combined_plot,
  device = "jpeg",
  width = 12,  # Adjust width
  height = 9,  # Adjust height
  dpi = 300,
  quality = 90 # Optional: JPEG quality
)
cat("Saved combined JPEG:", paste(network1,"combined.jpeg"), "\n")

# === 4. Print full robustness table ===
print(robustness_df)

########################################################## brincar mas abajo
########## ojo seleccionar pdf o jpeg

# Loop through each layout and save it as a separate PDF
lapply(names(layout_list), function(name) {
  # Generate the plot object
  p_plot <- plot_network_layout(layout_list[[name]], name)

  # Construct the filename
  file_name <- paste(network1,tolower(name),".pdf")#file.path(output_dir, paste0(gsub(" ", "_", tolower(name)), "_layout.pdf")) # Cleans name for filename
  file_name <- paste(network1,tolower(name),".jpeg")#file.path(output_dir, paste0(gsub(" ", "_", tolower(name)), "_layout.pdf")) # Cleans name for filename

  # Save the plot to PDF with high resolution
  ggsave(
    filename = file_name,
    plot = p_plot,
#    device = "pdf",
    device = "jpeg",
    width = 8,  # Adjust width as needed (in inches)
    height = 6, # Adjust height as needed (in inches)
#    dpi = 300   # High resolution (dots per inch)
    dpi = 600,   # High resolution (dots per inch)
quality=90
  )

  cat("Saved:", file_name, "\n") # Print message for confirmation
})

################################
# 1. Install and load necessary packages
# You only need to run these lines once.
# install.packages("igraph")
# install.packages("dplyr")
library(igraph)
library(dplyr)

# 2. Define the obfuscation methods
# These are your custom functions, with the circular_swap function fixed.

# Arnold Map
arnold_map <- function(x, y, mod = 1) {
  x_new <- (x + y) %% mod
  y_new <- (x + 2 * y) %% mod
  return(c(x_new, y_new))
}

apply_arnold <- function(coords, iterations = 3, mod = 1) {
  coords_out <- coords
  for (i in 1:iterations) {
    coords_out <- t(apply(coords_out, 1, function(p) arnold_map(p[1], p[2], mod)))
  }
  return(coords_out)
}

# Jitter
apply_jitter <- function(coords, noise = 0.5) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}

# Affine Transformation
apply_affine <- function(coords, angle = pi/4, scale = 1.2) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

# Circular Swap (FIXED)
apply_circular_swap <- function(coords) {
  center <- colMeans(coords)
  radius <- sqrt(rowSums(sweep(coords, 2, center, "-")^2))
  angle <- atan2(coords[,2] - center[2], coords[,1] - center[1])
  angle <- sample(angle)
  new_coords <- cbind(radius * cos(angle), radius * sin(angle))
  # Add the center back to each row, using R's recycling rule
  return(sweep(new_coords, 2, center, "+"))
}

# Shift
apply_shift <- function(coords, shift = 5) {
  n <- nrow(coords)
  coords_shifted <- coords[((1:n - shift - 1) %% n) + 1, ]
  return(coords_shifted)
}

# 3. Prepare the network and original layout
#g <- make_graph("Zachary")
#original_coords <- layout_with_kk(g)
#original_coords <-layout_norm

############################################################# aqui hacia abajo
vertex_names <- if (!is.null(V(g)$name)) {
  V(g)$name
} else {
  as.character(1:vcount(g))
}

nodes_df <- data.frame(
  name = vertex_names,
  x = original_coords[, 1],
  y = original_coords[, 2]
)

# 4. Calculate topological importance (The Adversary's "Ground Truth")
nodes_df <- nodes_df %>%
  mutate(
    degree = degree(g),
    betweenness = betweenness(g, normalized = TRUE),
    eigenvector = evcent(g)$vector
  ) %>%
  mutate(
    rank_degree = rank(-degree, ties.method = "min"),
    rank_betweenness = rank(-betweenness, ties.method = "min"),
    rank_eigenvector = rank(-eigenvector, ties.method = "min")
  )

# 5. Measure Visual Prominence (The Adversary's "Inferred Importance")
calculate_visual_prominence <- function(coords, dist_threshold = 2.5) {
  num_nodes <- nrow(coords)
  prominence <- numeric(num_nodes)
  for (i in 1:num_nodes) {
    distances <- apply(coords, 1, function(p) sqrt((coords[i, 1] - p[1])^2 + (coords[i, 2] - p[2])^2))
    prominence[i] <- sum(distances < dist_threshold) - 1
  }
  return(prominence)
}

# 6. Evaluate the Attack's Success for a specific method
evaluate_attack <- function(data, k, centrality_type) {
  top_k_topo_nodes <- data %>%
    filter(!!as.symbol(paste0("rank_", centrality_type)) <= k) %>%
    pull(name)
  top_k_visual_nodes <- data %>%
    filter(rank_visual <= k) %>%
    pull(name)
  correct_guesses <- length(intersect(top_k_topo_nodes, top_k_visual_nodes))
  success_rate <- (correct_guesses / k) * 100
  return(success_rate)
}

# 7. Run the full experiment for each obfuscation method and print results
methods <- c("jitter", "affine", "circular_swap", "shift", "arnold")
results_list <- list()
dist_threshold = .2#2.
dist_threshold 
###
kkk=0
tempo_success <- matrix(NA,length(methods),3)
for (method in methods) {
  cat(paste0("\n--- Results for ", method, " obfuscation ---\n"))
kkk=kkk+1  
  obfuscated_coords <- switch(method,
    "jitter" = apply_jitter(original_coords),
    "affine" = apply_affine(original_coords),
    "circular_swap" = apply_circular_swap(original_coords),
    "shift" = apply_shift(original_coords),
    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
#    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
  )
  
  obfuscated_df <- nodes_df %>%
    mutate(
      x_obfuscated = obfuscated_coords[, 1],
      y_obfuscated = obfuscated_coords[, 2],
      visual_prominence = calculate_visual_prominence(obfuscated_coords, dist_threshold = dist_threshold )
    ) %>%
    mutate(rank_visual = rank(-visual_prominence, ties.method = "min"))
 kj=0   
  for (k in c(5, 10, 15)) {
kj=kj+1
    success_rate <- evaluate_attack(obfuscated_df, k, "degree")
tempo_success [kkk,kj]=success_rate
    cat(paste0("  Top-", k, " success rate (by Degree): ", round(success_rate, 2), "%\n"))
  }
}
rownames(tempo_success) <- methods

tempo_success
###############################################

#chat# ================================
# Batch Evaluation of Multiple Obfuscation Methods
# ================================

library(igraph)
library(ggplot2)
library(gridExtra)
library(infotheo)
library(mclust)

# ---- Utility: entropy function ----
shannon_entropy <- function(x) {
  p <- prop.table(table(x))
  -sum(p * log(p))
}

# ---- Core evaluation (same as before) ----
evaluate_obfuscation <- function(g, coords_orig, coords_obf, method_name="") {
  # Node IDs
  if(is.null(V(g)$name)) V(g)$name <- as.character(1:vcount(g))
  coords_orig$node <- V(g)$name
  coords_obf$node  <- V(g)$name
  
  # Displacement
  disp <- sqrt((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  avg_disp <- mean(disp)
  max_disp <- max(disp)
  disp_entropy <- shannon_entropy(round(disp, 2))
  nde <- mean((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  
  # Centrality recovery
  centroid_obf  <- colMeans(coords_obf[,c("x","y")])
  spatial_cent_obf <- sqrt((coords_obf$x - centroid_obf[1])^2 + (coords_obf$y - centroid_obf[2])^2)
  true_cent <- betweenness(g)
  rank_corr_obf <- suppressWarnings(cor(rank(true_cent), rank(spatial_cent_obf), method="kendall"))
  disc_true  <- discretize(true_cent, disc="equalfreq", nbins=5)
  disc_spat  <- discretize(spatial_cent_obf, disc="equalfreq", nbins=5)
  mi_obf     <- mutinformation(disc_true, disc_spat)
  
  # Mapping entropy
  perm <- match(paste(coords_obf$x, coords_obf$y), paste(coords_orig$x, coords_orig$y))
  mapping_entropy <- shannon_entropy(perm)
  
  # Community leakage
  comm_true <- cluster_louvain(g)$membership
  k <- length(unique(comm_true))
  comm_spatial <- kmeans(coords_obf[,c("x","y")], centers=k, nstart=10)$cluster
  comm_ARI <- adjustedRandIndex(comm_true, comm_spatial)
  
  return(data.frame(
    Method = method_name,
    AvgDisplacement = avg_disp,
    MaxDisplacement = max_disp,
    DispEntropy = disp_entropy,
    NDE = nde,
    RankCorr = rank_corr_obf,
    MI = mi_obf,
    MappingEntropy = mapping_entropy,
    CommunityARI = comm_ARI
  ))
}
###############################################################3
## de aqui no hacia abjo... buscar proximo
# ---- Obfuscation methods ----
apply_obfuscations <- function(g) {
  coords_orig <- data.frame(layout_with_fr(g))
  colnames(coords_orig) <- c("x","y")
  
  n <- nrow(coords_orig)
  results <- data.frame()
  
  # Jitter
  coords_jitter <- coords_orig + data.frame(x=rnorm(n, 0, 0.5), y=rnorm(n, 0, 0.5))
  results <- rbind(results, evaluate_obfuscation(g, coords_orig, coords_jitter, "Jitter"))
  
  # Affine (rotation by 45°)
  theta <- pi/4
  R <- matrix(c(cos(theta), -sin(theta), sin(theta), cos(theta)), ncol=2)
  coords_affine <- as.data.frame(as.matrix(coords_orig) %*% R)
  colnames(coords_affine) <- c("x","y")
  results <- rbind(results, evaluate_obfuscation(g, coords_orig, coords_affine, "Affine"))
  
  # Circular
  coords_circ <- data.frame(
    x = cos(seq(0, 2*pi, length.out=n+1)[1:n]),
    y = sin(seq(0, 2*pi, length.out=n+1)[1:n])
  )
  results <- rbind(results, evaluate_obfuscation(g, coords_orig, coords_circ, "Circular"))
  
  # Permutation
  coords_perm <- coords_orig[sample(1:n), ]
  rownames(coords_perm) <- NULL
  results <- rbind(results, evaluate_obfuscation(g, coords_orig, coords_perm, "Permutation"))
  
  # Arnold transform (simple modular scrambling, toy version)
  coords_arnold <- coords_orig
  coords_arnold$x <- (coords_orig$x + coords_orig$y) %% max(coords_orig$x)
  coords_arnold$y <- (coords_orig$x + 2*coords_orig$y) %% max(coords_orig$y)
  results <- rbind(results, evaluate_obfuscation(g, coords_orig, coords_arnold, "Arnold"))
  
  return(results)
}

# ================================
# Example usage: Zachary Karate Club
# ================================
######################################## aqui si hacia abajo
evaluate_from_list <- function(g, layout_list) {
  results <- data.frame()
  
  # Extract original
  coords_orig <- as.data.frame(layout_list$Original)
  colnames(coords_orig) <- c("x","y")
  
  # Loop over all obfuscated versions
  for (name in names(layout_list)) {
    if (name == "Original") next  # skip original
    coords_obf <- as.data.frame(layout_list[[name]])
    colnames(coords_obf) <- c("x","y")
    
    # Evaluate
    res <- evaluate_obfuscation(g, coords_orig, coords_obf, method_name=name)
    results <- rbind(results, res)
  }
  
  return(results)
}

# Example run
#library(igraph)
#g <- make_graph("Zachary")
#V(g)$name <- as.character(1:vcount(g))

results_from_list <- evaluate_from_list(g, layout_list)
print(results_from_list)




###################################
#g <- make_graph("Zachary")
#V(g)$name <- as.character(1:vcount(g))
#coords_orig <- 
#results_all <- apply_obfuscations(g)
#print(results_all)

results <-results_from_list


# ================================
# Radar Chart of Privacy Effectiveness
# ================================

# Install package if needed
# install.packages("fmsb")

library(fmsb)

# ---- Data: your results ----
resultsxx <- data.frame(
  Method = c("Jitter", "Affine", "Circular", "Permutation", "Arnold"),
  AvgDisplacement = c(0.7027344, 3.1636854, 4.0545331, 4.2355644, 4.4078964),
  MaxDisplacement = c(1.749123, 6.633358, 8.172122, 11.121577, 8.991237),
  DispEntropy     = c(3.322494, 3.485587, 3.485587, 3.485587, 3.526361),
  NDE             = c(0.5989409, 12.1442519, 19.5850286, 24.7566402, 24.0728461),
  RankCorr        = c(-0.36851640, -0.38371296, -0.04991428, -0.19375605, -0.15576467),
  MI              = c(0.4630779, 0.5907936, 0.1385731, 0.2825589, 0.2338274),
  MappingEntropy  = c(0.0, 0.0, 0.0, 3.526361, 0.0),
  CommunityARI    = c(0.32723243, 0.55666494, 0.16094483, 0.02275605, 0.05308465)
)

# ---- Normalize to 0–1 scale ----
metrics <- colnames(results)[-1]
df_norm <- results

for (col in metrics) {
  if (col %in% c("RankCorr","MI","CommunityARI")) {
    # invert: lower leakage = better privacy
    df_norm[[col]] <- 1 - (results[[col]] - min(results[[col]], na.rm=TRUE)) /
                            (max(results[[col]], na.rm=TRUE) - min(results[[col]], na.rm=TRUE))
  } else {
    # normal: higher displacement/entropy = stronger privacy
    df_norm[[col]] <- (results[[col]] - min(results[[col]], na.rm=TRUE)) /
                      (max(results[[col]], na.rm=TRUE) - min(results[[col]], na.rm=TRUE))
  }
}

# ---- Radar chart needs max + min rows ----
df_radar <- rbind(
  rep(1, length(metrics)),  # max
  rep(0, length(metrics)),  # min
  df_norm[, metrics]
)
rownames(df_radar) <- c("Max","Min",results$Method)

# ---- Plot ----
colors <- c("blue","red","green","purple","orange")
windows()
radarchart(df_radar, axistype=1,
           # grid
           cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
           # polygons
           pcol=colors, plwd=2, plty=1,
           pfcol=scales::alpha(colors,0.2),
           # labels
           vlcex=0.8)

legend("bottomleft", legend=results$Method, col=colors, lty=1, lwd=2, bty="n", cex=0.8)


metrics <- colnames(results)[-1]  # exclude Method

# Ranking function: always return consistent column names
get_rankings <- function(df, metric, invert=FALSE) {
  if (invert) {
    ranked <- df %>% arrange(!!sym(metric))
  } else {
    ranked <- df %>% arrange(desc(!!sym(metric)))
  }
  ranked <- ranked %>%
    mutate(Rank = row_number(),
           Metric = metric) %>%
    select(Metric, Method, Value = !!sym(metric), Rank)
  return(ranked)
}

# Apply to all metrics
rankings_list <- list()
for (m in metrics) {
  if (m %in% c("RankCorr","MI","CommunityARI")) {
    rankings_list[[m]] <- get_rankings(results, m, invert=TRUE)
  } else {
    rankings_list[[m]] <- get_rankings(results, m, invert=FALSE)
  }
}

# Combine safely
rankings <- bind_rows(rankings_list)

# Best method per metric
best_methods <- rankings %>% filter(Rank == 1) %>% select(Metric, BestMethod = Method)
rankings_list
print(rankings)
print(best_methods)

 results_norm <- df_norm

# --- Define user weights (example: emphasize MappingEntropy & CommunityARI) ---
weights <- c(
  AvgDisplacement = 0.1,
  MaxDisplacement = 0.1,
  DispEntropy     = 0.1,
  NDE             = 0.1,
  RankCorr        = 0.2,
  MI              = 0.2,
  MappingEntropy  = 0.1,
  CommunityARI    = 0.1
)

# --- Compute global score ---
results_norm$GlobalScore <- as.numeric(as.matrix(results_norm[,-1]) %*% weights)

# --- Sort by best privacy score ---
results_ranked <- results_norm %>% arrange(desc(GlobalScore))

print(results_ranked[,c("Method","GlobalScore")])
######
# A script to quantify the visual similarity between original and obfuscated layouts.

# This script requires the 'vegan' package for Procrustes analysis.
# If you don't have it, uncomment the line below and run it once:
# install.packages("vegan")
library(vegan)

# --- Define the Similarity Analysis Function ---
# This function takes a list of named layouts, where the first one is the original.
# It returns a data frame with the similarity metrics for each obfuscated layout.
calculate_layout_similarity <- function(layout_list) {
  
  # 1. Get the original layout and its name
  original_name <- names(layout_list)[1]
  original_coords <- layout_list[[1]]

  # 2. Calculate the pairwise Euclidean distances for the original layout
  original_dist_matrix <- as.matrix(dist(original_coords, method = "euclidean"))
  
  # Prepare a data frame to store the results
  results_df <- data.frame(
    Method = character(),
    Procrustes_Distance = numeric(),
    Stress_Measure = numeric(),
    Distance_Correlation = numeric(),
    stringsAsFactors = FALSE
  )
  
  # 3. Loop through the obfuscated layouts and perform the analysis
  for (i in 2:length(layout_list)) {
    obfuscated_name <- names(layout_list)[i]
    obfuscated_coords <- layout_list[[i]]
    
    # Calculate the pairwise Euclidean distances for the obfuscated layout
    obfuscated_dist_matrix <- as.matrix(dist(obfuscated_coords, method = "euclidean"))
    
    # -- 4. Quantify the Similarity Metrics --
    
    # A) Procrustes Distance
    # This measures the minimum distance between the two layouts after optimal rotation and scaling.
    # It requires the same number of rows (nodes) in both data sets.
    proc_result <- procrustes(original_coords, obfuscated_coords, symmetric = TRUE)
    procrustes_dist <- proc_result$ss
    
    # B) Stress Measure (Sum of Squared Differences of Distances)
    # This directly compares the two distance matrices. A lower value indicates less distortion.
    stress_val <- sum((original_dist_matrix - obfuscated_dist_matrix)^2)
    
    # C) Correlation of Inter-node Euclidean Distances
    # This measures the linear correlation between the two sets of distances. A high correlation
    # indicates that the relative distances between nodes were preserved.
    distance_cor <- cor(as.vector(original_dist_matrix), as.vector(obfuscated_dist_matrix))
    
    # Add the results to the data frame
    results_df <- rbind(results_df, data.frame(
      Method = obfuscated_name,
      Procrustes_Distance = procrustes_dist,
      Stress_Measure = stress_val,
      Distance_Correlation = distance_cor
    ))
  }
  
  return(results_df)
}

# --- Example of how to use the function ---

# Create dummy layouts for demonstration (replace with your actual layouts)
# Make sure they are data frames or matrices with two columns (x and y)
dummy_original <- data.frame(x = 1:5, y = 1:5)
dummy_jitter <- data.frame(x = 1:5 + rnorm(5, 0, 0.1), y = 1:5 + rnorm(5, 0, 0.1))
dummy_scrambled <- data.frame(x = sample(1:5), y = sample(1:5))

# Create a named list of your layouts
my_layouts <- list(
  "Original" = dummy_original,
  "Jitter" = dummy_jitter,
  "Scrambled" = dummy_scrambled
)

# Run the analysis

similarity_results <- calculate_layout_similarity(layout_list)
print(similarity_results)

# --- Interpretation of the Results ---

#The three metrics you've chosen provide a robust, quantitative way to measure the visual similarity that can lead to a "sense of familiarity" and a false sense of security in an observer.
#
#1.  **Procrustes Distance:** This metric quantifies the overall **shape similarity** between the two layouts after they have been optimally aligned (rotated, scaled, and translated). A low Procrustes distance indicates that the obfuscated layout retains the overall "shape" of the original. Methods like your "Shift" approach, which preserve the spatial footprint, would be expected to have a much lower Procrustes distance than a chaotic method like the Arnold Transform.
#
#2.  **Stress or Distortion Measure:** This metric directly measures the **distortion of internal relationships** by comparing the distances between all pairs of nodes. A low stress value means that the obfuscation did not significantly alter the relative proximities of the nodes. This is a critical metric for a geographical layout, as it indicates whether local neighborhoods and clusters have been preserved.
#
#3.  **Correlation of Inter-node Distances:** This is a simple but powerful measure of how well the obfuscation method **breaks the overall spatial structure**. A high correlation would show that nodes that were close in the original layout are still relatively close in the obfuscated one. This would suggest the obfuscation is a less robust defense against a savvy adversary.

######### chapt
library(igraph)
library(vegan)     # for Procrustes, Mantel
library(infotheo)  # for MI
library(cluster)   # for ARI
library(entropy)   # for Shannon entropy

# --- helper: Shannon entropy ---
shannon_entropy <- function(x) {
  x <- table(x)
  p <- x / sum(x)
  -sum(p * log(p + 1e-12))
}

# --- evaluation function for one obfuscation ---
evaluate_obfuscation_full <- function(g, coords_orig, coords_obf, method_name="") {
  # --- Node IDs ---
  if (is.null(V(g)$name)) V(g)$name <- as.character(1:vcount(g))
  coords_orig$node <- V(g)$name
  coords_obf$node  <- V(g)$name
  
  # --- Displacement metrics ---
  disp <- sqrt((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  avg_disp <- mean(disp)
  max_disp <- max(disp)
  disp_entropy <- shannon_entropy(round(disp, 2))
  nde <- mean((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  
  # --- Layout similarity metrics ---
  proc <- procrustes(coords_orig[,c("x","y")], coords_obf[,c("x","y")], scale=TRUE)
  proc_dist <- proc$ss
  
  d_orig <- dist(coords_orig[,c("x","y")])
  d_obf  <- dist(coords_obf[,c("x","y")])
  mantel_res <- suppressWarnings(mantel(d_orig, d_obf))
  mantel_corr <- mantel_res$statistic
  
  # --- Spatial centrality (obfuscated) ---
  centroid_obf <- colMeans(coords_obf[,c("x","y")])
  spatial_cent_obf <- sqrt((coords_obf$x - centroid_obf[1])^2 +
                           (coords_obf$y - centroid_obf[2])^2)
  
  # --- Topological centralities ---
  centralities <- list(
    Degree      = degree(g),
    Closeness   = closeness(g, normalized=TRUE),
    Betweenness = betweenness(g),
    Eigenvector = eigen_centrality(g)$vector
  )
  
  rankcorr_list <- c()
  mi_list <- c()
  
  for (cname in names(centralities)) {
    true_cent <- centralities[[cname]]
    
    rank_corr <- suppressWarnings(cor(rank(true_cent), rank(spatial_cent_obf), method="kendall"))

disc_true <- infotheo::discretize(true_cent, disc="equalfreq", nbins=5)
disc_spat <- infotheo::discretize(spatial_cent_obf, disc="equalfreq", nbins=5)
mi <- mutinformation(disc_true, disc_spat)    
    rankcorr_list[paste0("RankCorr_", cname)] <- rank_corr
    mi_list[paste0("MI_", cname)] <- mi
  }
  
  # --- Mapping entropy ---
  perm <- match(paste(coords_obf$x, coords_obf$y), paste(coords_orig$x, coords_orig$y))
  mapping_entropy <- shannon_entropy(perm)
  
  # --- Community leakage ---
  comm_true <- cluster_louvain(g)$membership
  k <- length(unique(comm_true))
  comm_spatial <- kmeans(coords_obf[,c("x","y")], centers=k, nstart=10)$cluster
  comm_ARI <- adjustedRandIndex(comm_true, comm_spatial)
  
  # --- Assemble result ---
  res <- data.frame(
    Method = method_name,
    AvgDisplacement = avg_disp,
    MaxDisplacement = max_disp,
    DispEntropy = disp_entropy,
    NDE = nde,
    ProcrustesSS = proc_dist,
    MantelCorr = mantel_corr,
    MappingEntropy = mapping_entropy,
    CommunityARI = comm_ARI,
    t(rankcorr_list),
    t(mi_list),
    row.names=NULL
  )
  return(res)
}

# --- apply to all obfuscation methods in layout_list ---
evaluate_all_methods <- function(g, layout_list) {
  results <- data.frame()
  
  coords_orig <- as.data.frame(layout_list$Original)
  colnames(coords_orig) <- c("x","y")
  
  for (name in names(layout_list)) {
    if (name == "Original") next
    coords_obf <- as.data.frame(layout_list[[name]])
    colnames(coords_obf) <- c("x","y")
    
    res <- evaluate_obfuscation_full(g, coords_orig, coords_obf, method_name=name)
    results <- rbind(results, res)
  }
  return(results)
}

# --- Example run with Zachary karate club ---
#g <- make_graph("Zachary")
V(g)$name <- as.character(1:vcount(g))

# Assume you have layout_list already
 results_all <- evaluate_all_methods(g, layout_list)
 print(results_all)

library(dplyr)
library(ggplot2)
library(scales)

# Assume results_all already computed
df <- results_all

# ---- Construct composite axes ----
# Visual scrambling = high ProcrustesSS + low MantelCorr
df <- df %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  )

# Normalize to 0–1
df <- df %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore)
  )

# ---- Plot ----
windows()
biplot_0 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
print(biplot_0)
ggsave(paste(network1,"biplot_0.pdf"), plot = biplot_0, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_0.jpeg"), plot = biplot_0, width = 10, height = 8, dpi = 300)

library(dplyr)
library(ggplot2)
library(scales)

# Assume results_all already computed
df <- results_all

# ---- Composite scores ----
df <- df %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore)
  )

# ---- Plot with quadrants ----
windows()
biplot_1 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(biplot_1)
ggsave(paste(network1,"biplot_1.pdf"), plot = biplot_1, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_1.jpeg"), plot = biplot_1, width = 10, height = 8, dpi = 300)

df1 <-df
################### aqui weighted
library(dplyr)
library(ggplot2)
library(scales)

# ---- Composite scores ----
df <- results_all %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore),
    GlobalScore = (ScramblingScore + PrivacyScore) / 2   # weighted sum, equal weights
  )

# ---- Plot with quadrants and global ranking ----
windows()
biplot_2 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = GlobalScore)) +
  geom_point(size = 6) +
  geom_text(vjust = -1, size = 4, color = "black") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
#  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
#  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
#  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
#  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  scale_color_gradient(low = "orange", high = "darkgreen", name = "Global Score") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Scrambling vs Privacy + Global Ranking") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right")
print(biplot_2)
ggsave(paste(network1,"biplot_2.pdf"), plot = biplot_2, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_2.jpeg"), plot = biplot_2, width = 10, height = 8, dpi = 300)
df

####################################################################

# Install and load necessary packages
# install.packages("igraph")
# install.packages("ggplot2")
# install.packages("dplyr")
library(igraph)
library(ggplot2)
library(dplyr) # For data manipulation, which includes tibble() and %>%

# --- 1. Network Setup: Zachary's Karate Club ---
# Load Zachary's Karate Club graph
g <- make_graph("Zachary")

# --- 2. Assign Spatial Coordinates (Arbitrary for Demonstration) ---
# Set a seed for reproducibility of the layout
set.seed(42) 
layout_fr <- layout_with_fr(g) # Fruchterman-Reingold layout

# Store coordinates as vertex attributes
V(g)$x <- layout_fr[, 1]
V(g)$y <- layout_fr[, 2]

# Add a unique ID for easier tracking
V(g)$name <- as.character(1:vcount(g))

# --- IMPORTANT DISCLAIMER FOR GWN ---
message("NOTE: The spatial layout for Zachary's Karate Club is generated arbitrarily (Fruchterman-Reingold).")
message("      For real-world applications of Geometric Winding Number, truly meaningful geographical")
message("      coordinates are absolutely essential for valid spatial interpretation.")
message("      This example serves primarily to demonstrate the computational methodology.")

# --- 3. Community Detection ---
# Using the Louvain method for community detection
communities <- cluster_louvain(g)
V(g)$community <- communities$membership

message(paste("\nCommunity Detection Results:", communities$no, "communities found."))
print(communities)

# --- 4. Loop Selection (S-T-S Cycles) ---

# --- PARAMETERS for Cycle Selection ---
# Option 1: Define specific S-T pairs for targeted analysis (set to NULL to use random)
# Example: Focus on key leaders (Node 1 and Node 34) or within communities (2 and 3)
manual_st_pairs <- list(
  c("1", "34"), # Path between the two main leaders
  c("2", "3") ,  # Path between two members of Community 1
c("13", "27") 
)
# If you want to rely purely on random sampling, set this to NULL:
# manual_st_pairs <- NULL

# Option 2: Parameters for Random S-T pair sampling (used if manual_st_pairs is NULL)
num_random_st_pairs <- NULL#3#5 # Number of distinct random S-T pairs to sample

# General Loop Limits (apply to both manual and random selection)
max_loops_per_st_pair <- 20#5 # Max simple S-T-S loops to find for each S-T pair
max_loops_to_consider <- 60#20 # Overall maximum number of unique S-T-S loops for the entire analysis
path_cutoff <- 7           # CRITICAL: Max path length (number of edges). Adjust for performance vs. comprehensiveness.
                           # Lower values (e.g., 4-6 for Karate Club) = much faster.

selected_loops <- list()
loop_count <- 0
unique_cycles_list <- list() # To store string representations of cycles for uniqueness check

all_node_names <- V(g)$name
processed_st_pairs_keys <- c() # To track unique (S,T) pairs already processed

message(paste0("\nInitiating loop selection."))
message(paste0("  Paths will be limited to a maximum length (cutoff) of ", path_cutoff, " edges for performance."))

# Prepare the list of S-T pairs to iterate through
st_pairs_to_process <- list()
if (!is.null(manual_st_pairs) && length(manual_st_pairs) > 0) {
  st_pairs_to_process <- manual_st_pairs
  message(paste0("  Using ", length(manual_st_pairs), " manually specified S-T pair(s)."))
} else {
  message(paste0("  Using random S-T pair selection (up to ", num_random_st_pairs, " pairs)."))
  # Generate random pairs until num_random_st_pairs or max_loops_to_consider is met
  st_pair_attempts_count <- 0
  while (st_pair_attempts_count < num_random_st_pairs && loop_count < max_loops_to_consider * 1) { # *2 as a heuristic for attempts
print(c(st_pair_attempts_count,loop_count))
    S_node_idx <- sample(1:vcount(g), 1)
    T_node_idx <- sample(1:vcount(g), 1)
    S_node_rand <- all_node_names[S_node_idx]
    T_node_rand <- all_node_names[T_node_idx]
    if (S_node_rand == T_node_rand) next
    current_pair_key <- paste(sort(c(S_node_rand, T_node_rand)), collapse = "-")
    if (!(current_pair_key %in% processed_st_pairs_keys)) {
      st_pairs_to_process[[length(st_pairs_to_process) + 1]] <- c(S_node_rand, T_node_rand)
      processed_st_pairs_keys <- c(processed_st_pairs_keys, current_pair_key)
      st_pair_attempts_count <- st_pair_attempts_count + 1
    }
  }
}

# Variables to store the S and T nodes of the first successfully found loop, for plotting labels
first_S_node_for_label <- NA
first_T_node_for_label <- NA

# Iterate through the determined S-T pairs
st_pair_idx <- 0
for (pair_nodes in st_pairs_to_process) {
#print(c(st_pair_attempts_count,loop_count))
  if (loop_count >= max_loops_to_consider) break # Overall loop limit reached

  st_pair_idx <- st_pair_idx + 1
  S_node <- pair_nodes[1]
  T_node <- pair_nodes[2]

  message(paste0("\n  Processing S-T pair (", st_pair_idx, "/", length(st_pairs_to_process), "): S=", S_node, ", T=", T_node))

  # Find all simple paths between the current S and T
  all_paths_S_to_T <- all_simple_paths(g, from = V(g)[S_node], to = V(g)[T_node], cutoff = path_cutoff)
  all_paths_T_to_S <- all_simple_paths(g, from = V(g)[T_node], to = V(g)[S_node], cutoff = path_cutoff)

  # Sort paths by length to prioritize shorter cycles (optional, but good practice)
  all_paths_S_to_T <- all_paths_S_to_T[order(sapply(all_paths_S_to_T, length))]
  all_paths_T_to_S <- all_paths_T_to_S[order(sapply(all_paths_T_to_S, length))]

  current_st_pair_loops_found <- 0 # Counter for loops found for THIS S-T pair

  # Iterate through paths to form unique simple cycles
  for (path_s_t in all_paths_S_to_T) {
    # Break if overall max loops reached or max for this S-T pair reached
    if (loop_count >= max_loops_to_consider || current_st_pair_loops_found >= max_loops_per_st_pair) break 
    
    for (path_t_s in all_paths_T_to_S) {
      # Break if overall max loops reached or max for this S-T pair reached
      if (loop_count >= max_loops_to_consider || current_st_pair_loops_found >= max_loops_per_st_pair) break 
      
      path_s_t_names <- as.character(V(g)[path_s_t]$name)
      path_t_s_names <- as.character(V(g)[path_t_s]$name)

      # Ensure paths are node-disjoint except for S and T endpoints
      internal_nodes_s_t <- path_s_t_names[2:(length(path_s_t_names)-1)]
      internal_nodes_t_s <- path_t_s_names[2:(length(path_t_s_names)-1)]
      
      # Special case for direct S-T edge forming a 2-node cycle (which is simple)
      if (length(internal_nodes_s_t) == 0 && length(internal_nodes_t_s) == 0) {
        if (length(path_s_t_names) + length(path_t_s_names) - 2 < 3) { # total unique nodes in cycle
           next # Skip if resulting cycle would have less than 3 unique nodes for polygon
        }
      } else if (length(intersect(internal_nodes_s_t, internal_nodes_t_s)) > 0) {
        next # Skip if internal nodes overlap (not a simple cycle for GWN polygon)
      }
      
      # Combine paths to form the cycle
      cycle_nodes <- c(path_s_t_names, path_t_s_names[-1]) 
      
      # Check if the cycle has enough unique nodes to form a polygon (>2 unique nodes)
      if (length(unique(cycle_nodes)) < 3) {
          next
      }

      # Use sorted node names to create a unique string identifier for the cycle
      sorted_cycle_str <- paste(sort(unique(cycle_nodes)), collapse = "-") # Use unique for sorted_cycle_str

      if (!(sorted_cycle_str %in% names(unique_cycles_list))) {
        unique_cycles_list[[sorted_cycle_str]] <- cycle_nodes
        loop_count <- loop_count + 1
        selected_loops[[paste0("Loop_", loop_count, "_S", S_node, "T", T_node)]] <- cycle_nodes # Append S/T to loop name for clarity
        current_st_pair_loops_found <- current_st_pair_loops_found + 1

        # Store the S and T for labeling the plot, if this is the first loop found
        if (is.na(first_S_node_for_label)) {
            first_S_node_for_label <- S_node
            first_T_node_for_label <- T_node
        }
      }
    }
  }
}

message(paste("\nFinished loop selection. Total identified S-T-S cycles:", length(selected_loops)))
# Print the identified loops
for (i in seq_along(selected_loops)) {
  message(paste0("  ", names(selected_loops)[i], ": ", paste(selected_loops[[i]], collapse = " - ")))
}


# --- 5. Geometric Winding Number (GWN) Calculation ---

# Basic Point-in-Polygon (PIP) function using Ray Casting Algorithm
# Returns 1 if point is inside, 0 if outside
# Assumes polygon vertices are ordered (clockwise or counter-clockwise)
# From: https://rosettacode.org/wiki/Ray-casting_algorithm
point_in_polygon <- function(px, py, poly_x, poly_y) {
  num_vertices <- length(poly_x)
  intersections <- 0
  
  # The loop needs to connect back to the start for the last segment
  # So we iterate up to num_vertices, and use modulo for x2, y2
  for (i in 1:num_vertices) {
    x1 <- poly_x[i]
    y1 <- poly_y[i]
    x2 <- poly_x[i %% num_vertices + 1] 
    y2 <- poly_y[i %% num_vertices + 1]
    
    # Check if the ray from (px,py) crosses the segment (x1,y1)-(x2,y2)
    # The condition `(y1 <= py && py < y2) || (y2 <= py && py < y1)` checks if the ray's y-coordinate is between the segment's y-coordinates.
    # The `px < (x2 - x1) * (py - y1) / (y2 - y1) + x1` checks if the intersection point's x-coordinate is to the right of the ray's origin x-coordinate.
    if (((y1 <= py && py < y2) || (y2 <= py && py < y1)) && (px < (x2 - x1) * (py - y1) / (y2 - y1) + x1)) {
      intersections <- intersections + 1
    }
  }
  
  return(intersections %% 2 == 1) # True if odd intersections (inside), False if even (outside)
}

# Initialize GWN matrix: rows are nodes, columns are loops
# Only create matrix if loops were found
if (length(selected_loops) > 0) {
  gwn_matrix <- matrix(0, nrow = vcount(g), ncol = length(selected_loops),
                       dimnames = list(V(g)$name, names(selected_loops)))

  # Calculate GWN for each node relative to each loop
  for (loop_name in names(selected_loops)) {
    current_loop_nodes <- selected_loops[[loop_name]]
    
    # Get coordinates for the loop's polygon. Ensure order is preserved.
    # Access coordinates directly from the graph's vertex attributes, preserving the path order
    poly_x <- V(g)$x[V(g)[current_loop_nodes]]
    poly_y <- V(g)$y[V(g)[current_loop_nodes]]
    
    # Ensure the loop forms a valid polygon with at least 3 unique vertices (redundant with earlier check, but good here too)
    if (length(unique(current_loop_nodes)) < 3) {
      message(paste0("  Skipping GWN for ", loop_name, ": Not enough unique vertices to form a polygon (", length(unique(current_loop_nodes)), ")."))
      next
    }

    # Test every node (except those part of the current loop)
    for (node_idx in 1:vcount(g)) {
      node_name <- V(g)$name[node_idx]
      
      # Nodes on the loop itself are not "enclosed" by the winding number definition
      if (!(node_name %in% current_loop_nodes)) {
        px <- V(g)$x[node_idx]
        py <- V(g)$y[node_idx]
        
        # Determine if the node is inside the loop
        is_inside <- point_in_polygon(px, py, poly_x, poly_y)
        
        if (is_inside) {
          gwn_matrix[node_name, loop_name] <- 1
        }
      }
    }
  }

  message("\nGeometric Winding Number (GWN) Matrix:")
  print(gwn_matrix)

  # --- 6. Total GWN for Each Node ---
  # Sum of absolute GWN values for each node across all loops
  V(g)$total_gwn <- rowSums(abs(gwn_matrix))

  message("\nTotal GWN for each node (sum of enclosures):")
  print(V(g)$total_gwn)
} else {
  message("\nNo loops were found, GWN matrix and total GWN will not be calculated.")
  V(g)$total_gwn <- 0 # Assign 0 if no loops found
}


# --- 7. Integration of Community Detection and GWN ---
# Create node_data tibble explicitly from vertex attributes
node_data <- tibble(
  name = V(g)$name,
  community = V(g)$community,
  total_gwn = V(g)$total_gwn,
  x = V(g)$x,
  y = V(g)$y
) %>%
  arrange(desc(total_gwn))

message("\nNode Data with Community and Total GWN (sorted by Total GWN):")
print(node_data)

# Analyze average GWN per community (simple example)
community_gwn_summary <- node_data %>%
  group_by(community) %>%
  summarise(
    avg_total_gwn = mean(total_gwn),
    num_nodes = n()
  ) %>%
  arrange(desc(avg_total_gwn))

message("\nAverage Total GWN per Community:")
print(community_gwn_summary)

# --- 8. Visualization ---

# Create edge_df tibble explicitly from graph edges
edge_df <- tibble(
  from = ends(g, E(g))[,1], # Get 'from' node IDs (numeric)
  to = ends(g, E(g))[,2]    # Get 'to' node IDs (numeric)
) %>%
mutate(
    x = V(g)$x[from], # Use numeric IDs to directly index coordinates
    y = V(g)$y[from],
    xend = V(g)$x[to],
    yend = V(g)$y[to]
)

# --- Corrected and more robust edge_df creation ---

# 1. Get the edges as a data frame with 'from' and 'to' as node NAMES
#    Using igraph::as_data_frame directly on the graph object
edge_data_raw <- igraph::as_data_frame(g, what = "edges") # This returns node names by default

# 2. Join with the node_df to get coordinates for both 'from' and 'to' nodes
edge_df <- edge_data_raw %>%
  # Join for the 'from' node's coordinates
  left_join(node_df %>% select(name, x_from = x, y_from = y), # Rename columns to 'x_from', 'y_from' here
            by = c("from" = "name")) %>%

  # Join for the 'to' node's coordinates
  left_join(node_df %>% select(name, x_to = x, y_to = y),     # Rename columns to 'x_to', 'y_to' here
            by = c("to" = "name")) %>%
  
  # Finally, select and rename to the desired final column names for geom_segment
  select(from, to, x = x_from, y = y_from, xend = x_to, yend = y_to)

# --- End Corrected edge_df creation ---
edge_df 

# Create node_df tibble explicitly (same as node_data for ggplot)
node_df <- node_data %>%
  mutate(community = as.factor(community)) # Ensure community is a factor for coloring

# Initialize the plot
# Initialize the plot
p <- ggplot() +
  # Plot edges - MODIFIED FOR MAXIMUM VISIBILITY
  geom_segment(data = edge_df, aes(x = x, y = y, xend = xend, yend = yend),
               color = "gray",  # Changed to black for contrast
               alpha = 0.8,      # Increased alpha (less transparent)
               size = 0.8) +     # Increased line thickness significantly
  # Plot communities as nodes
  geom_point(data = node_df, aes(x = x, y = y, color = community, size = total_gwn),
             alpha = 0.8) +
  # Add node labels
  geom_text(data = node_df, aes(x = x, y = y, label = name), 
            nudge_y = 0.05, size = 3, color = "black")
# ... (rest of your plotting code) ...


# Add the S-T-S loops as polygons ONLY if loops were found
if (length(selected_loops) > 0) {
  p <- p + lapply(names(selected_loops), function(loop_name) {
    loop_nodes <- selected_loops[[loop_name]]
    
    # Ensure the loop has at least 3 UNIQUE nodes to form a polygon for geom_polygon
    if (length(unique(loop_nodes)) < 3) {
      warning(paste0("Skipping plotting loop '", loop_name, "': Not enough unique vertices to form a polygon (", length(unique(loop_nodes)), ")."))
      return(NULL) # Return NULL for this specific layer
    }

    # Extract coordinates in the correct order for geom_polygon
    ordered_x_coords <- V(g)$x[V(g)[loop_nodes]]
    ordered_y_coords <- V(g)$y[V(g)[loop_nodes]]
    
    # Create a small tibble for geom_polygon
    polygon_df <- tibble(x = ordered_x_coords, y = ordered_y_coords)
    
    geom_polygon(data = polygon_df, aes(x = x, y = y),
                 fill = NA, color = "darkgreen", linetype = "dashed", size = 0.8, alpha = 0.5)
  })
}

# Highlight the S and T nodes that were involved in any of the selected loops, ONLY if loops were found
if (length(selected_loops) > 0) {
  all_loop_st_nodes <- unique(unlist(lapply(names(selected_loops), function(ln) {
    # Extract S and T from the loop name, e.g., "Loop_1_S1T34" -> "1", "34"
    parts <- unlist(strsplit(ln, "_"))
    c(gsub("S", "", parts[3]), gsub("T", "", parts[4]))
  })))

  p <- p + geom_point(data = filter(node_df, name %in% all_loop_st_nodes),
             aes(x = x, y = y), color = "red", size = 6, shape = 8)
  
  # Add labels for the first S and T nodes sampled (if available)
  if (!is.na(first_S_node_for_label) && !is.na(first_T_node_for_label)) {
    p <- p + geom_text(data = filter(node_df, name == first_S_node_for_label), aes(x = x, y = y, label = "S"), 
              nudge_y = 0.08, color = "red", size = 4) +
             geom_text(data = filter(node_df, name == first_T_node_for_label), aes(x = x, y = y, label = "T"), 
              nudge_y = 0.08, color = "red", size = 4)
  }
}
  
p <- p + labs(title = "Zachary's Karate Club: Communities, S-T-S Cycles, and Total GWN",
       subtitle = paste0("Node size corresponds to Total GWN. S-T-S Cycles limited to ", max_loops_to_consider, " loops with path cutoff = ", path_cutoff, ". Layout is arbitrary for demo."),
       x = "X Coordinate", y = "Y Coordinate",
       size = "Total GWN", color = "Community") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5))

print(p)
for (i in seq_along(selected_loops)) {
  message(paste0("  ", names(selected_loops)[i], ": ", paste(selected_loops[[i]], collapse = " - ")))
}

as.data.frame(community_gwn_summary)
as.data.frame(node_data)
# You can save the plot
# ggsave("zachary_gwn_analysis.png", plot = p, width = 10, height = 8, dpi = 300)

####################### bologna ok

# --- 0. Install and Load Necessary Packages ---
# You only need to run install.packages() once per package on your system.
# install.packages("osmdata")
# install.packages("sf")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("sfnetworks")
# install.packages("tidygraph")
setwd("C:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
library(osmdata)
library(sf)
library(ggplot2)
library(dplyr)
library(sfnetworks)
library(tidygraph)

# --- 1. Efficient Data Acquisition for Bologna (Expanded Area) ---

message("Step 1: Acquiring OSM data for Bologna (expanded area)...")

# Define a much wider bounding box for Greater Bologna, encompassing the Tangenziale
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # [min_lon, min_lat, max_lon, max_lat]
names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")

# Query for all major roads within this wider area, including motorway types and links
message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

# Use these combined lines directly for network creation
all_network_lines <- all_roads_osm$osm_lines %>%
  filter(sf::st_is_valid(.)) 

message(paste("  Downloaded", nrow(all_network_lines), "road segments from the wider Bologna area."))


# --- 2. Convert sf Data to an sfnetwork object ---

message("Step 2: Converting sf data to sfnetwork object...")

# Create an sfnetwork object. This object IS your graph.
network_sfn <- sfnetworks::as_sfnetwork(all_network_lines, directed = FALSE)

# Add sequential 'name' attribute to nodes - NOW KEPT AS INTEGER
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(name = dplyr::row_number()) # Removed as.character()

message(paste("  Created sfnetwork object with", network_sfn %>% tidygraph::activate("nodes") %>% nrow(), "nodes and", network_sfn %>% tidygraph::activate("edges") %>% nrow(), "edges."))
message("  Nodes in the sfnetwork object now have 'name' (sequential ID) and 'x', 'y' coordinates.")


# --- Plotting for S/T Node Selection ---

message("\nPlotting network with node numbers for S/T node selection...")

# Extract node and edge data frames for plotting
node_df_plot <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(x = sf::st_coordinates(geometry)[, "X"], 
                y = sf::st_coordinates(geometry)[, "Y"]) %>%
  sf::st_drop_geometry() %>% 
  dplyr::as_tibble() %>% 
  dplyr::select(name, x, y) 

edge_df_plot <- network_sfn %>%
  tidygraph::activate("edges") %>%
  dplyr::as_tibble() %>%
  dplyr::left_join(node_df_plot %>% dplyr::select(name, x_from = x, y_from = y), by = c("from" = "name")) %>%
  dplyr::left_join(node_df_plot %>% dplyr::select(name, x_to = x, y_to = y), by = c("to" = "name")) %>%
  dplyr::select(from, to, x = x_from, y = y_from, xend = x_to, yend = y_to)

# Create the plot
plot_s_t_selection <- ggplot2::ggplot() +
  # Plot edges
  ggplot2::geom_segment(data = edge_df_plot, ggplot2::aes(x = x, y = y, xend = xend, yend = yend),
                        color = "gray50", size = 0.2, alpha = 0.7) + 
  # Plot nodes
  ggplot2::geom_point(data = node_df_plot, ggplot2::aes(x = x, y = y),
                      color = "red", size = 0.5, alpha = 0.8) + 
  # Label nodes with their 'name' (sequential ID)
  ggplot2::geom_text(data = node_df_plot, ggplot2::aes(x = x, y = y, label = name),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  ggplot2::labs(title = "Bologna Network: Nodes with Sequential IDs for S/T Selection",
                subtitle = paste0("Total nodes: ", nrow(node_df_plot), ", Total edges: ", nrow(edge_df_plot), "\nZoom in heavily to see node numbers clearly."),
                x = "Longitude", y = "Latitude") +
  ggplot2::theme_minimal() +
  ggplot2::coord_sf() 

print(plot_s_t_selection)

message("\nInspect the plot to find the 'name' (number) of your desired S and T nodes on the Tangenziale.")
message("You will likely need to zoom in very far on the plot to distinguish individual node numbers.")
message("Once found, update the 'S_node_igraph_name' and 'T_node_igraph_name' variables in Step 3 of the main script, then run the full script.")
 ggsave("bologna1.png", plot = plot_s_t_selection, width = 10, height = 8, dpi = 300)


#########################################################

# --- 0. Install and Load Necessary Packages ---
# You only need to run install.packages() once per package on your system.
# install.packages("osmdata")
# install.packages("sf") # Make sure this is updated!
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("sfnetworks")
# install.packages("tidygraph")

library(osmdata)
library(sf)
library(ggplot2)
library(dplyr)
library(sfnetworks)
library(tidygraph)
library(igraph) 

# Check sf package version (already confirmed OK, but keep for completeness)
message(paste0("  sf package version: ", packageVersion("sf")))
if (packageVersion("sf") < "0.9.0") {
  stop("ERROR: Your 'sf' package is too old. 'st_is_closed' requires sf >= 0.9.0. Please update your 'sf' package using install.packages('sf') and then restart R.")
}

# --- 1. Efficient Data Acquisition for Bologna (Expanded Area) ---

message("Step 1: Acquiring OSM data for Bologna (expanded area)...")

# Define a much wider bounding box for Greater Bologna, encompassing the Tangenziale
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # [min_lon, min_lat, max_lon, max_lat]
names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")

# Query for all major roads within this wider area, including motorway types and links
message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

# Use these combined lines directly for network creation
all_network_lines <- all_roads_osm$osm_lines %>%
  filter(sf::st_is_valid(.)) 

message(paste("  Downloaded", nrow(all_network_lines), "road segments from the wider Bologna area."))

######
# --- 1. Data Acquisition from OpenStreetMap ---
message("Step 1: Acquiring data from OpenStreetMap...")

# Define the bounding box for wider Bologna (these coordinates are good)
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # min_lon, min_lat, max_lon, max_lat

# Use osmdata to get a broader set of major roads within the bbox first
# We'll filter these down to just the Tangenziale-specific roads.
all_major_roads_osm <- osmdata::opq(bologna_wider_bbox) %>%
  osmdata::add_osm_feature(key = 'highway', 
                           value = c('motorway', 'motorway_link', 'trunk', 'trunk_link', 'primary', 'secondary')) %>%
  osmdata::osmdata_sf()

# Convert the acquired data to an sf object (lines) for easier dplyr filtering
all_major_roads_lines <- all_major_roads_osm$osm_lines

# --- IMPORTANT FILTERING STEP FOR TANGENZIALE ---
# Now, filter these major roads to specifically target the Bologna Tangenziale.
# We look for common OSM tags/names/refs associated with it.
tangenziale_lines <- all_major_roads_lines %>%
  dplyr::filter(
    (highway %in% c("motorway", "motorway_link", "trunk", "trunk_link")) & # Must be a major highway type
    ( (ref == "RA 1") | (name == "Tangenziale di Bologna") | (stringr::str_detect(name, "Raccordo Autostradale 1")) ) # Check for specific identifiers
  ) %>%
  # Optionally, remove very short segments that might be irrelevant ramps or noise,
  # but be careful not to remove critical small connections.
  # For now, let's keep it simple.
  # Filter out any NA geometries that might result from incomplete OSM data
  dplyr::filter(!sf::st_is_empty(geometry)) %>%
  dplyr::mutate(edge_geom = geometry) # Store original geometry for later in sfnetwork conversion

# Check if any lines were found
if (nrow(tangenziale_lines) == 0) {
  stop("ERROR: No Tangenziale-specific road lines found with the current filters.
        Please verify OSM tags for Bologna Tangenziale or broaden the search.")
}

message(paste0("  Downloaded and filtered down to ", nrow(tangenziale_lines), " Tangenziale-specific road segments."))

# --- 2. Convert sf Data to an sfnetwork object ---

message("Step 2: Converting sf data to sfnetwork object...")

# Create an sfnetwork object. This object IS your graph.
#network_sfn <- sfnetworks::as_sfnetwork(all_network_lines, directed = FALSE)

##
# --- 2. Convert to sfnetwork and Simplify Graph ---
message("Step 2: Converting to sfnetwork and simplifying graph...")

# Convert the *filtered* Tangenziale lines to an sfnetwork object
network_sfn <- sfnetworks::as_sfnetwork(tangenziale_lines, directed = FALSE, edges_as_lines = TRUE)

##
# Add sequential 'name' attribute to nodes - KEPT AS INTEGER
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(name = dplyr::row_number()) 

message(paste("  Created sfnetwork object with", network_sfn %>% tidygraph::activate("nodes") %>% nrow(), "nodes and", network_sfn %>% tidygraph::activate("edges") %>% nrow(), "edges."))
message("  Nodes in the sfnetwork object now have 'name' (sequential ID) and 'x', 'y' coordinates.")


# --- 3. Manually Define Specific Cycles (Tangenziale Example) ---

message("Step 3: Defining specific cycles (COORDINATE-BASED S/T node selection and two shortest paths method)...")

# --- CRITICAL FIX: Convert sfnetwork to an igraph object for direct igraph functions ---
graph_igraph_original <- as.igraph(network_sfn) # Keep original for reference


# --- Identify and work with the largest connected component ---
message("\n  Checking graph connectivity and identifying largest component...")
if (!igraph::is.connected(graph_igraph_original)) {
  message("  Graph is disconnected. Extracting the largest connected component for pathfinding...")
  
  components_result <- igraph::components(graph_igraph_original)
  largest_comp_id <- which.max(components_result$csize)
  
  # Get the internal igraph vertex indices of nodes that belong to the largest component
  nodes_in_largest_comp_indices <- which(components_result$membership == largest_comp_id)
  
  # Create a subgraph containing only the largest component
  graph_igraph <- igraph::induced_subgraph(graph_igraph_original, vids = nodes_in_largest_comp_indices)
  
  message(paste0("  Original graph has ", igraph::vcount(graph_igraph_original), " nodes and ", components_result$no, " components."))
  message(paste0("  Largest component has ", igraph::vcount(graph_igraph), " nodes (", 
                 round(igraph::vcount(graph_igraph) / igraph::vcount(graph_igraph_original) * 100, 2), "% of original nodes)."))
  
} else {
  message("  Graph is connected. Proceeding with the full graph.")
  graph_igraph <- graph_igraph_original # Use the original if it's connected
}


# --- IMPORTANT MANUAL STEP: Provide approximate coordinates for S and T nodes ---
point_S_coord <- c(lon = 11.380, lat = 44.505) # Example: Near Bologna Fiere/Arcoveggio exit
point_T_coord <- c(lon = 11.270, lat = 44.480) # Example: Near Casalecchio di Reno exit

# Please use these updated coordinates (or your own more precise ones from the map):
point_S_coord <- c(lon = 11.390, lat = 44.520) # Example: North-East Tangenziale (near Exit 9)
point_T_coord <- c(lon = 11.290, lat = 44.475) # Example: South-West Tangenziale (near Exit 1)

message(paste0("  Searching for graph nodes closest to S_coord (Lon: ", point_S_coord["lon"], ", Lat: ", point_S_coord["lat"], ")"))
message(paste0("  Searching for graph nodes closest to T_coord (Lon: ", point_T_coord["lon"], ", Lat: ", point_T_coord["lat"], ")"))

# Convert coordinates to sf POINT objects, ensuring they have the same CRS as the network
sf_point_S <- sf::st_sfc(sf::st_point(point_S_coord), crs = sf::st_crs(network_sfn))
sf_point_T <- sf::st_sfc(sf::st_point(point_T_coord), crs = sf::st_crs(network_sfn))

# Extract the nodes as an sf object for spatial operations
nodes_sf_geom <- network_sfn %>% tidygraph::activate("nodes") %>% sf::st_as_sf()

# Filter nodes_sf_geom to only include nodes from the current 'graph_igraph' (largest component if applicable)
# Get the 'name' attributes (sequential IDs) of nodes in the current graph_igraph
nodes_in_current_graph_names <- as.integer(igraph::V(graph_igraph)$name)
nodes_sf_geom_filtered <- nodes_sf_geom %>%
    dplyr::filter(name %in% nodes_in_current_graph_names)

# Find the closest node in the *filtered* network to the provided coordinates
S_node_igraph_index_in_filtered <- sf::st_nearest_feature(sf_point_S, nodes_sf_geom_filtered)
T_node_igraph_index_in_filtered <- sf::st_nearest_feature(sf_point_T, nodes_sf_geom_filtered)

# Get the 'name' (sequential ID) of these closest nodes
S_node_igraph_name <- nodes_sf_geom_filtered$name[S_node_igraph_index_in_filtered]
T_node_igraph_name <- nodes_sf_geom_filtered$name[T_node_igraph_index_in_filtered]

message(paste0("  Selected S node (ID: ", S_node_igraph_name, ") closest to provided coordinates."))
message(paste0("  Selected T node (ID: ", T_node_igraph_name, ") closest to provided coordinates."))


# --- NEW PATHFINDING STRATEGY: Two Shortest Paths ---
message("\n  Attempting to find an edge-disjoint loop using two shortest paths (S->T then T->S)...")

# Assign initial edge weights (all 1 for unweighted shortest path)
igraph::E(graph_igraph)$weight <- 1

# 1. Find Shortest Path from S to T (Path 1)
path1_result <- igraph::shortest_paths(graph_igraph, 
                                       from = igraph::V(graph_igraph)[S_node_igraph_name], 
                                       to = igraph::V(graph_igraph)[T_node_igraph_name], 
                                       mode = "all", 
                                       output = "both") # Get both vertices and edges

if (length(path1_result$vpath[[1]]) == 0 || length(path1_result$epath[[1]]) == 0) {
  stop("ERROR: No path found from S to T in the initial graph. Check S/T nodes or network density.")
}

path1_nodes_igraph_indices <- as.integer(path1_result$vpath[[1]])
path1_edges_igraph_indices <- as.integer(path1_result$epath[[1]]) # These are internal igraph edge IDs

message(paste0("  Found Path 1 (S->T) with ", length(path1_nodes_igraph_indices), " nodes and ", length(path1_edges_igraph_indices), " edges."))

# 2. Assign infinite weight to edges on Path 1 to "remove" them for the next search
temp_weights <- igraph::E(graph_igraph)$weight # Get current weights (which are all 1)
temp_weights[path1_edges_igraph_indices] <- Inf # Set weights of these specific edges to infinity

# 3. Find Shortest Path from T to S on the modified graph (Path 2)
path2_result <- igraph::shortest_paths(graph_igraph, 
                                       from = igraph::V(graph_igraph)[T_node_igraph_name], 
                                       to = igraph::V(graph_igraph)[S_node_igraph_name], 
                                       mode = "all", 
                                       weights = temp_weights, # Use the modified weights here
                                       output = "both")

if (length(path2_result$vpath[[1]]) == 0 || length(path2_result$epath[[1]]) == 0) {
  stop("ERROR: No second (edge-disjoint) path found from T to S. This means no edge-disjoint loop can be formed between S and T. Try different S/T nodes or expand search area.")
}

path2_nodes_igraph_indices <- as.integer(path2_result$vpath[[1]])
path2_edges_igraph_indices <- as.integer(path2_result$epath[[1]])

message(paste0("  Found Path 2 (T->S) with ", length(path2_nodes_igraph_indices), " nodes and ", length(path2_edges_igraph_indices), " edges."))
# Check for edge-disjoint paths
  common_edges <- intersect(path1_edges_igraph_indices, path2_edges_igraph_indices)

  # This is the specific line that prints the number of common edges:
  message(paste0("  DEBUG: Number of common edges between paths: ", length(common_edges)))

# 4. Combine Path 1 and Path 2 to form the loop
# The loop will be: S -> ... (Path 1) ... -> T -> ... (Path 2) ... -> S
# We need the node sequence from Path 1, followed by the nodes from Path 2 (excluding the first node of Path 2, which is T, as it's the last node of Path 1).
ordered_cycle_nodes_igraph_indices <- c(path1_nodes_igraph_indices, path2_nodes_igraph_indices[-1]) 

# Get the 'name' (sequential ID) of the nodes in the cycle
first_tangenziale_loop_nodes <- as.integer(igraph::V(graph_igraph)[ordered_cycle_nodes_igraph_indices]$name)

# Also get the combined edges that form the loop
selected_loop_edges_igraph_indices <- unique(c(path1_edges_igraph_indices, path2_edges_igraph_indices))

# Store the loop (only one, as this method finds a specific one)
selected_loops <- list()
selected_loops[["Tangenziale_Segment_Loop_1"]] <- first_tangenziale_loop_nodes

message(paste0("  Successfully identified a Tangenziale loop with ", length(first_tangenziale_loop_nodes), " nodes and ", 
               length(selected_loop_edges_igraph_indices), " edges."))


# --- 4. Geometric Winding Number (GWN) Calculation ---
message("\nStep 4: Calculating Geometric Winding Number (GWN)...")

gwn_area <- NA 
if (length(selected_loops) > 0) {
  # Get node coordinates for the selected loop. Ensure these are from the original network_sfn
  # as 'graph_igraph' might be a subgraph and not carry full sf features directly.

  # --- CRITICAL FIX: Ensure the loop_node_coords_df contains all 60 points in sequence ---
  # The issue was that filter() + arrange() was producing only unique nodes,
  # causing the closing point (which is the same as the start point) to be absent.
  
  # 1. Get unique coordinates for all involved nodes first
  unique_loop_node_coords_df <- network_sfn %>%
    tidygraph::activate("nodes") %>%
    dplyr::filter(name %in% unique(first_tangenziale_loop_nodes)) %>% # Only filter for unique node IDs first
    dplyr::mutate(x = sf::st_coordinates(geometry)[, "X"],
                  y = sf::st_coordinates(geometry)[, "Y"]) %>%
    sf::st_drop_geometry() %>%
    dplyr::as_tibble() %>%
    dplyr::select(name, x, y)

  # --- DIAGNOSTIC PRINT 1 (for your verification) ---
  message(paste0("  DEBUG 1: Dimensions of unique_loop_node_coords_df (should be # of unique nodes): ", paste(dim(unique_loop_node_coords_df), collapse = "x")))
  message("  DEBUG 1: Head of unique_loop_node_coords_df:")
  print(head(unique_loop_node_coords_df))
  message("  DEBUG 1: Tail of unique_loop_node_coords_df:")
  print(tail(unique_loop_node_coords_df))
  # --- END DIAGNOSTIC PRINT 1 ---


  # 2. Join these unique coordinates back to the *full, ordered sequence* of nodes (first_tangenziale_loop_nodes)
  # This ensures that if the start node ID appears again at the end, its coordinates are duplicated.
  loop_node_coords_df <- tibble::tibble(name = first_tangenziale_loop_nodes) %>%
    dplyr::left_join(unique_loop_node_coords_df, by = "name")

  # --- DIAGNOSTIC PRINT 2 (for your verification) ---
  message(paste0("  DEBUG 2: Length of first_tangenziale_loop_nodes (expected 60): ", length(first_tangenziale_loop_nodes)))
  message(paste0("  DEBUG 2: Dimensions of loop_node_coords_df AFTER JOIN (expected 60x3): ", paste(dim(loop_node_coords_df), collapse = "x")))
  message("  DEBUG 2: Head of loop_node_coords_df:")
  print(head(loop_node_coords_df))
  message("  DEBUG 2: Tail of loop_node_coords_df:")
  print(tail(loop_node_coords_df))
  # --- END DIAGNOSTIC PRINT 2 ---

  # Create LINESTRING from ordered nodes, then POLYGON
  # This should now be a truly closed linestring because loop_node_coords_df has the start node duplicated at the end.
  loop_linestring <- loop_node_coords_df %>% 
    sf::st_as_sf(coords = c("x", "y"), crs = sf::st_crs(network_sfn)) %>%
    sf::st_combine() %>%
    sf::st_cast("LINESTRING")

  # Now, because loop_node_coords_df should explicitly contain the closing point,
  # the st_force_closed() might become redundant, but it's good for robustness against floating point errors.
  tangenziale_polygon <- loop_linestring %>%
#    sf::st_force_closed() %>%  # Ensures the linestring is truly closed (important for any floating point issues)
    sf::st_cast("POLYGON")     # Directly cast to a polygon

  # Check if the polygon creation was successful (st_cast will return empty if it fails to form)
  if (!is.null(tangenziale_polygon) && !sf::st_is_empty(tangenziale_polygon)) { 
    gwn_area <- sf::st_area(tangenziale_polygon)
    message(paste0("  Approximate area enclosed by Tangenziale loop: ", round(as.numeric(gwn_area), 2), " square meters."))
  } else {
    message("  WARNING: Could not form a valid closed polygon from the detected loop nodes for GWN calculation.")
    message("  This might be due to the path not truly forming a simple polygon (e.g., self-intersections), or other geometric issues.")
    gwn_area <- NA # Ensure gwn_area is set to NA if polygon failed
  }

} else {
  message("  No loops found, skipping GWN calculation.")
}

# --- 5. Calculate Centrality Measures ---
message("\nStep 5: Calculating Centrality Measures...")

# Calculate centrality on the FULL original network_sfn for a comprehensive view
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(degree = tidygraph::centrality_degree())

message("  Calculating Betweenness Centrality (this may take a while for large networks)...")
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(betweenness = tidygraph::centrality_betweenness())
message("  Betweenness Centrality calculation complete.")

node_df <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(x = sf::st_coordinates(geometry)[, "X"],
                y = sf::st_coordinates(geometry)[, "Y"]) %>%
  sf::st_drop_geometry()

message("  Centrality measures calculated and stored in node_df.")


# --- 6. Integrate and Visualize the Results ---
message("\nStep 6: Integrating and Visualizing Results...")

# 6.1 Visualize the Network with Node Centrality (e.g., Betweenness)
plot_centrality <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = sf::st_as_sf(network_sfn, "edges"), 
                   color = "gray80", size = 0.3, alpha = 0.7) +
  ggplot2::geom_point(data = node_df, ggplot2::aes(x = x, y = y, color = betweenness, size = degree), 
                       alpha = 0.7) +
  ggplot2::scale_color_viridis_c(option = "magma", trans = "log1p", name = "Betweenness Centrality") +
  ggplot2::scale_size_continuous(range = c(0.5, 3), name = "Degree Centrality") +
  ggplot2::labs(title = "Bologna Road Network: Node Centrality",
                subtitle = "Node color and size based on Betweenness and Degree Centrality",
                x = "Longitude", y = "Latitude") +
  ggplot2::theme_minimal()

print(plot_centrality)
message("  Network centrality plot generated.")

ggsave("bologna1_cen.png", plot = plot_centrality, width = 10, height = 8, dpi = 300)

# 6.2 Visualize the Defined Loop on the Network
if (length(selected_loops) > 0) {
  first_loop_nodes_vector <- selected_loops[[1]]
  
  # Get the actual edges corresponding to the selected_loop_edges_igraph_indices
  # from the original network_sfn
  
  # Create a data frame of the selected edges including their geometries
  # Need to map igraph edge IDs back to sfnetwork edge rows.
  
  # Get the 'from' and 'to' node names for the selected igraph edges
  loop_edges_igraph_df <- igraph::as_data_frame(graph_igraph, what = "edges") %>%
      dplyr::mutate(igraph_edge_id = 1:igraph::ecount(graph_igraph)) %>%
      dplyr::filter(igraph_edge_id %in% selected_loop_edges_igraph_indices) %>%
      dplyr::select(from, to) # These are the 'name' (sequential IDs) of nodes

  # Now, filter the original sfnetwork edges using these 'from' and 'to' nodes
  loop_edges_sf <- network_sfn %>% 
      tidygraph::activate("edges") %>% 
      sf::st_as_sf() %>%
      dplyr::inner_join(loop_edges_igraph_df, by = c("from", "to")) # This correctly handles multi-edges

  plot_loop <- ggplot2::ggplot() +
    ggplot2::geom_sf(data = sf::st_as_sf(network_sfn, "edges"), 
                     color = "gray80", size = 0.3, alpha = 0.7) + 
    ggplot2::geom_sf(data = loop_edges_sf, 
                     color = "darkgreen", size = 1, alpha = 0.9) + 
    ggplot2::geom_point(data = node_df %>% dplyr::filter(name %in% first_tangenziale_loop_nodes),
                         ggplot2::aes(x = x, y = y), 
                         color = "darkgreen", size = 1.5, shape = 16, alpha = 0.9) + 
    ggplot2::geom_point(data = node_df %>% dplyr::filter(name %in% c(S_node_igraph_name, T_node_igraph_name)),
                         ggplot2::aes(x = x, y = y), 
                         color = "orange", size = 3, shape = 8, 
                         stroke = 1.5) + 
    ggplot2::labs(title = "Bologna Road Network: Identified Tangenziale Loop",
                  subtitle = paste0("Loop between Node ", S_node_igraph_name, " and Node ", T_node_igraph_name, 
                                    ". Area: ", ifelse(is.na(gwn_area), "N/A", round(as.numeric(gwn_area), 2)), " sq m."),
                  x = "Longitude", y = "Latitude") +
    ggplot2::theme_minimal()

  print(plot_loop)
  message("  Tangenziale loop plot generated.")

} else {
  message("  No loop to visualize. Ensure loop detection was successful.")
}

message("\nScript finished successfully (assuming loops were found).")

44.527109, 11.356121
44.524906, 11.358370


##############################################################
####################### bologna ok

# una sintesis
#leo bologna, creo red, algo centrality y plot (el png es mejor?)
# --- 0. Install and Load Necessary Packages ---
# You only need to run install.packages() once per package on your system.
# install.packages("osmdata")
# install.packages("sf")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("sfnetworks")
# install.packages("tidygraph")
setwd("D:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
library(osmdata)
library(sf)
library(ggplot2)
library(dplyr)
library(sfnetworks)
library(tidygraph)

# --- 1. Efficient Data Acquisition for Bologna (Expanded Area) ---

message("Step 1: Acquiring OSM data for Bologna (expanded area)...")

# Define a much wider bounding box for Greater Bologna, encompassing the Tangenziale
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # [min_lon, min_lat, max_lon, max_lat]
names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")

# Query for all major roads within this wider area, including motorway types and links
message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

# Use these combined lines directly for network creation
all_network_lines <- all_roads_osm$osm_lines %>%
  filter(sf::st_is_valid(.)) 

message(paste("  Downloaded", nrow(all_network_lines), "road segments from the wider Bologna area."))


# --- 2. Convert sf Data to an sfnetwork object ---

message("Step 2: Converting sf data to sfnetwork object...")

# Create an sfnetwork object. This object IS your graph.
network_sfn <- sfnetworks::as_sfnetwork(all_network_lines, directed = FALSE)

# Add sequential 'name' attribute to nodes - NOW KEPT AS INTEGER
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(name = dplyr::row_number()) # Removed as.character()

message(paste("  Created sfnetwork object with", network_sfn %>% tidygraph::activate("nodes") %>% nrow(), "nodes and", network_sfn %>% tidygraph::activate("edges") %>% nrow(), "edges."))
message("  Nodes in the sfnetwork object now have 'name' (sequential ID) and 'x', 'y' coordinates.")


# --- Plotting for S/T Node Selection ---

message("\nPlotting network with node numbers for S/T node selection...")

# Extract node and edge data frames for plotting
node_df_plot <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(x = sf::st_coordinates(geometry)[, "X"], 
                y = sf::st_coordinates(geometry)[, "Y"]) %>%
  sf::st_drop_geometry() %>% 
  dplyr::as_tibble() %>% 
  dplyr::select(name, x, y) 

edge_df_plot <- network_sfn %>%
  tidygraph::activate("edges") %>%
  dplyr::as_tibble() %>%
  dplyr::left_join(node_df_plot %>% dplyr::select(name, x_from = x, y_from = y), by = c("from" = "name")) %>%
  dplyr::left_join(node_df_plot %>% dplyr::select(name, x_to = x, y_to = y), by = c("to" = "name")) %>%
  dplyr::select(from, to, x = x_from, y = y_from, xend = x_to, yend = y_to)

# Create the plot
plot_s_t_selection <- ggplot2::ggplot() +
  # Plot edges
  ggplot2::geom_segment(data = edge_df_plot, ggplot2::aes(x = x, y = y, xend = xend, yend = yend),
                        color = "gray50", size = 0.2, alpha = 0.7) + 
  # Plot nodes
  ggplot2::geom_point(data = node_df_plot, ggplot2::aes(x = x, y = y),
                      color = "red", size = 0.5, alpha = 0.8) + 
  # Label nodes with their 'name' (sequential ID)
  ggplot2::geom_text(data = node_df_plot, ggplot2::aes(x = x, y = y, label = name),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  ggplot2::labs(title = "Bologna Network: Nodes with Sequential IDs for S/T Selection",
                subtitle = paste0("Total nodes: ", nrow(node_df_plot), ", Total edges: ", nrow(edge_df_plot), "\nZoom in heavily to see node numbers clearly."),
                x = "Longitude", y = "Latitude") +
  ggplot2::theme_minimal() +
  ggplot2::coord_sf() 

print(plot_s_t_selection)

message("\nInspect the plot to find the 'name' (number) of your desired S and T nodes on the Tangenziale.")
message("You will likely need to zoom in very far on the plot to distinguish individual node numbers.")
message("Once found, update the 'S_node_igraph_name' and 'T_node_igraph_name' variables in Step 3 of the main script, then run the full script.")
 ggsave("bologna1.png", plot = plot_s_t_selection, width = 10, height = 8, dpi = 300)

# --- CRITICAL FIX: Convert sfnetwork to an igraph object for direct igraph functions ---
graph_igraph_original <- as.igraph(network_sfn)

 # Keep original for reference


# --- Identify and work with the largest connected component ---
message("\n  Checking graph connectivity and identifying largest component...")
if (!igraph::is.connected(graph_igraph_original)) {
  message("  Graph is disconnected. Extracting the largest connected component for pathfinding...")
  
  components_result <- igraph::components(graph_igraph_original)
  largest_comp_id <- which.max(components_result$csize)
  
  # Get the internal igraph vertex indices of nodes that belong to the largest component
  nodes_in_largest_comp_indices <- which(components_result$membership == largest_comp_id)
  
  # Create a subgraph containing only the largest component
  graph_igraph <- igraph::induced_subgraph(graph_igraph_original, vids = nodes_in_largest_comp_indices)
  
  message(paste0("  Original graph has ", igraph::vcount(graph_igraph_original), " nodes and ", components_result$no, " components."))
  message(paste0("  Largest component has ", igraph::vcount(graph_igraph), " nodes (", 
                 round(igraph::vcount(graph_igraph) / igraph::vcount(graph_igraph_original) * 100, 2), "% of original nodes)."))
  
} else {
  message("  Graph is connected. Proceeding with the full graph.")
  graph_igraph <- graph_igraph_original # Use the original if it's connected
}

# --- 5. Calculate Centrality Measures ---
message("\nStep 5: Calculating Centrality Measures...")

# Calculate centrality on the FULL original network_sfn for a comprehensive view
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(degree = tidygraph::centrality_degree())

message("  Calculating Betweenness Centrality (this may take a while for large networks)...")
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(betweenness = tidygraph::centrality_betweenness())
message("  Betweenness Centrality calculation complete.")

node_df <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::as_tibble() %>%
  dplyr::mutate(x = sf::st_coordinates(geometry)[, "X"],
                y = sf::st_coordinates(geometry)[, "Y"]) %>%
  sf::st_drop_geometry()

message("  Centrality measures calculated and stored in node_df.")


# --- 6. Integrate and Visualize the Results ---
message("\nStep 6: Integrating and Visualizing Results...")

# 6.1 Visualize the Network with Node Centrality (e.g., Betweenness)
plot_centrality <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = sf::st_as_sf(network_sfn, "edges"), 
                   color = "gray80", size = 0.3, alpha = 0.7) +
  ggplot2::geom_point(data = node_df, ggplot2::aes(x = x, y = y, color = betweenness, size = degree), 
                       alpha = 0.7) +
  ggplot2::geom_text(data = node_df_plot, ggplot2::aes(x = x, y = y, label = name),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  ggplot2::scale_color_viridis_c(option = "magma", trans = "log1p", name = "Betweenness Centrality") +
  ggplot2::scale_size_continuous(range = c(0.5, 3), name = "Degree Centrality") +
  ggplot2::labs(title = "Bologna Road Network: Node Centrality",
                subtitle = "Node color and size based on Betweenness and Degree Centrality",
                x = "Longitude", y = "Latitude") +
  ggplot2::theme_minimal()

print(plot_centrality)
message("  Network centrality plot generated.")

 ggsave("bologna1cent.png", plot = plot_centrality, width = 10, height = 8, dpi = 300)
ggsave("bologna1cent.pdf", plot = plot_centrality, width = 10, height = 8, dpi = 300)


####### chatgpt bologna 1 poligono ok
####### chatgpt
####### chatgpt
####### chatgpt
####### chatgpt

library(sf)
library(sfnetworks)
library(tidygraph)
library(igraph)
library(dplyr)
library(ggplot2)
library(sp)
library(osmdata)

setwd("C:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")

# Step 1: Download street network (e.g. Münster) via osmdata
mun_sf <- osmdata::opq("Munster, Germany") %>%
  osmdata::add_osm_feature(key="highway",
value = c("motorway", "trunk", "primary", "secondary", "tertiary")) %>%
  osmdata::osmdata_sf()
streets <- mun_sf$osm_lines
network1="Munster"
######## bologna
# Define a much wider bounding box for Greater Bologna, encompassing the Tangenziale
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # [min_lon, min_lat, max_lon, max_lat]
names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")

# Query for all major roads within this wider area, including motorway types and links
message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()
network1="Bologna"

mun_sf <-all_roads_osm 
streets <- mun_sf$osm_lines
########
# Convert to sfnetwork
#net <- sfnetworks::sfn_asnetwork(streets, directed = FALSE)
net <- sfnetworks::as_sfnetwork(streets, directed = FALSE)
# Step 2: Compute layout (projected)
net <- activate(net, "nodes") %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2])

net <- net %>% 
  activate("nodes") %>% 
  mutate(node_id = row_number())

# Step 3: Choose or detect a loop cycle manually
# Example: a known city ring intersection IDs (replace with real node indices)

####
# Suppose these are your manually selected node IDs
#loop_nodes <- c(10, 22, 35, 48, 60, 75, 300,10)
#loop_nodes <- c(10,  60, 75, 300,10)
loop_nodes <- c(10,  30, 75, 300,10)
# Get node coordinates and keep node_id
coords <- net %>% 
  activate("nodes") %>% 
  as_tibble()

# Extract polygon from selected nodes
poly <- coords %>% filter(node_id %in% loop_nodes) %>% select(x, y)

########## se calcula cent ....
combined_points <- sf::st_combine(poly)

# 3. Cast the MULTIPOINT to a LINESTRING.
#    This connects the points in the order they appear.
#    It's crucial that your 'poly' sf object has the points in the desired sequence.
linestring_from_points <- sf::st_cast(combined_points, "LINESTRING")
final_polygon <- sf::st_cast(linestring_from_points, "POLYGON")

# Now, 'final_polygon' is your polygon object.
#print(final_polygon)
gwn_area <- sf::st_area(final_polygon)
gwn_area

####


coords <- coords %>%
  rowwise() %>%
  mutate(gwn = as.integer(sp::point.in.polygon(x, y, poly$x, poly$y) == 1 &&
                          !(node_id %in% loop_nodes)))

net <- net %>%
  activate(nodes) %>%
  mutate(name = as.character(node_id))  # Or whatever column identifies nodes



# Step 5: Compute centralities
graph <- as.igraph(net)

cent <- data.frame(
  name = V(graph)$name,
  degree = degree(graph, normalized = FALSE),
  betweenness = betweenness(graph, normalized = TRUE)
)

cent <- cent %>% rename(node_id = name)
cent <- cent %>% mutate(node_id = as.integer(node_id))
node_df <- coords %>% left_join(cent, by = "node_id")

windows()

# Step 6: Plot analysis
tempo <-ggplot(node_df) +
  geom_sf(data = streets, color="gray", size=0.3) +
ggplot2::geom_text(data = node_df, ggplot2::aes(x = x, y = y, label = node_id),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  geom_polygon(data = poly, aes(x = x, y = y), fill = NA, color = "blue", linetype = "dashed") +
  geom_point(aes(x = x, y = y, size = degree, color = factor(gwn))) +
#  geom_point(aes(x = x, y = y, size = 1, color = factor(gwn))) +

  theme_minimal() +
  scale_color_manual(values = c("lightgray","red"), name="GWN") +
  labs(title = "Nodes inside loop (red) & degree centrality (size)")

print(tempo)

ggsave(paste(network1,".pdf"), plot = tempo, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,".jpeg"), plot = tempo, width = 10, height = 8, dpi = 300)

####################################
windows()

tempo1 <-ggplot(node_df) +
  geom_sf(data = streets, color="gray", size=0.3) +
ggplot2::geom_text(data = node_df, ggplot2::aes(x = x, y = y, label = node_id),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  geom_polygon(data = poly, aes(x = x, y = y), fill = NA, color = "red", linewidth=1,linetype = "dashed") +
#  geom_point(aes(x = x, y = y, size = 0.5, color = "black")) +
ggplot2::geom_point(data = node_df, ggplot2::aes(x = x, y = y),
                      color = "black", size = 0.5, alpha = 0.1) + 
  theme_minimal() #+
#  scale_color_manual(values = c("lightgray","red"), name="GWN") +
#  labs(title = "Nodes inside loop (red) & degree centrality (size)")

print(tempo1)

ggsave(paste(network1,"x.pdf"), plot = tempo1, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"x.jpeg"), plot = tempo1, width = 10, height = 8, dpi = 300)

####### ok hasta aqui

############################################################################### no se
#################################################################
mun_sf <- osmdata::opq("Bologna, Italy") %>%
  osmdata::add_osm_feature(key="highway") %>%
  osmdata::osmdata_sf()
streets <- mun_sf$osm_lines


combined_points <- sf::st_combine(poly)

# 3. Cast the MULTIPOINT to a LINESTRING.
#    This connects the points in the order they appear.
#    It's crucial that your 'poly' sf object has the points in the desired sequence.
linestring_from_points <- sf::st_cast(combined_points, "LINESTRING")
final_polygon <- sf::st_cast(linestring_from_points, "POLYGON")

# Now, 'final_polygon' is your polygon object.
print(final_polygon)
gwn_area <- sf::st_area(final_polygon)
gwn_area
 






>  gwn_area <- sf::st_area(final_polygon)
>  gwn_area 
###################################################################### fin no se

#################################### para tangenziale ok
network_sfn <- net

# View the first few rows of edge attributes
network_sfn %>%
  activate("edges") %>%
  as_tibble() %>%
  head()

# To see all unique road names (might be very long)
# network_sfn %>%
#   activate("edges") %>%
#   as_tibble() %>%
#   distinct(name) %>%
#   pull(name) # Extract the column as a vector

# To see unique road references (e.g., "RA1" for Tangenziale)
 network_sfn %>%
   activate("edges") %>%
   as_tibble() %>%
  distinct(ref) %>%
  pull(ref)

# To see unique highway types (e.g., "motorway", "primary")
# network_sfn %>%
#   activate("edges") %>%
#   as_tibble() %>%
#   distinct(highway) %>%
#   pull(highway)



# --- EXAMPLE 2: Highlight a road by its reference (e.g., the Tangenziale itself, if 'ref' is available) ---
selected_road_ref <- "RA1" # This is likely the Tangenziale's reference
highlighted_road_sf <- network_sfn %>%
  activate("edges") %>%
  filter(ref == selected_road_ref) %>%
  st_as_sf()

# Check if the selected road was found
if (nrow(highlighted_road_sf) == 0) {
  warning("The selected road(s) could not be found with the given criteria. Plotting without highlighted road.")
  highlighted_road_sf <- NULL # Set to NULL to prevent plotting if not found
} else {
  message(paste0("  Found ", nrow(highlighted_road_sf), " segments for the selected road(s)."))
}


plot_title_loop <- "Bologna Road Network: Identified Tangenziale Loop"
tangenziale_polygon_geom <- highlighted_road_sf


ggplot() +
  # Plot all network edges in gray (background)
  geom_sf(data = network_sfn %>% activate("edges") %>% st_as_sf(),
          color = "gray", linewidth = 0.3, alpha = 0.5) +

#  # Plot the identified Tangenziale loop's polygon (filled)
  {if (!is.null(tangenziale_polygon_geom)) geom_sf(data = tangenziale_polygon_geom, fill = "lightgreen", alpha = 0.3)} +

  # Plot the identified Tangenziale loop's linestring (outline)
#  {if (!is.null(loop_linestring_geom)) geom_sf(data = loop_linestring_geom, color = "darkgreen", linewidth = 1.2)} +

  # --- Add the HIGHLIGHTED ROAD layer here ---
#  {if (!is.null(highlighted_road_sf)) geom_sf(data = highlighted_road_sf, color = "orange", linewidth = 1.5, linetype = "solid")} +

  # Plot nodes with centrality measures (color by degree, size by betweenness)
#  geom_sf(data = network_sfn %>% activate("nodes") %>% st_as_sf(),
#          aes(color = degree, size = betweenness),
#          alpha = 0.7) +
#  scale_color_viridis_c(option = "B", direction = -1, name = "Degree Centrality") +
#  scale_size_area(max_size = 5, name = "Betweenness Centrality") +

  # Plot the S and T points (origin and destination for pathfinding)
#  geom_sf(data = st_sfc(st_point(point_S_coord), crs = st_crs(network_sfn)),
#          color = "red", shape = 8, size = 5, stroke = 1.5) + # Red star for S
#  geom_sf(data = st_sfc(st_point(point_T_coord), crs = st_crs(network_sfn)),
#          color = "red", shape = 8, size = 5, stroke = 1.5) + # Red star for T

  # Add labels and theme
  labs(title = plot_title_loop, 
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5))





windows()

tempo2 <-ggplot(node_df) +
  geom_sf(data = streets, color="gray", size=0.3) +
ggplot2::geom_text(data = node_df, ggplot2::aes(x = x, y = y, label = node_id),
                     size = 1.5, 
                     color = "blue",
                     nudge_x = 0.0001, nudge_y = 0.0001, 
                     check_overlap = TRUE) + 
  geom_polygon(data = poly, aes(x = x, y = y), fill = NA, color = "red", linewidth=1,linetype = "dashed") +
#  geom_point(aes(x = x, y = y, size = 0.5, color = "black")) +
{if (!is.null(highlighted_road_sf)) geom_sf(data = highlighted_road_sf, color = "orange", linewidth = 1.5, linetype = "solid")} +
ggplot2::geom_point(data = node_df, ggplot2::aes(x = x, y = y),
                      color = "black", size = 0.5, alpha = 0.1) + 
  theme_minimal() #+
#  scale_color_manual(values = c("lightgray","red"), name="GWN") +
#  labs(title = "Nodes inside loop (red) & degree centrality (size)")

print(tempo2)

ggsave(paste(network1,"x2.pdf"), plot = tempo2, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"x2.jpeg"), plot = tempo2, width = 10, height = 8, dpi = 300)


######################## power 

library(osmdata)
library(sf)
library(ggplot2) # For plotting

# Define a bounding box for Emilia-Romagna (approximate)
# min_lon, min_lat, max_lon, max_lat
emilia_romagna_bbox <- c(9.3, 43.8, 12.8, 45.1)<- c(9.3, 43.8, 12.8, 45.1)


message("Querying OpenStreetMap for electric power system data in Emilia-Romagna...")

# --- Option 1: Query for ALL power features (might be very large) ---
# This will return points, lines, and polygons for any 'power' tag
# power_osm_all <- opq(emilia_romagna_bbox) %>%
#   add_osm_feature(key = "power") %>%
#   osmdata_sf()
# message(paste0("  Downloaded all 'power' features. Points: ", nrow(power_osm_all$osm_points),
#                ", Lines: ", nrow(power_osm_all$osm_lines),
#                ", Polygons: ", nrow(power_osm_all$osm_polygons)))

# --- Option 2: More targeted query for specific components ---
# This is usually more practical. Let's focus on major power lines and substations.

# Query for power lines (overhead and underground)
power_lines_osm <- opq(emilia_romagna_bbox) %>%
  add_osm_feature(key = "power", value = c("line", "cable", "minor_line")) %>%
  osmdata_sf()

# Extract power line geometries (these are sf LINESTRINGs)
power_lines_sf <- power_lines_osm$osm_lines

if (!is.null(power_lines_sf)) {
  message(paste0("  Found ", nrow(power_lines_sf), " power lines."))
#   Optionally filter by voltage, e.g., for high-voltage lines
   power_lines_hv <- power_lines_sf %>% filter(stringr::str_detect(voltage, "132000|220000|380000"))
   power_lines_hv <- power_lines_sf %>% filter(stringr::str_detect(voltage, "380000"))
   message(paste0("  Found ", nrow(power_lines_hv), " high-voltage power lines."))
} else {
  message("  No power lines found for the specified area/tags.")
}


# Query for substations (major nodes in the power grid)
power_substations_osm <- opq(emilia_romagna_bbox) %>%
  add_osm_feature(key = "power", value = "substation") %>%
  osmdata_sf()

# Extract substation geometries (these are usually sf POINTS or POLYGONS)
power_substations_sf <- power_substations_osm$osm_points # Often tagged as points
if (!is.null(power_substations_sf)) {
  message(paste0("  Found ", nrow(power_substations_sf), " power substations (as points)."))
} else {
  message("  No power substations found for the specified area/tags.")
}

# You might also want power plants
# power_plants_osm <- opq(emilia_romagna_bbox) %>%
#   add_osm_feature(key = "power", value = "plant") %>%
#   osmdata_sf()
# power_plants_sf <- power_plants_osm$osm_points # Or osm_polygons
# if (!is.null(power_plants_sf)) {
#   message(paste0("  Found ", nrow(power_plants_sf), " power plants."))
# }

message("Power system data acquisition complete.")

# --- Basic Visualization (Optional) ---
if (!is.null(power_lines_sf) || !is.null(power_substations_sf)) {
  message("\nPlotting acquired power system data...")
  plot_power <- ggplot() +
    {if(!is.null(power_lines_sf)) geom_sf(data = power_lines_sf, aes(color = power), linewidth = 0.5)} +
    {if(!is.null(power_substations_sf)) geom_sf(data = power_substations_sf, color = "darkblue", size = 1, shape = 15)} + # Squares for substations
    labs(title = "Electric Power System in Emilia-Romagna (OSM Data)",
         subtitle = "Blue squares: Substations; Colored lines: Power Lines") +
    theme_minimal()
  print(plot_power)
} else {
  message("No power data to plot.")
}

message("\nStep 2: Creating sfnetwork from high-voltage lines (simplified)...")

hv_power_network <- as_sfnetwork(hv_lines_sf, directed = FALSE, edges_as_lines = TRUE)

# Corrected simplification:
hv_power_network <- hv_power_network %>%
  activate("edges") %>%        # <--- ACTIVATE EDGES HERE
  st_simplify(dTolerance = 1) %>% # <--- Use dTolerance, not tolerance
  activate("nodes") %>%        # <--- Re-activate nodes to work with them next
  filter(!node_is_isolated())  # Remove any nodes that are not connected to any edges

# Assign simple, sequential names to nodes for easier igraph handling
hv_power_network <- hv_power_network %>%
  activate("nodes") %>%
  mutate(name = as.character(dplyr::row_number()))

message(paste0("  sfnetwork created with ",
               nrow(as_tibble(activate(hv_power_network, "nodes"))), " nodes and ",
               nrow(as_tibble(activate(hv_power_network, "edges"))), " edges (after simplification)."))

message("\nAttempting to create igraph object via manual data frame construction...")

# 1. Extract Node Data from hv_power_network
# We need the 'name' column for vertex identification in igraph
# and 'geometry' for coordinates, but as_data_frame is better for igraph.
nodes_for_igraph <- hv_power_network %>%
  activate("nodes") %>%
  as_tibble() # Converts sf_point to x and y columns, and retains 'name'

# Ensure the 'name' column is character type for igraph
nodes_for_igraph$name <- as.character(nodes_for_igraph$name)

# 2. Extract Edge Data from hv_power_network
# We need the 'from' and 'to' columns which represent node IDs
edges_for_igraph <- hv_power_network %>%
  activate("edges") %>%
  as_tibble() %>%
  dplyr::select(from, to) # Select only the essential 'from' and 'to' columns

# Ensure 'from' and 'to' are integer or numeric as expected by igraph
edges_for_igraph$from <- as.integer(edges_for_igraph$from)
edges_for_igraph$to <- as.integer(edges_for_igraph$to)
 edgetempo=data.frame(from=edges_for_igraph$from,to=edges_for_igraph$to)
 hv_power_graph_igraph <- igraph::graph_from_data_frame(edgetempo,directed=FALSE)

node_coordinates_matrix <- st_coordinates(nodes_for_igraph)
head(node_coordinates_matrix)
plot(hv_power_graph_igraph,layout=node_coordinates_matrix)

 V(hv_power_graph_igraph)$X <- node_coordinates_matrix[,1]
 V(hv_power_graph_igraph)$Y <- node_coordinates_matrix[,2]
 g <- hv_power_graph_igraph
 g <- simplify(delete_vertices(g, which(components(g)$membership!=which.max(components(g)$csize))))
 g
 layout_orig <-cbind(V(g)$X,V(g)$Y)
 plot(g,layout=layout_orig)


# Assuming 'g' is your igraph object as provided:
# IGRAPH b508f67 UN-- 40 39 --
# + attr: name (v/c), X (v/n), Y (v/n)
# ...

# 1. Inspect current vertex names (optional)
message("Original vertex names:")
print(V(g)$name)

# 2. Create a new graph object to modify (good practice)
g_reindexed <- g

# 3. Remove the 'name' attribute from the vertices
# This makes igraph default to its internal 1-based integer IDs for display/reference
g_reindexed <- delete_vertex_attr(g_reindexed, "name")

# Alternatively, you can set the 'name' attribute to NULL:
# V(g_reindexed)$name <- NULL


# 4. Inspect the re-indexed graph
message("\nRe-indexed graph (nodes starting from 1):")
print(g_reindexed)

# Check the vertex IDs (they should now be 1, 2, 3... N)
message("\nNew vertex IDs:")
print(V(g_reindexed))

# Check if the 'name' attribute is gone or NULL
message("\n'name' attribute after re-indexing (should be NULL or empty):")
print(V(g_reindexed)$name)

# You can still access other attributes like X and Y, as they were not removed
message("\nExample of retaining other attributes (e.g., X coordinate of first 5 nodes):")
print(head(V(g_reindexed)$X, 5))

 windows()
  plot(g_reindexed,layout=layout_orig)
networrk1= "TempoER"
g <- g_reindexed

#################################
message("\n--- Bounding the plot with a Box ---")

# Define your desired bounding box (example: a smaller area within Emilia-Romagna)
# Format: c(xmin, ymin, xmax, ymax) in your plot's CRS (e.g., longitude/latitude)
plot_bbox <- c(xmin = 11.2, ymin = 44.4, xmax = 11.6, ymax = 44.6) # Example: Around Bologna

# 1. Crop 'streets' (sf object)
streets_cropped <- st_crop(streets, plot_bbox)
message(paste0("  Streets cropped: From ", nrow(streets), " to ", nrow(streets_cropped), " features."))

# 2. Filter 'node_df' (data frame with x, y)
node_df_filtered_bbox <- node_df %>%
  dplyr::filter(x >= plot_bbox[1], x <= plot_bbox[3],
                y >= plot_bbox[2], y <= plot_bbox[4])
message(paste0("  Nodes filtered by bbox: From ", nrow(node_df), " to ", nrow(node_df_filtered_bbox), " features."))

tempo1_bounded_box <- ggplot() +
  geom_sf(data = streets_cropped, color="gray", size=0.3) +
  ggplot2::geom_text(data = node_df_filtered_bbox, ggplot2::aes(x = x, y = y, label = node_id),
                    size = 1.5,
                    color = "blue",
                    nudge_x = 0.0001, nudge_y = 0.0001,
                    check_overlap = TRUE) +
  geom_polygon(data = poly, aes(x = x, y = y), fill = NA, color = "red", linewidth=1, linetype = "dashed") +
  ggplot2::geom_point(data = node_df_filtered_bbox, ggplot2::aes(x = x, y = y),
                    color = "black", size = 0.5, alpha = 0.1) +
  theme_minimal() +
  labs(title = "Plot Bounded by a Box",
       subtitle = paste0("BBox: x=[", round(plot_bbox[1],2), ",", round(plot_bbox[3],2), "], y=[", round(plot_bbox[2],2), ",", round(plot_bbox[4],2), "]"))

print(tempo1_bounded_box)

################ based on chapt pero por gemini. multiple llops ok
library(osmdata)
library(sf)
library(sfnetworks)
library(igraph)
library(dplyr)
library(ggplot2)
library(sp) # For point.in.polygon
library(RColorBrewer) # For scale_color_manual colors
library(tidyr) # For pivot_wider

# --- 0. Data Acquisition and Network Setup ---
message("--- Setting up the Bologna Network ---")

bologna_wider_bbox <- c(left = 11.15, bottom = 44.35, right = 11.55, top = 44.65)
#bologna_wider_bbox <- c(left = 11.30, bottom = 44.45, right = 11.40, top = 44.51)

message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway",
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

streets <- all_roads_osm$osm_lines
message(paste0("  Found ", nrow(streets), " street segments."))

message("  Converting to sfnetwork...")
net <- sfnetworks::as_sfnetwork(streets, directed = FALSE)
message(paste0("  Network created with ", nrow(activate(net, "nodes") %>% as_tibble()), " nodes and ",
               nrow(activate(net, "edges") %>% as_tibble()), " edges."))

net <- activate(net, "nodes") %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  mutate(node_id = row_number()) %>%
  mutate(name = as.character(node_id))

# Ensure coords_all_nodes is a plain tibble without SF class
coords_all_nodes <- net %>%
  activate("nodes") %>%
  as_tibble() %>%
  select(node_id, x, y, name)

graph_full <- as.igraph(net)


# --- 1. Define your list of selected loop_node sets ---
selected_loop_node_sets <- list(
  "Loop_A" = c(10, 30, 75, 300, 10),
  "Loop_B" = c(40, 50, 60, 70, 40),
  "Loop_C" = c(100, 110, 120, 130, 100)
)


# --- 2. Function to process a single loop (SLIGHTLY MODIFIED FOR OUTPUT) ---
# Now returns GWN status more comprehensively within centralities and its own df.
process_single_loop <- function(loop_name, loop_nodes_vector, full_net, all_nodes_coords, full_graph) {
  message(paste0("  Processing loop: ", loop_name))

  poly_current_loop <- all_nodes_coords %>%
    filter(node_id %in% loop_nodes_vector) %>%
    slice(match(loop_nodes_vector, node_id)) %>%
    select(x, y, node_id)

  if (nrow(poly_current_loop) < 3) {
    warning(paste0("    Loop '", loop_name, "' has less than 3 unique points. Cannot form polygon. Skipping."))
    return(NULL)
  }

  final_polygon_sf <- tryCatch({
    combined_points <- sf::st_combine(sf::st_as_sf(poly_current_loop, coords = c("x", "y"), crs = st_crs(full_net)))
    linestring_from_points <- sf::st_cast(combined_points, "LINESTRING")
    sf::st_cast(linestring_from_points, "POLYGON")
  }, error = function(e) {
    warning(paste0("    Error creating polygon for '", loop_name, "': ", e$message))
    return(NULL)
  })

  if (is.null(final_polygon_sf) || st_is_empty(final_polygon_sf)) {
    warning(paste0("    Could not create valid polygon for '", loop_name, "'. Skipping."))
    return(NULL)
  }

  current_gwn_area <- sf::st_area(final_polygon_sf)
  message(paste0("    GWN Area for '", loop_name, "': ", round(current_gwn_area, 2), " sq m."))

  nodes_gwn_status <- all_nodes_coords %>%
    rowwise() %>%
    mutate(
      is_in_polygon = sp::point.in.polygon(x, y, poly_current_loop$x, poly_current_loop$y),
      gwn_inner = as.integer(is_in_polygon == 1),
      is_on_loop_boundary = as.integer(node_id %in% loop_nodes_vector),
      gwn_combined_status = case_when(
        is_on_loop_boundary == 1 ~ "Boundary Node",
        gwn_inner == 1 ~ "GWN Inner Node",
        TRUE ~ "Other"
      )
    ) %>%
    ungroup() %>%
    select(node_id, gwn_inner, is_on_loop_boundary, gwn_combined_status) %>%
    mutate(loop_id = loop_name)

  nodes_for_centrality <- nodes_gwn_status %>%
    filter(gwn_inner == 1 | is_on_loop_boundary == 1) %>%
    pull(node_id)

  if (length(nodes_for_centrality) == 0) {
      message(paste0("    No nodes found inside GWN or on boundary for '", loop_name, "'. Skipping centrality calculation."))
      return(NULL)
  }

  current_loop_centralities <- data.frame(
    node_id = V(full_graph)$name,
    degree = degree(full_graph, normalized = FALSE),
    betweenness = betweenness(full_graph, normalized = TRUE)
  ) %>%
  mutate(node_id = as.integer(node_id)) %>%
  filter(node_id %in% nodes_for_centrality) %>%
  # MERGE GWN status into centralities for easier access later
  left_join(nodes_gwn_status %>% select(node_id, gwn_inner, is_on_loop_boundary, gwn_combined_status), by = "node_id") %>%
  mutate(loop_id = loop_name) # Add loop identifier to centralities

  message(paste0("    Centralities calculated for ", nrow(current_loop_centralities), " relevant nodes."))

  return(list(
    centralities = current_loop_centralities, # Now includes GWN status for each node-loop pair
    loop_poly = poly_current_loop %>% mutate(loop_id = loop_name),
    # gwn_status is no longer needed separately as it's in 'centralities' for relevant nodes
    loop_area = data.frame(loop_id = loop_name, area_sqm = current_gwn_area)
  ))
}


# --- 3. Iterate and Collect Results from all loops ---
all_centralities_results <- list()
all_loop_polys_for_plotting <- list()
all_loop_areas_results <- list()

for (loop_id_name in names(selected_loop_node_sets)) {
  loop_node_set <- selected_loop_node_sets[[loop_id_name]]
  result <- process_single_loop(
    loop_name = loop_id_name,
    loop_nodes_vector = loop_node_set,
    full_net = net,
    all_nodes_coords = coords_all_nodes,
    full_graph = graph_full
  )
  if (!is.null(result)) {
    all_centralities_results[[loop_id_name]] <- result$centralities
    all_loop_polys_for_plotting[[loop_id_name]] <- result$loop_poly
    all_loop_areas_results[[loop_id_name]] <- result$loop_area
  }
}


# --- 4. Combine results into single data frames for plotting and analysis ---
final_centralities_df <- bind_rows(all_centralities_results) %>%
  as_tibble() %>%
  select(-any_of("geometry")) # Remove geometry column if it somehow persisted

final_loop_polys_df <- bind_rows(all_loop_polys_for_plotting) %>%
  as_tibble() %>%
  select(-any_of("geometry"))

final_loop_areas_df <- bind_rows(all_loop_areas_results) %>%
  as_tibble()


# --- Create node_plot_df for unified plotting (REWRITTEN FOR CLARITY AND MULTI-LOOP) ---
node_plot_df <- coords_all_nodes %>%
  # Left join all centralities and GWN status (which includes loop_id)
  left_join(final_centralities_df %>%
              select(node_id, loop_id, degree, betweenness, gwn_inner, is_on_loop_boundary, gwn_combined_status),
            by = "node_id", # Join on node_id
            relationship = "many-to-many" # Allow multiple matches per node_id
            ) %>%
  # Fill NA centralities for nodes not present in final_centralities_df
  mutate(degree = replace_na(degree, 0),
         betweenness = replace_na(betweenness, 0)) %>%
  # Determine if a node belongs to ANY loop (GWN inner or boundary)
  # This column is for general filtering/status, not specific loop ID
  mutate(is_part_of_any_loop = !is.na(loop_id)) %>% # If loop_id is not NA, it's part of some loop
  # Identify multi-loop nodes:
  group_by(node_id) %>%
  mutate(num_loop_affiliations = n_distinct(loop_id[!is.na(loop_id)])) %>% # Count unique loop_ids per node
  ungroup() %>%
  mutate(
    # This `plot_category` will determine the color in the plot
    plot_category = case_when(
      num_loop_affiliations > 1 ~ "Multi-Loop Node", # Special category for nodes in >1 loop
      num_loop_affiliations == 1 ~ loop_id,          # If in one loop, use that loop's ID
      TRUE ~ "Other Nodes"                           # Nodes not associated with any loop
    )
  ) %>%
  # IMPORTANT: For plotting, we still want one row per node.
  # If a node is a 'Multi-Loop Node', we'll keep one row but use the 'Multi-Loop Node' color.
  # If it's a single loop node, keep that one row.
  distinct(node_id, .keep_all = TRUE)


message("\n--- Final Data Frames for Analysis and Plotting ---")
message("Final Centralities Data Frame (final_centralities_df):")
print(head(final_centralities_df)) # Now includes GWN status columns for each loop-node pair
message(paste0("  Total entries: ", nrow(final_centralities_df)))

message("\nFinal Loop Polygons Data Frame (final_loop_polys_df):")
print(head(final_loop_polys_df))
message(paste0("  Total entries: ", nrow(final_loop_polys_df)))

message("\nFinal Loop Areas Data Frame (final_loop_areas_df):")
print(final_loop_areas_df)
message(paste0("  Total entries: ", nrow(final_loop_areas_df)))

message("\nNode Plot Data Frame (node_plot_df - includes GWN and centralities):")
print(head(node_plot_df))
message("\n--- Summary of plot_category in node_plot_df ---")
print(node_plot_df %>% count(plot_category)) # Check final categories for plotting
print(summary(node_plot_df$degree))


# --- 5. Plotting all loops and their GWN nodes (MODIFIED FOR COLORING) ---

# Prepare color palette for all possible categories in the plot
loop_boundary_ids <- unique(final_loop_polys_df$loop_id)
num_loops <- length(loop_boundary_ids)

# Generate distinct colors for each loop
colors_for_loops <- RColorBrewer::brewer.pal(max(3, num_loops), "Dark2")[1:num_loops]
loop_colors_palette <- setNames(colors_for_loops, loop_boundary_ids)

# Define specific colors for "Other Nodes" and "Multi-Loop Node"
other_node_color <- "lightgray"
multi_loop_node_color <- "purple" # Distinct color for multi-loop nodes

# Combine all necessary colors into a single named vector for scale_color_manual
all_plot_colors <- c(
  loop_colors_palette,          # Colors for each specific loop (e.g., "Loop_A", "Loop_B")
  "Multi-Loop Node" = multi_loop_node_color, # Color for nodes belonging to multiple loops
  "Other Nodes" = other_node_color # Color for nodes outside any GWN/boundary
)

tempo_multiple_loops <-ggplot() +
  geom_sf(data = streets, color="gray", linewidth=0.3, alpha = 0.5) + # All roads in background
  # Plot the loop polygons, colored by loop_id
  geom_polygon(data = final_loop_polys_df,
               aes(x = x, y = y, group = loop_id, color = loop_id), # `loop_id` for group and color
               fill = NA, linewidth=1, linetype = "dashed") +
  # Plot nodes, coloring them based on the new 'plot_category'
  geom_point(data = node_plot_df,
             aes(x = x, y = y,
                 size = degree, # Size by degree centrality
                 color = plot_category # Use the new plot_category for coloring
             ),
             alpha = 0.7) +
  # Plot node labels (only for relevant nodes to avoid clutter)
  ggplot2::geom_text(data = node_plot_df %>% filter(plot_category != "Other Nodes"),
                      ggplot2::aes(x = x, y = y, label = node_id, color = plot_category), # Label color by plot_category
                      size = 1.5,
                      nudge_x = 0.0001, nudge_y = 0.0001,
                      check_overlap = TRUE) +
  scale_color_manual(name = "Node Affiliation",
                     values = all_plot_colors # Use the combined palette
                     ) +
  scale_size_area(max_size = 3, name = "Degree Centrality") +
  theme_minimal() +
  labs(title = "Bologna Road Network: Loops, GWN & Multi-Loop Nodes",
       subtitle = "Loops are dashed, nodes sized by centrality, colored by loop affiliation (specific loop or multi-loop)")

print(tempo_multiple_loops)

# --- Final Analysis Data Frames ---
# "final_centralities_df" now includes `gwn_inner`, `is_on_loop_boundary`, and `gwn_combined_status`
# for *each node-loop pair* where the node is part of that loop's GWN or boundary.
message("\n--- Centralities and GWN status for each node within relevant loops ---")
print(head(final_centralities_df))
print(summary(final_centralities_df))
message(paste0("Total entries (node-loop pairs): ", nrow(final_centralities_df)))


# "final_loop_areas_df" explicitly lists the area for each loop
message("\n--- Calculated Loop Areas ---")
print(final_loop_areas_df)
#######

tempo_multiple_loops1 <-ggplot() +
  geom_sf(data = streets, color="gray", linewidth=0.3, alpha = 0.5) + # All roads in background
  # Plot the dashed polygons, colored by loop_id
  geom_polygon(data = final_loop_polys_df,
               aes(x = x, y = y, group = loop_id, color = loop_id), # `loop_id` for group and color
               fill = NA, linewidth=2, linetype = "dashed") +
  # Plot all nodes: same size, black color
  geom_point(data = node_plot_df,
             aes(x = x, y = y),
             size = 0.2, # Fixed size for all nodes
             color = "black", # Fixed color for all nodes
             alpha = 0.7) +
  # Optional: Plot node labels (only for relevant nodes to avoid clutter, but now black)
  # If you want NO labels, you can remove this geom_text block entirely.
  # If you want labels for ALL nodes, remove the filter.
  ggplot2::geom_text(data = node_plot_df %>% filter(plot_category != "Other Nodes"),
                      ggplot2::aes(x = x, y = y, label = node_id), # Label is node_id, color is now explicitly black
                      color = "black", # Labels are also black
                      size = 1.5,
                      nudge_x = 0.0001, nudge_y = 0.0001,
                      check_overlap = TRUE) +
  # Only scale_color_manual for loop polygons. Node color is fixed.
  scale_color_manual(name = "Loop ID",
                     values = loop_colors_palette # Use only the palette for loops
                     ) +
  # Remove scale_size_area as size is fixed
  # scale_size_area(max_size = 3, name = "Degree Centrality") + # REMOVED
  theme_minimal() +
  labs(title = "Bologna Road Network: Defined Loops",
       subtitle = "Loops are dashed polygons, all nodes shown in black")

print(tempo_multiple_loops1)
 ############################################## hasta aqui multiple loos  ok

#################################
message("\n--- Bounding the plot with a Box ---")

# Define your desired bounding box (example: a smaller area within Emilia-Romagna)
# Format: c(xmin, ymin, xmax, ymax) in your plot's CRS (e.g., longitude/latitude)
plot_bbox <- c(xmin = 11.2, ymin = 44.45, xmax = 11.6, ymax = 44.6) # Example: Around Bologna

# 1. Crop 'streets' (sf object)
streets_cropped <- st_crop(streets, plot_bbox)
message(paste0("  Streets cropped: From ", nrow(streets), " to ", nrow(streets_cropped), " features."))

# 2. Filter 'node_df' (data frame with x, y)
node_df_filtered_bbox <- node_df %>%
  dplyr::filter(x >= plot_bbox[1], x <= plot_bbox[3],
                y >= plot_bbox[2], y <= plot_bbox[4])
message(paste0("  Nodes filtered by bbox: From ", nrow(node_df), " to ", nrow(node_df_filtered_bbox), " features."))

tempo1_bounded_box <- ggplot() +
  geom_sf(data = streets_cropped, color="gray", size=0.3) +
  ggplot2::geom_text(data = node_df_filtered_bbox, ggplot2::aes(x = x, y = y, label = node_id),
                    size = 1.5,
                    color = "blue",
                    nudge_x = 0.0001, nudge_y = 0.0001,
                    check_overlap = TRUE) +
  geom_polygon(data = final_loop_polys_df,
               aes(x = x, y = y, group = loop_id, color = loop_id), # `loop_id` for group and color
               fill = NA, linewidth=2, linetype = "dashed") +
  ggplot2::geom_point(data = node_df_filtered_bbox, ggplot2::aes(x = x, y = y),
                    color = "black", size = 0.5, alpha = 0.1) +
  theme_minimal() +
  labs(title = "Plot Bounded by a Box",
       subtitle = paste0("BBox: x=[", round(plot_bbox[1],2), ",", round(plot_bbox[3],2), "], y=[", round(plot_bbox[2],2), ",", round(plot_bbox[4],2), "]"))

print(tempo1_bounded_box)

################################### venezuela

# Load necessary libraries
library(osmdata)    # For OpenStreetMap data
library(sf)         # For spatial data operations
library(dplyr)      # For data manipulation
library(ggplot2)    # For plotting

# --- 0. Define Bounding Box for Venezuela ---
# Bounding box for Venezuela (left, bottom, right, top) from sandstrom/country-bounding-boxes GitHub
# min_lon, min_lat, max_lon, max_lat
venezuela_bbox <- c(left = -73.42, bottom = 0.63, right = -59.78, top = 12.2)
message(paste0("Defined initial bounding box for Venezuela: ",
               "Left=", venezuela_bbox["left"], ", Bottom=", venezuela_bbox["bottom"],
               ", Right=", venezuela_bbox["right"], ", Top=", venezuela_bbox["top"]))

# --- 1. Query OpenStreetMap for main cities within the broad bbox ---
message("Querying OpenStreetMap for cities and towns within Venezuela's bounding box...")
cities_osm <- venezuela_bbox %>%
  opq() %>%
  add_osm_feature(key = "place", value = c("city", "town")) %>%
  osmdata_sf()

cities_points <- cities_osm$osm_points
message(paste0("Found ", nrow(cities_points), " raw points representing cities/towns in the bounding box area."))

# --- 2. Get Venezuela's Administrative Boundary for Precise Filtering ---
message("Querying for Venezuela's national administrative boundary...")
venezuela_boundary_osm <- opq(bbox = venezuela_bbox) %>%
  add_osm_feature(key = "admin_level", value = "2") %>%
  add_osm_feature(key = "name:en", value = "Venezuela") %>% # To specifically target Venezuela
  osmdata_sf()

# Extract the polygon for Venezuela
# admin_level=2 features can be polygons or multipolygons. Ensure we get a single polygon object.
venezuela_polygon <- venezuela_boundary_osm$osm_multipolygons
if (is.null(venezuela_polygon) || nrow(venezuela_polygon) == 0) {
  # Fallback to osm_polygons if multipolygons are empty
  venezuela_polygon <- venezuela_boundary_osm$osm_polygons
}

if (is.null(venezuela_polygon) || nrow(venezuela_polygon) == 0) {
  stop("Could not find a valid administrative boundary polygon for Venezuela. Please check the bbox or OSM data for Venezuela.")
}

# Ensure only one Venezuela polygon (if multiple are returned, pick the largest or most appropriate)
# For simplicity, we'll take the first one if multiple exist.
venezuela_polygon <- venezuela_polygon %>%
  filter(name %in% c("Venezuela", "Bolivarian Republic of Venezuela")) %>% # Filter by common names
  slice(1) # Take the first match

message("Successfully retrieved Venezuela's administrative boundary.")

# --- 3. Spatially Filter Cities to be within Venezuela's Boundary ---
message("Spatially filtering cities to be strictly within Venezuela's national boundary...")
cities_in_venezuela_sf <- cities_points %>%
  filter(!is.na(name)) %>% # Still ensure they have a name
  st_filter(venezuela_polygon, .predicate = st_within) # Keep only points strictly within the polygon

message(paste0("Filtered to ", nrow(cities_in_venezuela_sf), " named cities/towns strictly within Venezuela."))


# --- 4. Extract Coordinates and other Information to a Data Frame ---
cities_df <- cities_in_venezuela_sf %>%
  mutate(x = st_coordinates(geometry)[,1],
         y = st_coordinates(geometry)[,2]) %>%
  as_tibble() %>% # Convert to tibble to drop sf class attributes for easier data.frame manipulation
  select(osm_id, name, place, population, capital, x, y) # Include 'capital' if present

message("\n--- Extracted Main Cities Data Frame (Head) ---")
print(head(cities_df))
message(paste0("Total entries in data frame: ", nrow(cities_df)))

# --- 5. Plot the cities within Venezuela's context ---
message("\nPlotting main cities of Venezuela...")

# Get coastline for background map (optional, as the admin boundary also shows the outline)
venezuela_coastline_osm <- venezuela_bbox %>%
  opq() %>%
  add_osm_feature(key = "natural", value = "coastline") %>%
  osmdata_sf()
windows()
ggplot() +
  geom_sf(data = venezuela_coastline_osm$osm_lines, color = "darkblue", size = 0.5, alpha = 0.6) + # Coastline for detail
  geom_sf(data = venezuela_polygon, fill = NA, color = "darkgreen", size = 1) + # Country boundary
#  geom_point(data = cities_df, aes(x = x, y = y, color = place), size = 2, alpha = 0.8) + # Cities
  geom_point(data = cities_df, aes(x = x, y = y), size = 0.5, alpha = 0.8) + # Cities

  geom_text(data = cities_df, aes(x = x, y = y, label = name),
            nudge_x = 0.5, nudge_y = 0.2, size = 2.5, check_overlap = TRUE, color = "black") + # City labels
  labs(title = "Main Cities of Venezuela (Spatially Filtered)",
#       subtitle = "Points colored by 'place' type (city/town), within national boundary",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  coord_sf(xlim = c(venezuela_bbox["left"], venezuela_bbox["right"]),
           ylim = c(venezuela_bbox["bottom"], venezuela_bbox["top"]),
           crs = st_crs(cities_points))

message("\nPlot generated showing the main cities within Venezuela's borders.")

# Final data frame requested
message("\n--- Final Data Frame of Main Cities with Coordinates (Cleaned) ---")
print(head(cities_df))
message(paste0("Data frame includes ", nrow(cities_df), " cities."))


# randomly selected subset of cities...

# --- Ensure previous data is available ---
# If you are running this code block independently, make sure 'cities_df',
# 'venezuela_polygon', and 'venezuela_coastline_osm' are already loaded
# from the previous full script run.
# For demonstration, I'll assume they are in your environment.
# If not, you'd need to run the full previous script first to generate them.

# --- 1. Select a subset of cities at random ---
# Define the number of random cities to select

num_random_cities <- 80 # <--- You can change this number!

set.seed(123)
if (nrow(cities_df) < num_random_cities) {
  warning(paste0("Requested ", num_random_cities, " random cities, but only ",
                 nrow(cities_df), " cities are available. Selecting all available cities."))
  subset_cities_df <- cities_df
} else {
  message(paste0("Selecting ", num_random_cities, " random cities from the ",
                 nrow(cities_df), " available Venezuelan cities."))
  subset_cities_df <- cities_df %>%
    sample_n(num_random_cities) # Randomly select N rows
}

message("\n--- Subset of Randomly Selected Cities (Head) ---")
print(head(subset_cities_df))
message(paste0("Total entries in subset data frame: ", nrow(subset_cities_df)))

# --- 2. Plot the subset of cities on a new map ---
message("\nPlotting the randomly selected subset of cities...")

ggplot() +
  geom_sf(data = venezuela_coastline_osm$osm_lines, color = "darkblue", size = 0.5, alpha = 0.6) + # Coastline
  geom_sf(data = venezuela_polygon, fill = NA, color = "darkgreen", size = 1) + # Country boundary
  geom_point(data = subset_cities_df, aes(x = x, y = y),
             color = "black", # All points black
             size = 2.5,     # Fixed size
             alpha = 0.8) +
  geom_text(data = subset_cities_df, aes(x = x, y = y, label = name),
            color = "black", # Labels also black
            nudge_x = 0.5, nudge_y = 0.2, size = 3, check_overlap = TRUE) +
  labs(title = paste0("Venezuela: Random Subset of ", nrow(subset_cities_df), " Main Cities"),
       subtitle = "All selected cities shown in black",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  # Use the same coordinate limits as before to maintain context
  coord_sf(xlim = c(venezuela_bbox["left"], venezuela_bbox["right"]),
           ylim = c(venezuela_bbox["bottom"], venezuela_bbox["top"]),
           crs = st_crs(cities_points))

message("\nPlot generated for the randomly selected subset of cities.")


 layout_orig <-cbind(subset_cities_df$x,subset_cities_df$y)

 layout_orig 

# end venezuela ciudades+ random number of cities

############################################## como bologna power pero para venezuela no se

######################## power requiere codigo anterior de venezuela

library(osmdata)
library(sf)
library(ggplot2) # For plotting

# Define a bounding box for Emilia-Romagna (approximate)
# min_lon, min_lat, max_lon, max_lat

emilia_romagna_bbox <- c(left = -73.42, bottom = 0.63, right = -59.78, top = 12.2) #venezuela
message("Querying OpenStreetMap for electric power system data in Emilia-Romagna...")

# --- Option 1: Query for ALL power features (might be very large) ---
# This will return points, lines, and polygons for any 'power' tag
# power_osm_all <- opq(emilia_romagna_bbox) %>%
#   add_osm_feature(key = "power") %>%
#   osmdata_sf()
# message(paste0("  Downloaded all 'power' features. Points: ", nrow(power_osm_all$osm_points),
#                ", Lines: ", nrow(power_osm_all$osm_lines),
#                ", Polygons: ", nrow(power_osm_all$osm_polygons)))

# --- Option 2: More targeted query for specific components ---
# This is usually more practical. Let's focus on major power lines and substations.

# Query for power lines (overhead and underground)
power_lines_osm <- opq(emilia_romagna_bbox) %>%
#  add_osm_feature(key = "power", value = c("line", "cable", "minor_line")) %>%
 add_osm_feature(key = "power", value = c("line", "cable")) %>%
  osmdata_sf()

# Extract power line geometries (these are sf LINESTRINGs)
power_lines_sf <- power_lines_osm$osm_lines

power_lines_sf  <- power_lines_sf  %>%
#  filter(!is.na(name)) %>% # Still ensure they have a name
  st_filter(venezuela_polygon, .predicate = st_within) # Keep only points strictly within the polygon


if (!is.null(power_lines_sf)) {
  message(paste0("  Found ", nrow(power_lines_sf), " power lines."))
#   Optionally filter by voltage, e.g., for high-voltage lines
   power_lines_hv <- power_lines_sf %>% filter(stringr::str_detect(voltage, "230000|400000|765000"))
#   power_lines_hv <- power_lines_sf %>% filter(stringr::str_detect(voltage, "380000"))
   message(paste0("  Found ", nrow(power_lines_hv), " high-voltage power lines."))
} else {
  message("  No power lines found for the specified area/tags.")
}


# Query for substations (major nodes in the power grid)
power_substations_osm <- opq(emilia_romagna_bbox) %>%
  add_osm_feature(key = "power", value = "substation") %>%
  osmdata_sf()

# Extract substation geometries (these are usually sf POINTS or POLYGONS)
power_substations_sf <- power_substations_osm$osm_points # Often tagged as points
if (!is.null(power_substations_sf)) {
  message(paste0("  Found ", nrow(power_substations_sf), " power substations (as points)."))
} else {
  message("  No power substations found for the specified area/tags.")
}

power_substations_sf <- power_substations_sf  %>%
#  filter(!is.na(name)) %>% # Still ensure they have a name
  st_filter(venezuela_polygon, .predicate = st_within) # Keep only points strictly within the polygon

# You might also want power plants
# power_plants_osm <- opq(emilia_romagna_bbox) %>%
#   add_osm_feature(key = "power", value = "plant") %>%
#   osmdata_sf()
# power_plants_sf <- power_plants_osm$osm_points # Or osm_polygons
# if (!is.null(power_plants_sf)) {
#   message(paste0("  Found ", nrow(power_plants_sf), " power plants."))
# }

message("Power system data acquisition complete.")
windows()
# --- Basic Visualization (Optional) ---
if (!is.null(power_lines_sf) || !is.null(power_substations_sf)) {
  message("\nPlotting acquired power system data...")
 plot_power <- ggplot() +

  geom_sf(data = venezuela_coastline_osm$osm_lines, color = "darkblue", size = 0.5, alpha = 0.6) + # Coastline
  geom_sf(data = venezuela_polygon, fill = NA, color = "darkgreen", size = 1) + # Country boundary

#    {if(!is.null(power_lines_sf)) geom_sf(data = power_lines_sf, aes(color = power), linewidth = 0.5)} +
    {if(!is.null(power_lines_sf)) geom_sf(data = power_lines_sf,  linewidth = 0.5)} +
    {if(!is.null(power_substations_sf)) geom_sf(data = power_substations_sf, color = "darkblue", size = 1, shape = 15)} + # Squares for substations
    labs(title = "Venezuela Electric Power System  (OSM Data)",
         subtitle = "Blue squares: Substations; Colored lines: Power Lines") +
#geom_point(data = substations_df, aes(x = x, y = y), color = "red", size = 2) +
#   geom_text(data = substations_df, aes(x = x, y = y, label = name),
#             nudge_x = 0.01, nudge_y = 0.01, size = 2.5, check_overlap = TRUE) +
    theme_minimal()
  print(plot_power)
} else {
  message("No power data to plot.")
}

message("\nStep 2: Creating sfnetwork from high-voltage lines (simplified)...")

#######
hv_lines_sf <- power_lines_hv

hv_power_network <- as_sfnetwork(hv_lines_sf, directed = FALSE, edges_as_lines = TRUE)

# Corrected simplification:
hv_power_network <- hv_power_network %>%
  activate("edges") %>%        # <--- ACTIVATE EDGES HERE
  st_simplify(dTolerance = 1) %>% # <--- Use dTolerance, not tolerance
  activate("nodes") %>%        # <--- Re-activate nodes to work with them next
  filter(!node_is_isolated())  # Remove any nodes that are not connected to any edges

# Assign simple, sequential names to nodes for easier igraph handling
hv_power_network <- hv_power_network %>%
  activate("nodes") %>%
  mutate(name = as.character(dplyr::row_number()))

message(paste0("  sfnetwork created with ",
               nrow(as_tibble(activate(hv_power_network, "nodes"))), " nodes and ",
               nrow(as_tibble(activate(hv_power_network, "edges"))), " edges (after simplification)."))

message("\nAttempting to create igraph object via manual data frame construction...")

# 1. Extract Node Data from hv_power_network
# We need the 'name' column for vertex identification in igraph
# and 'geometry' for coordinates, but as_data_frame is better for igraph.
nodes_for_igraph <- hv_power_network %>%
  activate("nodes") %>%
  as_tibble() # Converts sf_point to x and y columns, and retains 'name'

# Ensure the 'name' column is character type for igraph
nodes_for_igraph$name <- as.character(nodes_for_igraph$name)

# 2. Extract Edge Data from hv_power_network
# We need the 'from' and 'to' columns which represent node IDs
edges_for_igraph <- hv_power_network %>%
  activate("edges") %>%
  as_tibble() %>%
  dplyr::select(from, to) # Select only the essential 'from' and 'to' columns

# Ensure 'from' and 'to' are integer or numeric as expected by igraph
edges_for_igraph$from <- as.integer(edges_for_igraph$from)
edges_for_igraph$to <- as.integer(edges_for_igraph$to)
 edgetempo=data.frame(from=edges_for_igraph$from,to=edges_for_igraph$to)
 hv_power_graph_igraph <- igraph::graph_from_data_frame(edgetempo,directed=FALSE)

node_coordinates_matrix <- st_coordinates(nodes_for_igraph)
head(node_coordinates_matrix)
plot(hv_power_graph_igraph,layout=node_coordinates_matrix)

 V(hv_power_graph_igraph)$X <- node_coordinates_matrix[,1]
 V(hv_power_graph_igraph)$Y <- node_coordinates_matrix[,2]
 g <- hv_power_graph_igraph
 g <- simplify(delete_vertices(g, which(components(g)$membership!=which.max(components(g)$csize))))
 g
 layout_orig <-cbind(V(g)$X,V(g)$Y)
 plot(g,layout=layout_orig)


# Assuming 'g' is your igraph object as provided:
# IGRAPH b508f67 UN-- 40 39 --
# + attr: name (v/c), X (v/n), Y (v/n)
# ...

# 1. Inspect current vertex names (optional)
message("Original vertex names:")
print(V(g)$name)

# 2. Create a new graph object to modify (good practice)
g_reindexed <- g

# 3. Remove the 'name' attribute from the vertices
# This makes igraph default to its internal 1-based integer IDs for display/reference
g_reindexed <- delete_vertex_attr(g_reindexed, "name")

# Alternatively, you can set the 'name' attribute to NULL:
# V(g_reindexed)$name <- NULL


# 4. Inspect the re-indexed graph
message("\nRe-indexed graph (nodes starting from 1):")
print(g_reindexed)

# Check the vertex IDs (they should now be 1, 2, 3... N)
message("\nNew vertex IDs:")
print(V(g_reindexed))

# Check if the 'name' attribute is gone or NULL
message("\n'name' attribute after re-indexing (should be NULL or empty):")
print(V(g_reindexed)$name)

# You can still access other attributes like X and Y, as they were not removed
message("\nExample of retaining other attributes (e.g., X coordinate of first 5 nodes):")
print(head(V(g_reindexed)$X, 5))

 windows()
  plot(g_reindexed,layout=layout_orig)
networrk1= "TempoER"
g <- g_reindexed

###### pegando nombres:
# Load necessary libraries (if not already loaded)


# Assuming the following are in your environment from previous steps:
# - cities_df (a tibble of cities with x, y)
# - power_substations_sf (the sf object from your osmdata query)

# --- 1. Prepare data for spatial analysis ---
message("Preparing data for spatial nearest-neighbor analysis...")

# Separate substations that already have a name from those that don't
# Use st_filter for this as it's the most robust way to filter an sf object.
substations_with_names_sf <- power_substations_sf %>% filter(!is.na(name))
substations_without_names_sf <- power_substations_sf %>% filter(is.na(name))

message(paste0("Found ", nrow(substations_with_names_sf), " named and ",
               nrow(substations_without_names_sf), " unnamed substations."))

if (nrow(substations_without_names_sf) == 0) {
  message("No unnamed substations to process. Skipping nearest neighbor search.")
  final_substations_sf <- substations_with_names_sf
} else {
  # --- 2. Find the closest city for each unnamed substation ---
  message("Finding the nearest city for each unnamed substation...")
  # Convert cities_df to an sf object with the same CRS
  cities_sf <- st_as_sf(cities_df, coords = c("x", "y"), crs = st_crs(power_substations_sf))
  
  closest_city_indices <- st_nearest_feature(substations_without_names_sf, cities_sf)

  # --- 3. Assign the closest city's name to the unnamed substations ---
  closest_city_names <- cities_sf$name[closest_city_indices]

  substations_without_names_sf <- substations_without_names_sf %>%
    mutate(
      original_name = name, # Preserve the original NA name if needed
      name = paste("Substation near", closest_city_names),
      inferred_name = TRUE
    )

  # --- 4. Prepare substations with original names for binding ---
  # Add the same columns to the named substations to ensure a consistent structure
  substations_with_names_sf <- substations_with_names_sf %>%
    mutate(
      original_name = name,
      inferred_name = FALSE
    )

  # --- 5. Combine the named and now-named substations ---
  message("Combining substations with original names and inferred names...")
  final_substations_sf <- bind_rows(substations_with_names_sf, substations_without_names_sf)
  
  # The final data frame should now have all substations and an 'inferred_name' flag
  # Convert the final sf object back to a data frame for easier viewing if needed
  final_substations_df <- final_substations_sf %>%
    mutate(x = st_coordinates(geometry)[,1],
           y = st_coordinates(geometry)[,2]) %>%
    as_tibble() %>%
    select(-geometry)
}

# --- 6. Inspect the result and plot for verification ---
message("\n--- Final Substation Data Frame (Head) ---")
print(head(final_substations_df))
message(paste0("Total substations in final data frame: ", nrow(final_substations_df)))
message(paste0("Number of inferred names: ", sum(final_substations_df$inferred_name, na.rm = TRUE)))

# Plot the result (using the `final_substations_df` tibble)
# ... The plotting code remains the same from the previous response ...

# --- 5. Inspect the result and plot for verification ---
message("\n--- Final Substation Data Frame (Head) ---")
print(head(final_substations_df))
message(paste0("Total substations in final data frame: ", nrow(final_substations_df)))
message(paste0("Number of inferred names: ", sum(final_substations_df$inferred_name, na.rm = TRUE)))

# Plot the result to visualize the substations and their inferred names
# Note: You'll need a background map (e.g., 'emilia_romagna_bbox') to make sense of this plot.
# This assumes 'streets' and 'cities_df' are available from previous steps.

# Let's create a simplified plot to show the substations and their inferred names
ggplot() +
  geom_sf(data = venezuela_coastline_osm$osm_lines, color = "darkblue", size = 0.5, alpha = 0.6) + # Coastline
  geom_sf(data = venezuela_polygon, fill = NA, color = "darkgreen", size = 1) + # Country boundary

  # Plot cities in the background for context
  geom_point(data = cities_df, aes(x = x, y = y), color = "gray", size = 1, alpha = 0.5) +
  geom_text(data = cities_df, aes(x = x, y = y, label = name),
            color = "gray", size = 2, check_overlap = TRUE, nudge_x = 0.05) +
  # Plot the substations
#  geom_point(data = final_substations_df, aes(x = x, y = y, color = inferred_name), size = 2) +
  geom_point(data = final_substations_df, aes(x = x, y = y), size = 0.5) +
  geom_text(data = final_substations_df, aes(x = x, y = y, label = name),
            color = "black", size = 2, nudge_x = 0.01, nudge_y = 0.01, check_overlap = TRUE) +
#  scale_color_manual(values = c("FALSE" = "red", "TRUE" = "blue"),
#                     labels = c("Original Name", "Inferred Name")) +
  labs(title = "Power Substations with Original and Inferred Names",
       subtitle = "Substations are colored by name origin; cities are in gray for context.",
       color = "Name Source") +
  theme_minimal()

################################################################### Venezuela ok

# Load necessary libraries
library(osmdata)    # For OpenStreetMap data
library(sf)         # For spatial data operations
library(sfnetworks) # For network analysis on spatial data
library(igraph)     # For graph analysis and centrality calculations
library(dplyr)      # For data manipulation (e.g., %>% and data frames)
library(ggplot2)    # For plotting
library(sp)         # For point.in.polygon
library(RColorBrewer) # For generating distinct colors for plots
library(stringr)    # For string manipulation (str_detect)
library(tibble)     # For creating tibbles
setwd("C:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
# --- 0. Data Acquisition and Network Setup ---
message("--- Setting up the Venezuela Power Network ---")
venezuela_bbox <- c(left = -73.42, bottom = 0.63, right = -59.78, top = 12.2)
message(paste0("Defined initial bounding box for Venezuela: ",
               "Left=", venezuela_bbox["left"], ", Bottom=", venezuela_bbox["bottom"],
               ", Right=", venezuela_bbox["right"], ", Top=", venezuela_bbox["top"]))

message("Querying for Venezuela's national administrative boundary...")
venezuela_boundary_osm <- opq(bbox = venezuela_bbox) %>%
  add_osm_feature(key = "admin_level", value = "2") %>%
  add_osm_feature(key = "name:en", value = "Venezuela") %>%
  osmdata_sf()
venezuela_polygon <- venezuela_boundary_osm$osm_multipolygons
if (is.null(venezuela_polygon) || nrow(venezuela_polygon) == 0) {
  venezuela_polygon <- venezuela_boundary_osm$osm_polygons
}
venezuela_polygon <- venezuela_polygon %>%
  filter(name %in% c("Venezuela", "Bolivarian Republic of Venezuela")) %>%
  slice(1)
message("Successfully retrieved Venezuela's administrative boundary.")

venezuela_coastline_osm <- venezuela_bbox %>%
  opq() %>%
  add_osm_feature(key = "natural", value = "coastline") %>%
  osmdata_sf()

message("Querying OpenStreetMap for electric power lines...")
power_lines_osm <- opq(venezuela_bbox) %>%
  add_osm_feature(key = "power", value = c("line", "cable")) %>%
  osmdata_sf()
power_lines_sf <- power_lines_osm$osm_lines %>%
  st_filter(venezuela_polygon, .predicate = st_within)
if (!is.null(power_lines_sf)) {
  message(paste0("  Found ", nrow(power_lines_sf), " power lines."))
  power_lines_hv <- power_lines_sf %>% filter(str_detect(voltage, "230000|400000|765000"))
  message(paste0("  Found ", nrow(power_lines_hv), " high-voltage power lines."))
} else {
  message("  No power lines found for the specified area/tags.")
  power_lines_hv <- NULL
}

message("Querying OpenStreetMap for power substations...")
power_substations_osm <- opq(venezuela_bbox) %>%
  add_osm_feature(key = "power", value = "substation") %>%
  osmdata_sf()
power_substations_sf <- power_substations_osm$osm_points %>%
  st_filter(venezuela_polygon, .predicate = st_within)
if (!is.null(power_substations_sf)) {
  message(paste0("  Found ", nrow(power_substations_sf), " power substations (as points)."))
} else {
  message("  No power substations found for the specified area/tags.")
  power_substations_sf <- NULL
}
message("Power system data acquisition complete.")

substations_sf <-power_substations_sf

# --- 1. Process Substations: Assign Names to Unnamed Substations ---
message("\n--- Processing Substations: Assigning names to unnamed substations ---")
message("  Fetching cities data for substation naming...")
cities_osm_full <- venezuela_bbox %>% opq() %>% add_osm_feature(key = "place", value = c("city", "town")) %>% osmdata_sf()
cities_points_full <- cities_osm_full$osm_points %>% filter(!is.na(name)) %>% st_filter(venezuela_polygon, .predicate = st_within)
cities_df <- cities_points_full %>% mutate(x = st_coordinates(geometry)[,1], y = st_coordinates(geometry)[,2]) %>% as_tibble() %>% select(osm_id, name, place, population, capital, x, y)
message(paste0("  Loaded ", nrow(cities_df), " cities for substation naming."))

substations_with_names_sf <- power_substations_sf %>% filter(!is.na(name))
substations_without_names_sf <- power_substations_sf %>% filter(is.na(name))
message(paste0("  Found ", nrow(substations_with_names_sf), " named and ",
               nrow(substations_without_names_sf), " unnamed substations."))

if (nrow(substations_without_names_sf) == 0) {
  message("  No unnamed substations to process. Skipping nearest neighbor search.")
  final_substations_sf <- substations_with_names_sf
} else {
  message("  Finding the nearest city for each unnamed substation...")
  cities_sf <- st_as_sf(cities_df, coords = c("x", "y"), crs = st_crs(power_substations_sf))
  closest_city_indices <- st_nearest_feature(substations_without_names_sf, cities_sf)
  closest_city_names <- cities_sf$name[closest_city_indices]
  substations_without_names_sf <- substations_without_names_sf %>%
    mutate(original_name = name, name = paste("Substation near", closest_city_names), inferred_name = TRUE)
  substations_with_names_sf <- substations_with_names_sf %>%
    mutate(original_name = name, inferred_name = FALSE)
  message("  Combining substations with original names and inferred names...")
  final_substations_sf <- bind_rows(substations_with_names_sf, substations_without_names_sf)
}
final_substations_df <- final_substations_sf %>%
  mutate(x = st_coordinates(geometry)[,1], y = st_coordinates(geometry)[,2]) %>%
  as_tibble() %>% select(-geometry)

# --- 2. Simplify Power Lines to Single Start-to-End Segments ---
message("\n--- Simplifying power lines to single start-to-end segments ---")
power_lines_processed_sf <- power_lines_hv %>% st_cast("LINESTRING")
if (nrow(power_lines_processed_sf) == 0) {
  stop("The 'power_lines_hv' object is empty or became empty after casting. Cannot simplify.")
}
line_coords <- st_coordinates(power_lines_processed_sf) %>% as_tibble()
line_endpoints_df <- line_coords %>%
  group_by(L1) %>%
  summarise(x_start = first(X), y_start = first(Y), x_end = last(X), y_end = last(Y), .groups = "drop") %>%
  left_join(tibble(L1 = seq_len(nrow(power_lines_processed_sf)), line_osm_id = power_lines_processed_sf$osm_id, line_name = power_lines_processed_sf$name, line_voltage = power_lines_processed_sf$voltage), by = "L1")
power_lines_simplified_sf <- line_endpoints_df %>%
  rowwise() %>%
  mutate(geometry = st_sfc(st_linestring(matrix(c(x_start, y_start, x_end, y_end), nrow = 2, byrow = TRUE)), crs = st_crs(power_lines_processed_sf))) %>%
  ungroup() %>% st_as_sf() %>% select(line_osm_id, line_name, line_voltage, geometry) %>% rename(osm_id = line_osm_id, name = line_name, voltage = line_voltage)
message(paste0("  Simplified ", nrow(power_lines_simplified_sf), " power lines to single segments."))

# --- 3. Create the Edge List by Linking Lines to Closest Substations ---
message("\n--- Creating the Edge List by Linking Lines to Closest Substations ---")
simplified_line_coords_for_search <- st_coordinates(power_lines_simplified_sf) %>% as_tibble()
simplified_line_endpoints_for_search <- simplified_line_coords_for_search %>%
  group_by(L1) %>% summarise(x_start = first(X), y_start = first(Y), x_end = last(X), y_end = last(Y), .groups = "drop")
line_starts_for_search_sf <- st_as_sf(simplified_line_endpoints_for_search, coords = c("x_start", "y_start"), crs = st_crs(substations_sf))
line_ends_for_search_sf <- st_as_sf(simplified_line_endpoints_for_search, coords = c("x_end", "y_end"), crs = st_crs(substations_sf))
substations_sf_for_lookup <- substations_sf %>% mutate(osm_id_char = as.character(osm_id))
start_substation_indices <- st_nearest_feature(line_starts_for_search_sf, substations_sf_for_lookup)
end_substation_indices <- st_nearest_feature(line_ends_for_search_sf, substations_sf_for_lookup)
edge_list_df <- tibble(
  line_osm_id = power_lines_simplified_sf$osm_id,
  from = substations_sf_for_lookup$osm_id_char[start_substation_indices],
  to = substations_sf_for_lookup$osm_id_char[end_substation_indices],
  line_name = power_lines_simplified_sf$name,
  line_voltage = power_lines_simplified_sf$voltage
)
edge_list_cleaned <- edge_list_df %>%
  filter(from != to) %>% rowwise() %>%
  mutate(from_sorted = min(from, to), to_sorted = max(from, to)) %>%
  ungroup() %>% distinct(from_sorted, to_sorted, .keep_all = TRUE) %>% select(-from_sorted, -to_sorted)
message(paste0("  Created an edge list with ", nrow(edge_list_cleaned), " connections between substations."))


# --- 4. CREATE THE IGRAPH OBJECT USING THE USER'S WORKING METHOD ---
message("\n--- Creating the Igraph Object with the user's working method ---")

# Step 1: Get all unique node IDs from the cleaned edge list
all_connected_osm_ids <- unique(c(edge_list_cleaned$from, edge_list_cleaned$to))

# Step 2: Create a mapping data frame from original osm_id to a simple integer node_id
node_id_mapping <- tibble(
  osm_id = all_connected_osm_ids,
  node_id = 1:length(all_connected_osm_ids)
)
message(paste0("  Created a mapping for ", nrow(node_id_mapping), " unique OSM IDs to sequential integers."))

# Step 3: Create the final edge list for igraph with integer IDs
edge_list_with_int_ids <- edge_list_cleaned %>%
  left_join(node_id_mapping %>% rename(from_node_id = node_id), by = c("from" = "osm_id")) %>%
  left_join(node_id_mapping %>% rename(to_node_id = node_id), by = c("to" = "osm_id")) %>%
  select(-from, -to) %>%
  rename(from = from_node_id, to = to_node_id)
message("  Replaced character node IDs with integer IDs in the edge list.")

# Step 4: Create the node data frame for igraph with all attributes
node_list_with_attributes <- node_id_mapping %>%
  left_join(
    final_substations_df %>%
      mutate(osm_id = as.character(osm_id)) %>%
      select(osm_id, name, x, y, inferred_name),
    by = "osm_id"
  ) %>%
  rename(node_name = name)

if (any(is.na(node_list_with_attributes$x)) || any(is.na(node_list_with_attributes$y))) {
  failed_joins <- node_list_with_attributes %>% filter(is.na(x) | is.na(y))
  stop(paste0("CRITICAL JOIN ERROR: Found ", nrow(failed_joins), " nodes that did not get coordinates after the join. ",
              "This indicates a data integrity problem between the edge list and the substation data. ",
              "First 10 failed IDs: ", paste(head(failed_joins$osm_id, 10), collapse = ", ")))
} else {
  message("  SUCCESS: All nodes in the new list have valid coordinates after the join.")
}

# --- CRITICAL FIX: Use the user's working method to create the graph ---
# We pass the integer edge list directly to the base graph() function.
message("\n--- Using graph() function with integer IDs, as confirmed to be working ---")
power_grid_igraph <- igraph::graph(as.vector(t(cbind(edge_list_with_int_ids$from, edge_list_with_int_ids$to))), directed=FALSE)
gtempo <-power_grid_igraph 
gtempo1 <- simplify(delete_vertices(gtempo, which(components(gtempo)$membership!=which.max(components(gtempo)$csize))))


# Now we manually add all the vertex attributes back to the graph object.
# This ensures all the plotting data is attached to the graph.
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "node_id", value = node_list_with_attributes$node_id)
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "osm_id", value = node_list_with_attributes$osm_id)
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "node_name", value = node_list_with_attributes$node_name)
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "x", value = node_list_with_attributes$x)
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "y", value = node_list_with_attributes$y)
power_grid_igraph <- igraph::set_vertex_attr(power_grid_igraph, "inferred_name", value = node_list_with_attributes$inferred_name)

message("\n--- New Igraph Object Summary ---")
print(power_grid_igraph)
message("\nGraph nodes (first 10) with their attributes:")
print(tibble(
  node_id = V(power_grid_igraph)$node_id[1:10],
  osm_id = V(power_grid_igraph)$osm_id[1:10],
  name = V(power_grid_igraph)$node_name[1:10],
  x = V(power_grid_igraph)$x[1:10],
  y = V(power_grid_igraph)$y[1:10]
))


# --- 5. Plot the New Topological Network ---
message("\n--- Plotting the New Topological Network ---")

windows()

plotting_nodes_df <- tibble(
  osm_id = V(power_grid_igraph)$osm_id,
  name = V(power_grid_igraph)$node_name,
  x = V(power_grid_igraph)$x,
  y = V(power_grid_igraph)$y,
  inferred_name = V(power_grid_igraph)$inferred_name
)

plot_network <-ggplot() +
  geom_sf(data = venezuela_coastline_osm$osm_lines, color = "darkblue", size = 0.5, alpha = 0.6) + # Coastline
  geom_sf(data = venezuela_polygon, fill = NA, color = "darkgreen", size = 1) + # Country boundary

  geom_sf(data = power_lines_hv, color = "gray", linewidth = 0.5, alpha = 0.4) +
#  geom_sf(data = power_lines_simplified_sf, color = "darkgreen", linewidth = 0.8, alpha = 0.7) +
#  geom_point(data = plotting_nodes_df, aes(x = x, y = y, color = inferred_name), size = 3, alpha = 0.8) +
  geom_point(data = plotting_nodes_df, aes(x = x, y = y), size = 1.5, alpha = 0.8) +
#  geom_text(data = plotting_nodes_df, aes(x = x, y = y, label = name),
#            size = 2, check_overlap = TRUE, nudge_x = 0.01, color = "black") +
  geom_segment(data = igraph::as_data_frame(power_grid_igraph, what = "edges"),
               aes(x = V(power_grid_igraph)$x[from],
                   y = V(power_grid_igraph)$y[from],
                   xend = V(power_grid_igraph)$x[to],
                   yend = V(power_grid_igraph)$y[to]),
#               color = "red", size = 0.5, linetype = "dashed") +
               color = "black", size = 0.5) +
#  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "blue")) +
  labs(title = "Venezuela Power Grid: Topological Network ") + #,
#       subtitle = "Nodes are substations, dashed red lines are inferred connections. Green lines are simplified power lines.",
#       color = "Name Inferred") +
  theme_minimal()
print(plot_network)

 ggsave("Ven_OSM.png", plot = plot_network, width = 10, height = 8, dpi = 300)
 ggsave("Ven_OSM.pdf", plot = plot_network, width = 10, height = 8, dpi = 300)

 layout_tempo=cbind(plotting_nodes_df$x,plotting_nodes_df$y)
# plot(gtempo,layout=layout_tempo,vertex.size=1,vertex.label=NA)
 g <- gtempo

layout_orig <-layout_tempo


#####################################
####################################
# R Script for Urban Network Analysis
# This tool uses OpenStreetMap data to analyze different urban planning loops.
# You can easily change the city and define as many loops as you need.

# ==============================================================================
# INSTRUCTIONS FOR USE:
#
# To analyze a different city or area, simply change the four coordinate values
# in the 'bbox' variable below (left, bottom, right, top).
#
# To add more loops for analysis, add more entries to the 'loops' list in
# Phase 3, following the existing format.
# ==============================================================================

# Load necessary libraries
library(osmdata)
library(sf)
library(sfnetworks)
library(igraph)
library(dplyr)
library(ggplot2)
library(stringr)
library(tibble)

# --- 1. Data Acquisition and GWN Construction ---
message("--- Phase 1: Acquiring Real Geographic Data and Building the GWN ---")
# Define a larger bounding box for a part of Caracas, Venezuela.
caracas_bbox <- c(left = -66.95, bottom = 10.46, right = -66.85, top = 10.51)
caracas_bbox <-  c(left = -73.42, bottom = 0.63, right = -59.78, top = 12.2)#c(11.15, 44.35, 11.55, 44.65)
message(paste0("Fetching urban road network data for a larger area of Caracas..."))
urban_streets_osm <- opq(caracas_bbox) %>%
  add_osm_feature(key = "highway", value = c("primary", "secondary", "tertiary", "residential", "unclassified")) %>%
  osmdata_sf()

if (is.null(urban_streets_osm$osm_lines)) {
  stop("No urban streets found. Please adjust the bounding box or check your internet connection.")
}

gwn_sfnetwork <- as_sfnetwork(urban_streets_osm$osm_lines, directed = FALSE)
message(paste0("  Initial GWN created with ", igraph::vcount(gwn_sfnetwork),
               " intersections and ", igraph::ecount(gwn_sfnetwork), " streets."))

# --- 2. Calculate Centrality for the Entire GWN ---
message("\n--- Phase 2: Computing Centrality for the Entire GWN ---")
message("  Calculating Betweenness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(betweenness = igraph::betweenness(., normalized = TRUE))

message("  Calculating Closeness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(closeness = igraph::closeness(., normalized = TRUE))

message("  Centrality measures added to all intersections in the GWN.")

# --- 3. Define User-Provided Loops (as Geographic Polygons) ---
message("\n--- Phase 3: Defining User-Provided Urban Loops ---")
# Define two loops with clear geometric boundaries. You can add as many as you need here.

# Loop 1: A smaller, compact loop (simulating a historic district or central business core)
loop_1_coords <- matrix(c(
  -66.90, 10.480,
  -66.89, 10.485,
  -66.88, 10.475,
  -66.89, 10.470,
  -66.90, 10.480
), ncol = 2, byrow = TRUE)
loop_1_polygon_sfg <- st_polygon(list(loop_1_coords))

# Loop 2: A larger, more spread-out loop (simulating a traffic beltway or a larger planning area)
loop_2_coords <- matrix(c(
  -66.93, 10.50,
  -66.88, 10.51,
  -66.86, 10.46,
  -66.91, 10.45,
  -66.93, 10.50
), ncol = 2, byrow = TRUE)
loop_2_polygon_sfg <- st_polygon(list(loop_2_coords))

# Assign the CRS of the GWN to the loop polygons to prevent the CRS mismatch error.
crs_of_gwn <- st_crs(gwn_sfnetwork)
loop_1_polygon <- st_sfc(loop_1_polygon_sfg, crs = crs_of_gwn)
loop_2_polygon <- st_sfc(loop_2_polygon_sfg, crs = crs_of_gwn)

# Combine the loops into a list for easy iteration. Add more loops here if needed.
loops <- list(
  list(name = "Historic Core Loop", polygon = loop_1_polygon, color = "darkblue"),
  list(name = "Traffic Beltway Loop", polygon = loop_2_polygon, color = "darkred")
)
message(paste0("  ", length(loops), " urban loops have been defined and their CRS's have been set."))

# --- 4. GWN (Winding Number) and Cost/Benefit Analysis ---
message("\n--- Phase 4: GWN Analysis and Cost/Benefit Scoring ---")
results <- data.frame(
  loop_name = character(),
  nodes_in_loop = integer(), # NEW: Added to show how many nodes are inside each loop
  area_sq_km = numeric(),
  avg_betweenness = numeric(),
  avg_closeness = numeric(),
  betweenness_score = numeric(),
  closeness_score = numeric(),
  stringsAsFactors = FALSE
)

# Loop through each user-defined loop to perform the analysis
for (loop in loops) {
  message(paste0("  Analyzing '", loop$name, "'..."))
  
  # Compute "Winding Number" (a practical proxy via spatial filtering)
  loop_nodes_sf <- st_as_sf(gwn_sfnetwork, "nodes")
  nodes_inside_loop <- loop_nodes_sf[st_within(loop_nodes_sf, loop$polygon, sparse = FALSE), ]
  
  if (nrow(nodes_inside_loop) == 0) {
    message(paste0("    Warning: No nodes found inside '", loop$name, "'. Skipping analysis."))
    next
  }
  
  # Calculate Metrics (Benefit)
  avg_betweenness_inside <- mean(nodes_inside_loop$betweenness, na.rm = TRUE)
  avg_closeness_inside <- mean(nodes_inside_loop$closeness, na.rm = TRUE)
  message(paste0("    Nodes in loop: ", nrow(nodes_inside_loop)))
  message(paste0("    Average Betweenness inside: ", round(avg_betweenness_inside, 4)))
  message(paste0("    Average Closeness inside: ", round(avg_closeness_inside, 4)))
  
  # Calculate Cost (Area)
  loop_area_sq_m <- st_area(loop$polygon)
  loop_area_sq_km <- as.numeric(loop_area_sq_m) / 10^6
  message(paste0("    Loop Area (Cost proxy): ", sprintf("%.4f", loop_area_sq_km), " sq km"))
  
  # Compute Cost/Benefit Scores
  if (loop_area_sq_km > 0) {
    betweenness_score <- avg_betweenness_inside / loop_area_sq_km
    closeness_score <- avg_closeness_inside / loop_area_sq_km
  } else {
    betweenness_score <- NA
    closeness_score <- NA
    message("    Warning: Area is zero. Cost/Benefit scores cannot be computed.")
  }

  # Add results to our data frame
  results <- results %>%
    add_row(
      loop_name = loop$name,
      nodes_in_loop = nrow(nodes_inside_loop),
      area_sq_km = loop_area_sq_km,
      avg_betweenness = avg_betweenness_inside,
      avg_closeness = avg_closeness_inside,
      betweenness_score = betweenness_score,
      closeness_score = closeness_score
    )
}

# --- 5. Plotting the Results ---
message("\n--- Phase 5: Plotting the Final Analysis ---")
# Plotting explanation:
# The light gray lines show the entire GWN for context.
# The black lines are the networks of roads UNDER STUDY inside the loops.
# The dashed lines mark the loops selected for analysis.

combined_polygons <- c(loop_1_polygon, loop_2_polygon)
nodes_within_any_loop_matrix <- st_within(st_as_sf(gwn_sfnetwork, "nodes"), combined_polygons, sparse = FALSE)
nodes_within_any_loop_vector <- rowSums(nodes_within_any_loop_matrix) > 0
all_nodes_in_loops <- st_as_sf(gwn_sfnetwork, "nodes")[nodes_within_any_loop_vector, ]

all_nodes_in_loops <- all_nodes_in_loops %>%
  mutate(loop_membership = case_when(
    st_within(geometry, loop_1_polygon, sparse = FALSE) ~ loops[[1]]$name,
    st_within(geometry, loop_2_polygon, sparse = FALSE) ~ loops[[2]]$name,
    TRUE ~ "None"
  ))

loop_labels_df <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name),
  polygon = c(loops[[1]]$polygon, loops[[2]]$polygon),
  color = c(loops[[1]]$color, loops[[2]]$color)
) %>%
  st_as_sf()

loop_labels_df$geometry <- st_centroid(loop_labels_df$polygon)

loop_labels_df <- loop_labels_df %>%
  mutate(x = st_coordinates(geometry)[, 1], y = st_coordinates(geometry)[, 2])
windows()
plot1 <- ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges")[rowSums(st_within(st_as_sf(gwn_sfnetwork, "edges"), combined_polygons, sparse = FALSE)) > 0, ], color = "black", linewidth = 0.7, alpha = 0.7) +
  geom_sf(data = loop_1_polygon, fill = NA, color = loops[[1]]$color, linewidth = 1.2, linetype = "dashed") +
  geom_sf(data = loop_2_polygon, fill = NA, color = loops[[2]]$color, linewidth = 1.2, linetype = "dashed") +
  geom_sf(data = all_nodes_in_loops, aes(size = betweenness, color = loop_membership), alpha = 0.8) +
  geom_label(data = loop_labels_df,
             aes(x = x, y = y, label = name),
             size = 3, color = loop_labels_df$color) +
  scale_color_manual(values = c("Historic Core Loop" = loops[[1]]$color, "Traffic Beltway Loop" = loops[[2]]$color), name = "Loop") +
  scale_size_continuous(range = c(1, 8), name = "Betweenness Score") +
  labs(title = "Urban Network Analysis: Cost/Benefit per Loop",
       subtitle = "Node size indicates betweenness centrality. Dashed lines mark the analyzed loops.") +
  theme_minimal() +
  theme(legend.position = "right")

# --- 6. Final Report for the Mayor ---
message("\n--- Final Report: Strategic Insights for Your City ---")
message("Below are the key metrics for the two loops we analyzed. A higher score indicates a better return on investment (ROI) for a project focused on improving these metrics.\n")
print(results)

message("\nAnalysis:")
message("The 'Cost/Benefit Score' is a powerful tool to compare two different project areas.")
message(" - Nodes in Loop: This is our 'Geometric Winding Number' proxy, providing the number of intersections within the loop.")
message(" - Betweenness Score: This tells you which loop contains a greater concentration of critical 'chokepoints' relative to its size (cost). A higher score here suggests this is a more impactful area to focus on for reducing traffic congestion or improving resilience.")
message(" - Closeness Score: This tells you which loop contains a greater concentration of 'hubs' or well-connected locations relative to its size. A higher score here suggests this area is a better candidate for projects that rely on high accessibility, such as new transit lines or business districts.")
message("\nBased on these scores, you can make a data-driven decision on where to focus your resources to achieve your specific planning goals.")

print(plot1)
windows()

plot2 <-ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges")[rowSums(st_within(st_as_sf(gwn_sfnetwork, "edges"), combined_polygons, sparse = FALSE)) > 0, ], color = "black", linewidth = 0.7, alpha = 0.7) +
  geom_sf(data = loop_1_polygon, fill = NA, color = loops[[1]]$color, linewidth = 1.2, linetype = "dashed") +
  geom_sf(data = loop_2_polygon, fill = NA, color = loops[[2]]$color, linewidth = 1.2, linetype = "dashed") +
#  geom_sf(data = all_nodes_in_loops, aes(size = betweenness, color = loop_membership), alpha = 0.8) +
#  geom_label(data = loop_labels_df,
#             aes(x = x, y = y, label = name),
#             size = 3, color = loop_labels_df$color) +
#  scale_color_manual(values = c("Historic Core Loop" = loops[[1]]$color, "Traffic Beltway Loop" = loops[[2]]$color), name = "Loop") +
#  scale_size_continuous(range = c(1, 8), name = "Betweenness Score") +
  labs(title = "Urban Network Analysis: Cost/Benefit per Loop",
       subtitle = "Node size indicates betweenness centrality. Dashed lines mark the analyzed loops.") +
  theme_minimal() #+
#  theme(legend.position = "right")

print(plot2)

################################
#Comprehensive R Script with Initial GWN Plot Fix
# Load necessary libraries
library(osmdata)
library(sf)
library(sfnetworks)
library(igraph)
library(dplyr)
library(ggplot2)
library(stringr)
library(tibble)

# --- 1. Data Acquisition and GWN Construction ---
message("--- Phase 1: Acquiring Real Geographic Data and Building the GWN ---")
# EXPANDED BOUNDING BOX for Caracas to include major highways
caracas_bbox <- c(left = -67.10, bottom = 10.40, right = -66.70, top = 10.60)
caracas_bbox <- c(left = -67.10, bottom = 10.40, right = -66.70, top = 10.55)
message(paste0("Fetching urban road network data for a larger area of Caracas..."))
urban_streets_osm <- opq(caracas_bbox) %>%
  # ADDED "motorway" and "trunk" to highway types
  add_osm_feature(key = "highway", value = c("motorway", "trunk", "primary", "secondary", "tertiary", "residential", "unclassified")) %>%
  osmdata_sf()

# --- 1. Data Acquisition and GWN Construction ---
#message("--- Phase 1: Acquiring Real Geographic Data and Building the GWN ---")
#caracas_bbox <- c(left = -66.95, bottom = 10.46, right = -66.85, top = 10.51)
#message(paste0("Fetching urban road network data for a larger area of Caracas..."))
#urban_streets_osm <- opq(caracas_bbox) %>%
#  add_osm_feature(key = "highway", value = c("primary", "secondary", "tertiary", "residential", "unclassified")) %>%
#  osmdata_sf()

if (is.null(urban_streets_osm$osm_lines)) {
  stop("No urban streets found. Please adjust the bounding box or check your internet connection.")
}
gwn_sfnetwork <- as_sfnetwork(urban_streets_osm$osm_lines, directed = FALSE)
message(paste0("  Initial GWN created with ", igraph::vcount(gwn_sfnetwork),
               " intersections and ", igraph::ecount(gwn_sfnetwork), " streets."))

# --- 2. Calculate Centrality for the Entire GWN ---
message("\n--- Phase 2: Computing Centrality for the Entire GWN ---")
message("  Calculating Betweenness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(betweenness = igraph::betweenness(., normalized = TRUE))

message("  Calculating Closeness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(closeness = igraph::closeness(., normalized = TRUE))
message("  Centrality measures added to all intersections in the GWN.")

# --- 3. Define User-Provided Loops (as Geographic Polygons) ---
message("\n--- Phase 3: Defining User-Provided Urban Loops ---")
loop_1_coords <- matrix(c(
  -66.90, 10.480,
  -66.89, 10.485,
  -66.88, 10.475,
  -66.89, 10.470,
  -66.90, 10.480
), ncol = 2, byrow = TRUE)
loop_1_polygon <- st_polygon(list(loop_1_coords))

loop_2_coords <- matrix(c(
  -66.93, 10.50,
  -66.88, 10.51,
  -66.86, 10.46,
  -66.91, 10.45,
  -66.93, 10.50
), ncol = 2, byrow = TRUE)
loop_2_polygon <- st_polygon(list(loop_2_coords))

network_crs <- st_crs(gwn_sfnetwork)

loop_1_polygon_sfg <- loop_1_polygon
loop_2_polygon_sfg <- loop_2_polygon

loop_1_polygon_sf <- st_sfc(loop_1_polygon_sfg, crs = network_crs)
loop_2_polygon_sf <- st_sfc(loop_2_polygon_sfg, crs = network_crs)

loops <- list(
  list(name = "Historic Core Loop", polygon = loop_1_polygon_sf, color = "darkblue"),
  list(name = "Traffic Beltway Loop", polygon = loop_2_polygon_sf, color = "darkred")
)
message(paste0("  Two urban loops have been defined for comparison."))

# --- NEW: Initial Plot of the entire GWN with Loops ---
message("\n--- Initial Plot: Entire GWN with Defined Loops ---")

plot_loops_df_initial <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], crs = network_crs)
) %>%
  st_as_sf()

# FIX: Create a single data frame for labels in the initial plot
initial_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], crs = network_crs)
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )
windows()
plot4 <- ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "black", linewidth = 0.5, alpha = 0.6) + # All streets
  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "black", size = 0.7, alpha = 0.6) + # All intersections
  geom_sf(data = plot_loops_df_initial, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") + # Loop boundaries
  # FIX: Use the new initial_labels_df and map aesthetics directly
#  geom_label(data = initial_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  scale_color_manual(values = c("Historic Core Loop" = loops[[1]]$color, "Traffic Beltway Loop" = loops[[2]]$color), name = "Loop Name") +
  labs(title = "General Web Network (GWN) with Defined Urban Loops",
       subtitle = "The entire road network with the areas selected for analysis.") +
  theme_minimal() +
  theme(legend.position = "bottom")

print(plot4)
setwd("C:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
 ggsave("ccs_OSM.jpeg", plot = plot4, width = 10, height = 8, dpi = 300)
 ggsave("ccs_OSM.pdf", plot = plot4, width = 10, height = 8, dpi = 300)

message("  Initial GWN and loop plot generated.")

# --- 4. GWN (Spatial Filtering) and Multi-Attribute Analysis ---
message("\n--- Phase 4: GWN Analysis and Multi-Attribute Scoring ---")
results <- data.frame(
  loop_name = character(),
  area_sq_km = numeric(),
  num_enclosed_nodes = integer(),
  avg_betweenness = numeric(),
  avg_closeness = numeric(),
  stringsAsFactors = FALSE
)

for (loop in loops) {
  message(paste0("  Analyzing '", loop$name, "'..."))
  loop_nodes_sf <- st_as_sf(gwn_sfnetwork, "nodes")
  
  nodes_inside_loop <- loop_nodes_sf[st_within(loop_nodes_sf, loop$polygon, sparse = FALSE), ]
  
  if (nrow(nodes_inside_loop) == 0) {
    message(paste0("    Warning: No nodes found inside '", loop$name, "'. Skipping analysis."))
    next
  }
  
  num_enclosed_nodes <- nrow(nodes_inside_loop)
  avg_betweenness_inside <- mean(nodes_inside_loop$betweenness, na.rm = TRUE)
  avg_closeness_inside <- mean(nodes_inside_loop$closeness, na.rm = TRUE)
  
  loop_area_sq_m <- st_area(loop$polygon)
  loop_area_sq_km <- as.numeric(loop_area_sq_m) / 10^6
  
  message(paste0("    Enclosed Nodes: ", num_enclosed_nodes))
  message(paste0("    Average Betweenness: ", round(avg_betweenness_inside, 4)))
  message(paste0("    Average Closeness: ", round(avg_closeness_inside, 4)))
  message(paste0("    Loop Area: ", round(loop_area_sq_km, 2), " sq km"))
  
  results <- results %>%
    add_row(
      loop_name = loop$name,
      area_sq_km = loop_area_sq_km,
      num_enclosed_nodes = num_enclosed_nodes,
      avg_betweenness = avg_betweenness_inside,
      avg_closeness = avg_closeness_inside
    )
}

# --- 5. Multi-Attribute Decision Making ---
message("\n--- Phase 5: Multi-Attribute Decision Making (Normalization & Weighting) ---")
weights <- c(
  num_enclosed_nodes = 0.20,
  avg_betweenness = 0.40,
  avg_closeness = 0.30,
  area_sq_km = 0.10
)

if (sum(weights) != 1.0) {
  stop("Weights must sum to 1.0. Please adjust your weights.")
}

normalized_results <- results %>%
  mutate(
    norm_num_enclosed_nodes = (num_enclosed_nodes - min(num_enclosed_nodes)) / (max(num_enclosed_nodes) - min(num_enclosed_nodes)),
    norm_avg_betweenness = (avg_betweenness - min(avg_betweenness)) / (max(avg_betweenness) - min(avg_betweenness)),
    norm_avg_closeness = (avg_closeness - min(avg_closeness)) / (max(avg_closeness) - min(avg_closeness)),
    norm_area_sq_km = (max(area_sq_km) - area_sq_km) / (max(area_sq_km) - min(area_sq_km))
  ) %>%
  mutate_at(vars(starts_with("norm_")), ~replace(., is.nan(.), 0.5))

final_scores <- normalized_results %>%
  rowwise() %>%
  mutate(
    composite_score = (norm_num_enclosed_nodes * weights["num_enclosed_nodes"]) +
                      (norm_avg_betweenness * weights["avg_betweenness"]) +
                      (norm_avg_closeness * weights["avg_closeness"]) +
                      (norm_area_sq_km * weights["area_sq_km"])
  ) %>%
  ungroup() %>%
  arrange(desc(composite_score))

message("  Composite scores calculated based on your defined weights.")

# --- 6. Plotting the Results ---
message("\n--- Phase 6: Plotting the Final Analysis ---")

plot_loops_df <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], crs = network_crs)
) %>%
  st_as_sf()

gwn_nodes_sf_for_plotting <- st_as_sf(gwn_sfnetwork, "nodes")

combined_loop_geometry <- st_union(loop_1_polygon_sf, loop_2_polygon_sf)

all_nodes_in_loops_sf <- gwn_nodes_sf_for_plotting[st_within(gwn_nodes_sf_for_plotting, combined_loop_geometry, sparse = FALSE), ]

all_nodes_in_loops_sf <- all_nodes_in_loops_sf %>%
  mutate(loop_membership = case_when(
    st_within(geometry, loop_1_polygon_sf, sparse = FALSE) ~ loops[[1]]$name,
    st_within(geometry, loop_2_polygon_sf, sparse = FALSE) ~ loops[[2]]$name,
    TRUE ~ "None"
  ))

all_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], crs = network_crs)
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )

windows()
# Create the final plot
plot5 <- ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) +
  geom_sf(data = plot_loops_df, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") +
  geom_sf(data = all_nodes_in_loops_sf, aes(size = betweenness, color = loop_membership), alpha = 0.8) +
  geom_label(data = all_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  scale_size_continuous(range = c(1, 8), name = "Betweenness Score") +
  scale_color_manual(values = c("Historic Core Loop" = loops[[1]]$color, "Traffic Beltway Loop" = loops[[2]]$color), name = "Loop") +
  labs(title = "Urban Network Analysis: Multi-Attribute Loop Comparison",
       subtitle = "Node size indicates betweenness centrality. Dashed lines mark the analyzed loops.") +
  theme_minimal() +
  theme(legend.position = "right")
print(plot5)
# --- 7. Final Report for the Mayor ---
message("\n--- Final Report: Strategic Insights for Your City ---")
message("Below are the detailed metrics and composite scores for the two loops. The loop with the highest composite score is recommended based on your defined priorities.\n")
print(final_scores %>% select(loop_name, area_sq_km, num_enclosed_nodes, avg_betweenness, avg_closeness, composite_score))

message("\nInterpretation of Composite Score:")
message("The Composite Score combines multiple factors (number of nodes, average centralities, and cost/area) into a single metric, weighted by your strategic priorities.")
message(" - A higher Composite Score indicates a better overall 'value' or 'impact' for a given loop, considering all your objectives.")
message(" - You can adjust the 'weights' in the code (Phase 5) to reflect changing priorities (e.g., if cost becomes more critical, increase the weight for 'area_sq_km').")
message("\nThis framework allows you to make data-driven decisions on where to focus your resources to achieve the most impactful urban development.")


# --- Select Specific Streets to Highlight with Different Colors ---
streets_to_highlight_names <- c(
  "Avenida Libertador",
  "Avenida Francisco de Miranda",
  "Calle Real de Sabana Grande"
)
streets_to_highlight_names

streets_to_highlight_colors <- c(
  "Avenida Libertador" = "orange",
  "Avenida Francisco de Miranda" = "purple",
  "Calle Real de Sabana Grande" = "cyan"
)

# FIX: Prepare highlighted_streets_sf to use 'name' for color aesthetic
highlighted_streets_sf <- gwn_sfnetwork %>%
  activate("edges") %>%
  filter(name %in% streets_to_highlight_names) %>%
  st_as_sf() %>%
  # Crucially, ensure the 'name' column itself holds the value for the color mapping
  # (which is already the case if 'name' is the street name)
  # No need for 'highlight_color' if 'name' is used directly for mapping
  select(name, geometry) # Keep only name and geometry for plotting

if (nrow(highlighted_streets_sf) > 0) {
  message(paste0("  Found ", nrow(highlighted_streets_sf), " segments for the specified streets to highlight."))
} else {
  message("  Warning: No segments found for the specified streets to highlight. Check names or bounding box.")
}

# --- Initial Plot of the entire GWN with Loops and Highlighted Streets ---
message("\n--- Initial Plot: Entire GWN with Defined Loops and Highlighted Streets ---")

plot_loops_df_initial <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], crs = network_crs)
) %>%
  st_as_sf()

initial_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], crs = network_crs)
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )

# FIX: Create a single, comprehensive named vector for all colors in the legend
# Combine loop names and their colors, and street names and their colors
combined_colors_for_legend <- c(
  setNames(plot_loops_df_initial$color, plot_loops_df_initial$name), # Loops
  streets_to_highlight_colors # Highlighted streets
)
windows()
plot44 <- ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "black", linewidth = 0.5, alpha = 0.6) + # All streets
  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "black", size = 0.7, alpha = 0.6) + # All intersections
  # Plot loop boundaries, mapping color to 'name'
  geom_sf(data = plot_loops_df_initial, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") +
  # Plot highlighted streets, also mapping color to 'name'
  geom_sf(data = highlighted_streets_sf, aes(color = name), linewidth = 1.2, inherit.aes = FALSE) +
#  geom_label(data = initial_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  # Use the combined_colors_for_legend in scale_color_manual
  scale_color_manual(values = combined_colors_for_legend, name = "Legend") +
  labs(title = "General Web Network (GWN) with Defined Urban Loops and Highlighted Streets",
       subtitle = "The entire road network with specific streets of interest and analyzed areas.") +
  theme_minimal() +
  theme(legend.position = "bottom")

message("  Initial GWN and loop plot with highlighted streets generated.")
print(plot44)
windows()

plot4 <- ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "black", linewidth = 0.5, alpha = 0.6) + # All streets
  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "black", size = 0.7, alpha = 0.6) + # All intersections
  geom_sf(data = plot_loops_df_initial, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") + # Loop boundaries
  # FIX: Use the new initial_labels_df and map aesthetics directly
#  geom_label(data = initial_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  scale_color_manual(values = c("Historic Core Loop" = loops[[1]]$color, "Traffic Beltway Loop" = loops[[2]]$color), name = "Loop Name") +
  labs(title = "General Web Network (GWN) with Defined Urban Loops",
       subtitle = "The entire road network with the areas selected for analysis.") +
  theme_minimal() +
  theme(legend.position = "bottom")
print(plot4)
ggsave("ccs_OSM_street.jpeg", plot = plot44, width = 10, height = 8, dpi = 300)
 ggsave("ccs_OSM_street.pdf", plot = plot44, width = 10, height = 8, dpi = 300)






###################################
# --- Select Specific Streets to Highlight with Different Colors ---
streets_to_highlight_names <- c(
  "Avenida Libertador",
  "Avenida Francisco de Miranda",
  "Autopista del Este",
  "Avenida Boyacá" # Added Avenida Boyacá (Cota Mil's official name)
)

streets_to_highlight_colors <- c(
  "Avenida Libertador" = "orange",
  "Avenida Francisco de Miranda" = "purple",
  "Autopista del Este" = "green",
  "Avenida Boyacá" = "magenta" # Assign same color as Cota Mil
)


# FIX: Prepare highlighted_streets_sf to use 'name' for color aesthetic
highlighted_streets_sf <- gwn_sfnetwork %>%
  activate("edges") %>%
  filter(name %in% streets_to_highlight_names) %>%
  st_as_sf() %>%
  # Ensure 'name' column is present and will be used for color mapping
  select(name, geometry)

if (nrow(highlighted_streets_sf) > 0) {
  message(paste0("  Found ", nrow(highlighted_streets_sf), " segments for the specified streets to highlight."))
} else {
  message("  Warning: No segments found for the specified streets to highlight. Check names or bounding box.")
}

# --- Initial Plot of the entire GWN with Loops and Highlighted Streets ---
message("\n--- Initial Plot: Entire GWN with Defined Loops and Highlighted Streets ---")

plot_loops_df_initial <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], crs = network_crs)
) %>%
  st_as_sf()

initial_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name),
  color = c(loops[[1]]$color, loops[[2]]$color),
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], crs = network_crs)
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )

# FIX: Create a single, comprehensive named vector for all colors in the legend
# Combine loop names and their colors, and street names and their colors
combined_colors_for_legend <- c(
  setNames(plot_loops_df_initial$color, plot_loops_df_initial$name), # Loops
  streets_to_highlight_colors # Highlighted streets
)
windows()

plot444<- ggplot() +
#  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) + # All streets (faded)
#  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "darkgray", size = 0.5, alpha = 0.6) + # All intersections (faded)
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "black", linewidth = 0.5, alpha = 0.6) + # All streets
  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "black", size = 0.7, alpha = 0.6) + # All intersections
  # Plot loop boundaries, mapping color to 'name' from plot_loops_df_initial
  geom_sf(data = plot_loops_df_initial, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") +
  # Plot highlighted streets, also mapping color to 'name'
  geom_sf(data = highlighted_streets_sf, aes(color = name), linewidth = 1.2, inherit.aes = FALSE) +
#  geom_sf(data = highlighted_streets_sf, aes(color = "black"), linewidth = 1.2, inherit.aes = FALSE) +
#  geom_label(data = initial_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  # Use the combined_colors_for_legend in scale_color_manual
#  scale_color_manual(values = combined_colors_for_legend, name = "Legend") +
#  labs(title = "General Web Network (GWN) with Defined Urban Loops and Highlighted Streets",
#       subtitle = "The entire road network with specific streets of interest and analyzed areas.") +
  theme_minimal() +
  # FIX: Move legend to the right for better visibility
  theme(legend.position = "bottom")
print(plot444)
ggsave("ccs_OSM_street1.jpeg", plot = plot444, width = 10, height = 8, dpi = 300)
 ggsave("ccs_OSM_street1.pdf", plot = plot444, width = 10, height = 8, dpi = 300)


############################ chatgpt

library(osmdata)
library(sf)
library(tidygraph)
library(ggraph)
library(tidyverse)

# 1. Define Small Bounding Box (Part of Münster)
bbox <- c(7.620, 51.955, 7.630, 51.960)

# 2. Download OSM Highway Data
osm_data <- opq(bbox = bbox) %>%
  add_osm_feature(key = "highway") %>%
  osmdata_sf()

edges_sf <- osm_data$osm_lines %>% st_transform(32632)  # UTM 32N

# 3. Extract Start/End Nodes
coords_start <- st_coordinates(st_line_sample(edges_sf, sample = 0))
coords_end   <- st_coordinates(st_line_sample(edges_sf, sample = 1))

# 4. Build Node Dataframe
nodes_df <- bind_rows(as_tibble(coords_start), as_tibble(coords_end)) %>%
  distinct(X, Y) %>%
  mutate(node_id = row_number())

nodes_sf <- st_as_sf(nodes_df, coords = c("X", "Y"), crs = st_crs(edges_sf))

# 5. Build Edge List by matching endpoints to nodes
edges_df <- edges_sf %>%
  rowwise() %>%
  mutate(
    from = nodes_df$node_id[which.min((st_coordinates(.)[1,1] - nodes_df$X)^2 + (st_coordinates(.)[1,2] - nodes_df$Y)^2)],
    to = nodes_df$node_id[which.min((st_coordinates(.)[nrow(st_coordinates(.)),1] - nodes_df$X)^2 + (st_coordinates(.)[nrow(st_coordinates(.)),2] - nodes_df$Y)^2)]
  ) %>%
  select(from, to) %>%
  distinct()

# 6. Create Graph Object
graph <- tbl_graph(nodes = nodes_df, edges = edges_df, directed = FALSE)

# 7. Compute Degree and Betweenness
graph <- graph %>%
  activate(nodes) %>%
  mutate(
    degree = centrality_degree(),
    betweenness = centrality_betweenness()
  )

nodes_df <- as_tibble(graph %>% activate(nodes))

# 8. Manually Define Loops using actual Coordinates (Make sure loops are closed)
loop1_coords <- nodes_df %>% filter(node_id %in% c(1,2,3,4)) %>% select(X, Y) %>% as.matrix()
loop1_coords <- rbind(loop1_coords, loop1_coords[1,])  # Close Loop

loop2_coords <- nodes_df %>% filter(node_id %in% c(5,6,7,8)) %>% select(X, Y) %>% as.matrix()
loop2_coords <- rbind(loop2_coords, loop2_coords[1,])  # Close Loop

# 9. Create Polygon Objects
loop1_poly <- st_polygon(list(loop1_coords))
loop2_poly <- st_polygon(list(loop2_coords))

# 10. Combine Polygons
loops_sfc <- st_sfc(loop1_poly, loop2_poly, crs = st_crs(nodes_sf))

# 11. Spatial Join: Nodes inside Loops
nodes_sf <- nodes_sf %>%
  mutate(
    gwn_loop1 = as.integer(st_within(geometry, loops_sfc[1], sparse = FALSE)[,1]),
    gwn_loop2 = as.integer(st_within(geometry, loops_sfc[2], sparse = FALSE)[,1])
  )

# 12. Compute Loop Areas
loop1_area <- st_area(loops_sfc[1])
loop2_area <- st_area(loops_sfc[2])

# 13. Plot Network and Loops
ggplot() +
  geom_sf(data = edges_sf, color = "grey80", size = 0.5) +
  geom_sf(data = nodes_sf, aes(color = factor(gwn_loop1 + 2 * gwn_loop2)), size = 2) +
  geom_sf(data = loops_sfc, fill = NA, color = "red", size = 1) +
  scale_color_manual(values = c("black", "blue", "green", "purple"), labels = c("Outside", "Loop 1", "Loop 2", "Both")) +
  theme_minimal()

# 14. Print Metrics
cat("Loop 1 Area (m^2):", loop1_area, "\n")
cat("Loop 2 Area (m^2):", loop2_area, "\n")
cat("Avg Degree in Loop 1:", mean(nodes_df$degree[nodes_sf$gwn_loop1 == 1], na.rm = TRUE), "\n")
cat("Avg Betweenness in Loop 2:", mean(nodes_df$betweenness[nodes_sf$gwn_loop2 == 1], na.rm = TRUE), "\n")



######Comprehensive R Script with Three Urban Loops sin otras modific
# Load necessary libraries
library(osmdata)
library(sf)
library(sfnetworks)
library(igraph)
library(dplyr)
library(ggplot2)
library(stringr)
library(tibble)

# --- 1. Data Acquisition and GWN Construction ---
message("--- Phase 1: Acquiring Real Geographic Data and Building the GWN ---")
caracas_bbox <- c(left = -67.10, bottom = 10.40, right = -66.70, top = 10.60)
message(paste0("Fetching urban road network data for a larger area of Caracas..."))
urban_streets_osm <- opq(caracas_bbox) %>%
  add_osm_feature(key = "highway", value = c("motorway", "trunk", "primary", "secondary", "tertiary", "residential", "unclassified")) %>%
  osmdata_sf()

if (is.null(urban_streets_osm$osm_lines)) {
  stop("No urban streets found. Please adjust the bounding box or check your internet connection.")
}
gwn_sfnetwork <- as_sfnetwork(urban_streets_osm$osm_lines, directed = FALSE)
message(paste0("  Initial GWN created with ", igraph::vcount(gwn_sfnetwork),
               " intersections and ", igraph::ecount(gwn_sfnetwork), " streets."))

# --- 2. Calculate Centrality for the Entire GWN ---
message("\n--- Phase 2: Computing Centrality for the Entire GWN ---")
message("  Calculating Betweenness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(betweenness = igraph::betweenness(., normalized = TRUE))

message("  Calculating Closeness Centrality for all nodes...")
gwn_sfnetwork <- gwn_sfnetwork %>%
  mutate(closeness = igraph::closeness(., normalized = TRUE))
message("  Centrality measures added to all intersections in the GWN.")

# --- 3. Define User-Provided Loops (as Geographic Polygons) ---
message("\n--- Phase 3: Defining User-Provided Urban Loops ---")
loop_1_coords <- matrix(c(
  -66.90, 10.480,
  -66.89, 10.485,
  -66.88, 10.475,
  -66.89, 10.470,
  -66.90, 10.480
), ncol = 2, byrow = TRUE)
loop_1_polygon <- st_polygon(list(loop_1_coords))

loop_2_coords <- matrix(c(
  -66.93, 10.50,
  -66.88, 10.51,
  -66.86, 10.46,
  -66.91, 10.45,
  -66.93, 10.50
), ncol = 2, byrow = TRUE)
loop_2_polygon <- st_polygon(list(loop_2_coords))

# NEW: Define a third loop
loop_3_coords <- matrix(c(
  -67.05, 10.43,
  -67.00, 10.44,
  -67.02, 10.41,
  -67.05, 10.43
), ncol = 2, byrow = TRUE)
loop_3_polygon <- st_polygon(list(loop_3_coords))


network_crs <- st_crs(gwn_sfnetwork)

loop_1_polygon_sfg <- loop_1_polygon
loop_2_polygon_sfg <- loop_2_polygon
loop_3_polygon_sfg <- loop_3_polygon # NEW

loop_1_polygon_sf <- st_sfc(loop_1_polygon_sfg, crs = network_crs)
loop_2_polygon_sf <- st_sfc(loop_2_polygon_sfg, crs = network_crs)
loop_3_polygon_sf <- st_sfc(loop_3_polygon_sfg, crs = network_crs) # NEW

loops <- list(
  list(name = "Historic Core Loop", polygon = loop_1_polygon_sf, color = "darkblue"),
  list(name = "Traffic Beltway Loop", polygon = loop_2_polygon_sf, color = "darkred"),
  list(name = "Western District Loop", polygon = loop_3_polygon_sf, color = "brown") # NEW
)
message(paste0("  ", length(loops), " urban loops have been defined for comparison."))

# --- Select Specific Streets to Highlight with Different Colors ---
streets_to_highlight_names <- c(
  "Avenida Libertador",
  "Avenida Francisco de Miranda",
  "Calle Real de Sabana Grande",
  "Cota Mil",
  "Autopista del Este",
  "Avenida Boyacá"
)

streets_to_highlight_colors <- c(
  "Avenida Libertador" = "orange",
  "Avenida Francisco de Miranda" = "purple",
  "Calle Real de Sabana Grande" = "cyan",
  "Cota Mil" = "magenta",
  "Autopista del Este" = "green",
  "Avenida Boyacá" = "magenta"
)

highlighted_streets_sf <- gwn_sfnetwork %>%
  activate("edges") %>%
  filter(name %in% streets_to_highlight_names) %>%
  st_as_sf() %>%
  select(name, geometry)

if (nrow(highlighted_streets_sf) > 0) {
  message(paste0("  Found ", nrow(highlighted_streets_sf), " segments for the specified streets to highlight."))
} else {
  message("  Warning: No segments found for the specified streets to highlight. Check names or bounding box.")
}

# --- Initial Plot of the entire GWN with Loops and Highlighted Streets ---
message("\n--- Initial Plot: Entire GWN with Defined Loops and Highlighted Streets ---")

# FIX: Update plot_loops_df_initial to include the third loop
plot_loops_df_initial <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name, loops[[3]]$name), # NEW
  color = c(loops[[1]]$color, loops[[2]]$color, loops[[3]]$color), # NEW
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], loops[[3]]$polygon[[1]], crs = network_crs) # NEW
) %>%
  st_as_sf()

# FIX: Update initial_labels_df to include the third loop
initial_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name, loops[[3]]$name), # NEW
  color = c(loops[[1]]$color, loops[[2]]$color, loops[[3]]$color), # NEW
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], st_centroid(loops[[3]]$polygon)[[1]], crs = network_crs) # NEW
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )

# FIX: Update combined_colors_for_legend to include the third loop's color
combined_colors_for_legend <- c(
  setNames(plot_loops_df_initial$color, plot_loops_df_initial$name), # Loops
  streets_to_highlight_colors # Highlighted streets
)

ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) + # All streets (faded)
  geom_sf(data = st_as_sf(gwn_sfnetwork, "nodes"), color = "darkgray", size = 0.5, alpha = 0.6) + # All intersections (faded)
  geom_sf(data = plot_loops_df_initial, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") + # Loop boundaries
  geom_sf(data = highlighted_streets_sf, aes(color = name), linewidth = 1.2, inherit.aes = FALSE) +
  geom_label(data = initial_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  scale_color_manual(values = combined_colors_for_legend, name = "Legend") +
  labs(title = "General Web Network (GWN) with Defined Urban Loops and Highlighted Streets",
       subtitle = "The entire road network with specific streets of interest and analyzed areas.") +
  theme_minimal() +
  theme(legend.position = "right") # Changed to right for better visibility

message("  Initial GWN and loop plot with highlighted streets generated.")

# --- 4. GWN (Spatial Filtering) and Multi-Attribute Analysis ---
message("\n--- Phase 4: GWN Analysis and Multi-Attribute Scoring ---")
results <- data.frame(
  loop_name = character(),
  area_sq_km = numeric(),
  num_enclosed_nodes = integer(),
  avg_betweenness = numeric(),
  avg_closeness = numeric(),
  stringsAsFactors = FALSE
)

for (loop in loops) { # Loop will now iterate through 3 loops
  message(paste0("  Analyzing '", loop$name, "'..."))
  loop_nodes_sf <- st_as_sf(gwn_sfnetwork, "nodes")
  
  nodes_inside_loop <- loop_nodes_sf[st_within(loop_nodes_sf, loop$polygon, sparse = FALSE), ]
  
  if (nrow(nodes_inside_loop) == 0) {
    message(paste0("    Warning: No nodes found inside '", loop$name, "'. Skipping analysis."))
    next
  }
  
  num_enclosed_nodes <- nrow(nodes_inside_loop)
  avg_betweenness_inside <- mean(nodes_inside_loop$betweenness, na.rm = TRUE)
  avg_closeness_inside <- mean(nodes_inside_loop$closeness, na.rm = TRUE)
  
  loop_area_sq_m <- st_area(loop$polygon)
  loop_area_sq_km <- as.numeric(loop_area_sq_m) / 10^6
  
  message(paste0("    Enclosed Nodes: ", num_enclosed_nodes))
  message(paste0("    Average Betweenness: ", round(avg_betweenness_inside, 4)))
  paste0("    Average Closeness: ", round(avg_closeness_inside, 4))
  message(paste0("    Loop Area: ", round(loop_area_sq_km, 2), " sq km"))
  
  results <- results %>%
    add_row(
      loop_name = loop$name,
      area_sq_km = loop_area_sq_km,
      num_enclosed_nodes = num_enclosed_nodes,
      avg_betweenness = avg_betweenness_inside,
      avg_closeness = avg_closeness_inside
    )
}

# --- 5. Multi-Attribute Decision Making ---
message("\n--- Phase 5: Multi-Attribute Decision Making (Normalization & Weighting) ---")
weights <- c(
  num_enclosed_nodes = 0.20,
  avg_betweenness = 0.40,
  avg_closeness = 0.30,
  area_sq_km = 0.10
)

if (sum(weights) != 1.0) {
  stop("Weights must sum to 1.0. Please adjust your weights.")
}

# Normalization will now work with three data points, providing more varied scores
normalized_results <- results %>%
  mutate(
    norm_num_enclosed_nodes = (num_enclosed_nodes - min(num_enclosed_nodes)) / (max(num_enclosed_nodes) - min(num_enclosed_nodes)),
    norm_avg_betweenness = (avg_betweenness - min(avg_betweenness)) / (max(avg_betweenness) - min(avg_betweenness)),
    norm_avg_closeness = (avg_closeness - min(avg_closeness)) / (max(avg_closeness) - min(avg_closeness)),
    norm_area_sq_km = (max(area_sq_km) - area_sq_km) / (max(area_sq_km) - min(area_sq_km))
  ) %>%
  mutate_at(vars(starts_with("norm_")), ~replace(., is.nan(.), 0.5))

final_scores <- normalized_results %>%
  rowwise() %>%
  mutate(
    composite_score = (norm_num_enclosed_nodes * weights["num_enclosed_nodes"]) +
                      (norm_avg_betweenness * weights["avg_betweenness"]) +
                      (norm_avg_closeness * weights["avg_closeness"]) +
                      (norm_area_sq_km * weights["area_sq_km"])
  ) %>%
  ungroup() %>%
  arrange(desc(composite_score))

message("  Composite scores calculated based on your defined weights.")

# --- 6. Plotting the Results ---
message("\n--- Phase 6: Plotting the Final Analysis ---")

# FIX: Update plot_loops_df to include the third loop
plot_loops_df <- tibble(
  name = c(loops[[1]]$name, loops[[2]]$name, loops[[3]]$name), # NEW
  color = c(loops[[1]]$color, loops[[2]]$color, loops[[3]]$color), # NEW
  geometry = st_sfc(loops[[1]]$polygon[[1]], loops[[2]]$polygon[[1]], loops[[3]]$polygon[[1]], crs = network_crs) # NEW
) %>%
  st_as_sf()

gwn_nodes_sf_for_plotting <- st_as_sf(gwn_sfnetwork, "nodes")

# FIX: Update combined_loop_geometry to include the third loop
combined_loop_geometry <- st_union(loop_1_polygon_sf, loop_2_polygon_sf, loop_3_polygon_sf) # NEW

all_nodes_in_loops_sf <- gwn_nodes_sf_for_plotting[st_within(gwn_nodes_sf_for_plotting, combined_loop_geometry, sparse = FALSE), ]

# FIX: Update loop_membership assignment to include the third loop
all_nodes_in_loops_sf <- all_nodes_in_loops_sf %>%
  mutate(loop_membership = case_when(
    st_within(geometry, loop_1_polygon_sf, sparse = FALSE) ~ loops[[1]]$name,
    st_within(geometry, loop_2_polygon_sf, sparse = FALSE) ~ loops[[2]]$name,
    st_within(geometry, loop_3_polygon_sf, sparse = FALSE) ~ loops[[3]]$name, # NEW
    TRUE ~ "None"
  ))

all_labels_df <- tibble(
  label = c(loops[[1]]$name, loops[[2]]$name, loops[[3]]$name), # NEW
  color = c(loops[[1]]$color, loops[[2]]$color, loops[[3]]$color), # NEW
  geometry = st_sfc(st_centroid(loops[[1]]$polygon)[[1]], st_centroid(loops[[2]]$polygon)[[1]], st_centroid(loops[[3]]$polygon)[[1]], crs = network_crs) # NEW
) %>%
  st_as_sf() %>%
  mutate(
    x = st_coordinates(geometry)[,1],
    y = st_coordinates(geometry)[,2]
  )


# Create the final plot
ggplot() +
  geom_sf(data = st_as_sf(gwn_sfnetwork, "edges"), color = "lightgray", linewidth = 0.5, alpha = 0.4) +
  geom_sf(data = plot_loops_df, fill = NA, aes(color = name), linewidth = 1.5, linetype = "dashed") +
  geom_sf(data = all_nodes_in_loops_sf, aes(size = betweenness, color = loop_membership), alpha = 0.8) +
  geom_label(data = all_labels_df, aes(x = x, y = y, label = label, color = color), size = 3) +
  scale_size_continuous(range = c(1, 8), name = "Betweenness Score") +
  # FIX: Update scale_color_manual to include the third loop's color
  scale_color_manual(values = c(
    setNames(plot_loops_df$color, plot_loops_df$name), # All loop colors
    streets_to_highlight_colors # All highlighted street colors
  ), name = "Legend") +
  labs(title = "Urban Network Analysis: Multi-Attribute Loop Comparison",
       subtitle = "Node size indicates betweenness centrality. Dashed lines mark the analyzed loops.") +
  theme_minimal() +
  theme(legend.position = "right") # Changed to right for better visibility

# --- 7. Final Report for the Mayor ---
message("\n--- Final Report: Strategic Insights for Your City ---")
message("Below are the detailed metrics and composite scores for the three loops. The loop with the highest composite score is recommended based on your defined priorities.\n")
print(final_scores %>% select(loop_name, area_sq_km, num_enclosed_nodes, avg_betweenness, avg_closeness, composite_score))

message("\nInterpretation of Composite Score:")
message("The Composite Score combines multiple factors (number of nodes, average centralities, and cost/area) into a single metric, weighted by your strategic priorities.")
message(" - A higher Composite Score indicates a better overall 'value' or 'impact' for a given loop, considering all your objectives.")
message(" - You can adjust the 'weights' in the code (Phase 5) to reflect changing priorities (e.g., if cost becomes more critical, increase the weight for 'area_sq_km').")
message("\nThis framework allows you to make data-driven decisions on where to focus your resources to achieve the most impactful urban development.")

######## enhancing r code for network... influence and cointauinment index GEMINI
# Required packages
library(igraph)
library(sf)
library(tidyverse)
library(ggplot2)
setwd("c:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
# Set seed for a reproducible layout
set.seed(42)

# ==============================================================================
# 1. Network Definition & Node Positions
# ==============================================================================

g <- make_graph("Zachary")
network1 <- "Zachary"

layout_coords <- layout_with_fr(g)

layout_coords <- layout_orig

positions <- tibble(
  name = as.character(1:vcount(g)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)

nodes_sf <- st_as_sf(positions, coords = c("x", "y"))

# Prepare edge data for plotting
edges <- igraph::as_data_frame(g, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  )

positions_from <- positions %>% rename(x_from = x, y_from = y)
positions_to <- positions %>% rename(x_to = x, y_to = y)

edges <- edges %>%
  left_join(positions_from, by = c("from" = "name")) %>%
  left_join(positions_to, by = c("to" = "name"))

# ==============================================================================
# 2. Define the Loops and Polygon Generation
# ==============================================================================

loops <- list(
  Loop_A = c(1, 2, 3, 4, 1),
  Loop_B = c(17, 6, 5, 11, 17),
  Loop_C = c(34, 13, 20, 19, 33, 34)
)

loops <- list( #sen troncal
  Loop_A = c(70,72,71,73,67,69,70),
  Loop_B = c(5,6,1,46,5),
  Loop_C = c(17,16,47,51,27,17)
)

loops

create_loop_polygon <- function(loop_nodes, all_positions) {
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(all_positions, by = "name")
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  st_polygon(list(poly_coords))
}

loops_sf_list <- lapply(loops, create_loop_polygon, all_positions = positions)

loops_sf <- st_sfc(loops_sf_list) %>%
  st_sf(Loop_ID = names(loops), geometry = .)

# ==============================================================================
# 3. Calculate All Metrics
# ==============================================================================

num_nodes <- nrow(positions)
num_loops <- length(loops)

geometric_matrix <- matrix(0, nrow = num_nodes, ncol = num_loops)
contained_matrix <- matrix(0, nrow = num_nodes, ncol = num_loops)
covered_matrix <- matrix(0, nrow = num_nodes, ncol = num_loops)

summary_metrics_df <- tibble(
  Loop_ID = character(),
  Area = numeric(),
  Perimeter = numeric(),
  Mean_Geometric_Influence = numeric(),
  Total_Contained_Nodes = numeric(),
  Total_Covered_Nodes = numeric()
)

colnames(geometric_matrix) <- names(loops)
colnames(contained_matrix) <- names(loops)
colnames(covered_matrix) <- names(loops)
rownames(geometric_matrix) <- positions$name
rownames(contained_matrix) <- positions$name
rownames(covered_matrix) <- positions$name

for (i in 1:num_loops) {
  polygon <- loops_sf_list[[i]]
  loop_id <- names(loops)[i]
  
  # --- Calculate Metrics for Summary Table ---
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  # --- Calculate Matrices ---
  centroid <- st_centroid(polygon)
  distances <- st_distance(nodes_sf, centroid)
  distances[distances == 0] <- 1e-9
  geometric_influence <- as.numeric(1 / distances^2)
  
  # Contained (strictly inside, not on boundary)
  strictly_contained <- st_within(nodes_sf, st_sfc(polygon), sparse = FALSE)
  contained_nodes_count <- sum(strictly_contained)
  
  # Covered (inside or on boundary)
  covered_nodes <- st_covers(st_sfc(polygon), nodes_sf, sparse = FALSE)
  covered_nodes_count <- sum(covered_nodes)

  geometric_matrix[, i] <- geometric_influence
  contained_matrix[, i] <- as.numeric(strictly_contained)
  covered_matrix[, i] <- as.numeric(covered_nodes)

  # Append summary metrics to the data frame
  summary_metrics_df <- summary_metrics_df %>%
    add_row(
      Loop_ID = loop_id,
      Area = area,
      Perimeter = perimeter,
      Mean_Geometric_Influence = mean(geometric_influence),
      Total_Contained_Nodes = contained_nodes_count,
      Total_Covered_Nodes = covered_nodes_count
    )
}

# ==============================================================================
# 4. Output Results and Plot
# ==============================================================================

cat("Loop Summary Metrics:\n")
print(as.data.frame(summary_metrics_df))

cat("\nGeometric Influence Matrix (nodes x loops):\n")
print(round(geometric_matrix, 3))

cat("\nContained Matrix (strictly inside):\n")
print(contained_matrix)

cat("\nCovered Matrix (inside or on boundary):\n")
print(covered_matrix)
windows()
cat("\nPlotting the network and loops for visual verification...\n")
plot8 <-ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), 
               color = "gray", linewidth = 0.5) +
  geom_sf(data = loops_sf, aes(fill = Loop_ID), alpha = 0.3) +
  geom_sf(data = nodes_sf, size = 1, color = "black") +
  geom_sf_text(data = nodes_sf, aes(label = name), nudge_y = 0.05, size = 3) +
  labs(title = network1) +
  theme_minimal()
print(plot8)
ggsave(paste(network1,"_influence.jpeg"), plot = plot8, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_influence.pdf"), plot = plot8, width = 10, height = 8, dpi = 300)

####################################### genini atan2
################### base, base+loopa, .....
##### indexes for global network and llops

# Required packages
library(igraph)
library(sf)
library(tidyverse)
setwd("d:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
# Set seed for a reproducible layout
set.seed(42)

# ==============================================================================
# 1. Network Definition & Node Positions
# ==============================================================================

# Define the base network (Zachary's Karate Club)
g_base <- make_graph("Zachary")
network1="Zachary"
layout_coords <- layout_with_fr(g_base)

set.seed(42)
#layout_orig <- layout_with_fr(g)
g_base <- g
layout_coords <-layout_orig 

positions <- tibble(
  name = as.character(1:vcount(g_base)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)

nodes_sf <- st_as_sf(positions, coords = c("x", "y"))

# Prepare edge data for plotting
edges <- igraph::as_data_frame(g_base, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  )

positions_from <- positions %>% rename(x_from = x, y_from = y)
positions_to <- positions %>% rename(x_to = x, y_to = y)

edges <- edges %>%
  left_join(positions_from, by = c("from" = "name")) %>%
  left_join(positions_to, by = c("to" = "name"))

# ==============================================================================
# 2. Define the Loops and Enhancement Scenarios
# ==============================================================================

# Define loops by their nodes. The new logic treats all as enhancements.
loops <- list(
  Loop_A = c(1, 2, 3, 4, 1),
  Loop_B = c(5, 6, 11, 17, 5),
  Loop_C = c(34, 13, 20, 19, 33, 34)
)

loops <- list( #sen troncal
  Loop_A = c(70,72,71,73,67,69,70),
  Loop_B = c(5,6,1,46,5),
  Loop_C = c(17,16,47,51,27,17)
)

loops

# New edges for the hypothetical loops
new_edges_A <- tibble(
  from = c(1, 2, 3, 4),
  to = c(2, 3, 4, 1)
)

new_edges_B <- tibble(
  from = c(5, 6, 11, 17),
  to = c(6, 11, 17, 5)
)

new_edges_C <- tibble(
  from = c(34, 13, 20, 19, 33),
  to = c(13, 20, 19, 33, 34)
)

# Create the enhanced networks
g_loop_a <- add_edges(g_base, as.vector(t(new_edges_A[, c("from", "to")])))
g_loop_b <- add_edges(g_base, as.vector(t(new_edges_B[, c("from", "to")])))
g_loop_c <- add_edges(g_base, as.vector(t(new_edges_C[, c("from", "to")])))

# ==============================================================================
# 3. Functions for Metrics and Analysis
# ==============================================================================

# Function to calculate global network metrics
calculate_global_metrics <- function(g) {
  # Shortest path lengths for all pairs
  sp <- distances(g, mode = "all")
  finite_sp <- sp[is.finite(sp)]
  
  avg_path_length <- mean(finite_sp)
  efficiency <- 1 / avg_path_length
  
  avg_betweenness_centrality <- mean(betweenness(g))
  num_components <- components(g)$no
  
  tibble(
    avg_path_length = avg_path_length,
    network_efficiency = efficiency,
    avg_betweenness_centrality = avg_betweenness_centrality,
    num_components = num_components
  )
}

# --- NEW: Function to compute winding number
compute_winding_number <- function(px, py, loop_coords_df) {
  angles <- 0
  n <- nrow(loop_coords_df)
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    angle <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    angles <- angles + angle
  }
  return(angles / (2 * pi))
}
# --- END OF NEW FUNCTION

# Function to create an sf polygon from loop nodes
create_loop_polygon <- function(loop_nodes, all_positions) {
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(all_positions, by = "name")
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  st_polygon(list(poly_coords))
}

# Main function to run the full analysis for a given loop and network
run_analysis <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(positions, by = "name")
  
  polygon <- create_loop_polygon(loop_nodes, positions)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  strictly_contained_result <- st_within(nodes_sf, st_sfc(polygon))
  on_boundary_result <- st_touches(nodes_sf, st_sfc(polygon))
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
  
  # --- NEW: Winding Number Calculation
  winding_number_values <- sapply(1:nrow(positions), function(i) {
    compute_winding_number(positions$x[i], positions$y[i], loop_coords_df)
  })
####
winding_number_values <-abs (winding_number_values)
####
  total_winding_number <- sum(winding_number_values[abs(winding_number_values) > 0.5])
  # --- END OF NEW CALCULATION
  
  # Geometric influence
  centroid <- st_centroid(polygon)
  distances <- st_distance(nodes_sf, centroid)
  geometric_influence <- as.numeric(1 / (distances^2))
  geometric_influence[!is.finite(geometric_influence)] <- 0
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Mean_Geometric_Influence = mean(geometric_influence),
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count,
    Total_Winding_Number = total_winding_number # --- NEW METRIC
  )
  
  geometric_matrix_col <- tibble(influence = geometric_influence)
  colnames(geometric_matrix_col) <- paste0("Loop_", loop_name)
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  winding_matrix_col <- tibble(winding_number = winding_number_values) # --- NEW MATRIX
  colnames(winding_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    summary = summary_row,
    geometric_matrix = geometric_matrix_col,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    winding_matrix = winding_matrix_col, # --- NEW IN THE LIST
    polygon = polygon
  )
}

# ==============================================================================
# 4. Run Analysis for All Scenarios
# ==============================================================================

all_summary <- tibble()
all_geometric <- tibble(node = positions$name)
all_contained <- tibble(node = positions$name)
all_covered <- tibble(node = positions$name)
all_winding <- tibble(node = positions$name) # --- NEW DATAFRAME
global_metrics_df <- tibble()
all_polygons_list <- vector("list", length = length(loops))

# Scenario 0: Basic network (BN)
cat("--- Scenario 0: Basic Network (BN) ---\n")
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_base) %>% mutate(Scenario = "0: BN"))
cat("\n\n")

# Scenario A: BN + loop A
cat("--- Scenario A: BN + Loop A ---\n")
results_a <- run_analysis(g_loop_a, "A", loops$Loop_A, positions, nodes_sf)
all_summary <- bind_rows(all_summary, results_a$summary)
all_geometric <- bind_cols(all_geometric, results_a$geometric_matrix)
all_contained <- bind_cols(all_contained, results_a$contained_matrix)
all_covered <- bind_cols(all_covered, results_a$covered_matrix)
all_winding <- bind_cols(all_winding, results_a$winding_matrix) # --- NEW BIND
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_loop_a) %>% mutate(Scenario = "A: BN + Loop A"))
all_polygons_list[[1]] <- results_a$polygon
cat("\n\n")

# Scenario B: BN + loop B
cat("--- Scenario B: BN + Loop B ---\n")
results_b <- run_analysis(g_loop_b, "B", loops$Loop_B, positions, nodes_sf)
all_summary <- bind_rows(all_summary, results_b$summary)
all_geometric <- bind_cols(all_geometric, results_b$geometric_matrix)
all_contained <- bind_cols(all_contained, results_b$contained_matrix)
all_covered <- bind_cols(all_covered, results_b$covered_matrix)
all_winding <- bind_cols(all_winding, results_b$winding_matrix) # --- NEW BIND
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_loop_b) %>% mutate(Scenario = "B: BN + Loop B"))
all_polygons_list[[2]] <- results_b$polygon
cat("\n\n")

# Scenario C: BN + loop C
cat("--- Scenario C: BN + Loop C ---\n")
results_c <- run_analysis(g_loop_c, "C", loops$Loop_C, positions, nodes_sf)
all_summary <- bind_rows(all_summary, results_c$summary)
all_geometric <- bind_cols(all_geometric, results_c$geometric_matrix)
all_contained <- bind_cols(all_contained, results_c$contained_matrix)
all_covered <- bind_cols(all_covered, results_c$covered_matrix)
all_winding <- bind_cols(all_winding, results_c$winding_matrix) # --- NEW BIND
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_loop_c) %>% mutate(Scenario = "C: BN + Loop C"))
all_polygons_list[[3]] <- results_c$polygon
cat("\n\n")

# ==============================================================================
# 5. Final Output
# ==============================================================================

cat("Global Network Metrics Comparison:\n")
print(as.data.frame(global_metrics_df))

cat("\nLoop Summary Metrics:\n")
print(as.data.frame(all_summary))

cat("\nGeometric Influence Matrix (nodes x loops):\n")
print(all_geometric)

cat("\nContained Matrix (strictly inside):\n")
print(all_contained)

cat("\nCovered Matrix (inside or on boundary):\n")
print(all_covered)

cat("\nWinding Number Matrix (nodes x loops):\n")
print(all_winding)

total_results <- cbind(all_geometric,all_contained,all_covered,all_winding)

# ==============================================================================
# 6. Plot the Network and Loops
# ==============================================================================

loop_polygons_df <- st_sf(
  ID = c("Loop_A", "Loop_B", "Loop_C"),
  geometry = st_sfc(all_polygons_list)
)
windows()
network_plot <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", size = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.3, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "steelblue", size = 3) +
  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(title = "Network Loops and Enhancements",
       subtitle = "Visualizing the Geometric Boundaries of Loops A, B, and C",
       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID")

#ggsave("network_loops_plot.png", plot = network_plot, width = 8, height = 6, dpi = 300)
print(network_plot)

ggsave(paste(network1,"_GEloops_plot.jpeg"), plot = network_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEloops_plot.pdf"), plot = network_plot, width = 10, height = 8, dpi = 300)

cat("\nPlot has been saved to 'network_loops_plot.png'\n")

##############################################################################

#### chatgpt atan2

library(igraph)
library(dplyr)
library(ggplot2)
library(ggraph)

# ---------------------------


# 1. Create Zachary network
g_base <- make_graph("Zachary")
V(g_base)$name <- as.character(seq_len(vcount(g_base))) # give each vertex a name

# Layout for coordinates (proxy for spatial positions)
coords <- layout_with_fr(g_base)
positions <- data.frame(
  node = V(g_base)$name,
  x = coords[,1],
  y = coords[,2]
)

# ---------------------------
# 2. Functions
# ---------------------------
compute_metrics <- function(g) {
  data.frame(
    ASPL = mean_distance(g, directed = FALSE),
    Diameter = diameter(g, directed = FALSE),
    AvgCloseness = mean(closeness(g)),
    MaxBetweenness = max(betweenness(g)),
    EdgeConn = edge_connectivity(g)
  )
}

compute_gwn <- function(loop_nodes, positions) {
  poly_coords <- positions %>% filter(node %in% loop_nodes) %>%
    arrange(match(node, loop_nodes)) %>% select(x, y)
  poly_coords <- rbind(poly_coords, poly_coords[1,])
  
  gwn_vals <- rep(0, nrow(positions))
  for (i in seq_len(nrow(positions))) {
    p <- as.numeric(positions[i, c("x", "y")])
    wn <- 0
    for (j in 1:(nrow(poly_coords)-1)) {
      x1 <- poly_coords[j,1] - p[1]
      y1 <- poly_coords[j,2] - p[2]
      x2 <- poly_coords[j+1,1] - p[1]
      y2 <- poly_coords[j+1,2] - p[2]
      angle <- atan2(x1*y2 - y1*x2, x1*x2 + y1*y2)
      wn <- wn + angle
    }
    gwn_vals[i] <- abs(wn) / (2*pi)
  }
  gwn_vals
}

perimeter_length <- function(loop_nodes, positions) {
  loop_coords <- positions %>% filter(node %in% loop_nodes) %>%
    arrange(match(node, loop_nodes))
  loop_coords <- rbind(loop_coords, loop_coords[1,])
  sum(sqrt(diff(loop_coords$x)^2 + diff(loop_coords$y)^2))
}

add_loop_edges <- function(g, loop_nodes) {
  edges_to_add <- c()
  for (i in seq_along(loop_nodes)) {
    from <- loop_nodes[i]
    to <- loop_nodes[ifelse(i == length(loop_nodes), 1, i+1)]
    edges_to_add <- c(edges_to_add, from, to)
  }
  g2 <- add_edges(g, edges_to_add)
  return(g2)
}

# ---------------------------
# 3. Baseline
# ---------------------------
baseline_metrics <- compute_metrics(g_base)

# ---------------------------
# 4. Loops
# ---------------------------
loops <- list(
  loop1 = c("31","11","12","13","3","31"),
  loop2 = c("1","2","3","4","5","1")
)

results <- data.frame()
for (lname in names(loops)) {
  loop_nodes <- loops[[lname]]
  
  g_new <- add_loop_edges(g_base, loop_nodes)
  new_metrics <- compute_metrics(g_new)
  
  gwn_vals <- compute_gwn(loop_nodes, positions)
print(lname)
print(gwn_vals)
  mean_gwn <- mean(gwn_vals)
  perim <- perimeter_length(loop_nodes, positions)
  
  diff_metrics <- new_metrics - baseline_metrics
  
  results <- rbind(results, data.frame(
    Loop = lname,
    LoopNodes = paste(loop_nodes, collapse="-"),
    MeanGWN = mean_gwn,
    Perimeter = perim,
    Delta_ASPL = diff_metrics$ASPL,
    Delta_Diameter = diff_metrics$Diameter,
    Delta_AvgCloseness = diff_metrics$AvgCloseness,
    Delta_MaxBetweenness = diff_metrics$MaxBetweenness,
    Delta_EdgeConn = diff_metrics$EdgeConn
  ))
}

# Simple score: improvement in connectivity and GWN / cost
results <- results %>%
  mutate(Score = ( -Delta_ASPL + -Delta_MaxBetweenness/100 + Delta_EdgeConn + MeanGWN ) / Perimeter)

# Ranking
results <- results %>% arrange(desc(Score))
print(results)

# ---------------------------
# 5. Plots
# ---------------------------
# Plot loops on network
plot_network_with_loops <- function(g, positions, loops) {
  g_df <- igraph::as_data_frame(g, what = "edges") %>%
    mutate(
      from_x = positions$x[match(from, positions$node)],
      from_y = positions$y[match(from, positions$node)],
      to_x = positions$x[match(to, positions$node)],
      to_y = positions$y[match(to, positions$node)]
    )
windows()
  gg <- ggplot() +
    geom_segment(data = g_df,
                 aes(x = from_x, y = from_y, xend = to_x, yend = to_y),
                 color = "grey80") +
    geom_point(data = positions, aes(x = x, y = y), size = 3, color = "black") +
    geom_text(data = positions, aes(x = x, y = y, label = node), vjust = -1)
  
  colors <- c("blue","red")
  for (i in seq_along(loops)) {
    loop_coords <- positions %>%
      filter(node %in% loops[[i]]) %>%
      arrange(match(node, loops[[i]]))
    loop_coords <- rbind(loop_coords, loop_coords[1,])
    gg <- gg + geom_path(data = loop_coords, aes(x = x, y = y), color = colors[i], size = 1)
  }
  return(gg)
}
print(plot_network_with_loops(g_base, positions, loops))

# ---------------------------
# 6. Heatmaps
# ---------------------------
travel_time_improvement <- function(g_old, g_new) {
  dist_old <- distances(g_old)
  dist_new <- distances(g_new)
  improve <- rowMeans(dist_old - dist_new)
  return(improve)
}

plot_heatmap <- function(g, positions, values, title) {
  df <- positions %>% mutate(value = values)
  g_df <- igraph::as_data_frame(g, what = "edges") %>%
    mutate(
      from_x = positions$x[match(from, positions$node)],
      from_y = positions$y[match(from, positions$node)],
      to_x = positions$x[match(to, positions$node)],
      to_y = positions$y[match(to, positions$node)]
    )
windows()
  ggplot() +
    geom_segment(data = g_df,
                 aes(x = from_x, y = from_y, xend = to_x, yend = to_y),
                 color = "grey80") +
    geom_point(data = df, aes(x = x, y = y, color = value), size = 3) +
    scale_color_gradient(low = "white", high = "red") +
    geom_text(data = df, aes(x = x, y = y, label = node), vjust = -1) +
    ggtitle(title) +
    theme_minimal()
}

# Loop through each loop and plot heatmaps
for (lname in names(loops)) {
  loop_nodes <- loops[[lname]]
  g_new <- add_loop_edges(g_base, loop_nodes)
  
  # Travel time improvement heatmap
  imp_vals <- travel_time_improvement(g_base, g_new)
  print(plot_heatmap(g_new, positions, imp_vals, paste("Travel Time Improvement -", lname)))
  
  # GWN heatmap
  gwn_vals <- compute_gwn(loop_nodes, positions)
  print(plot_heatmap(g_base, positions, gwn_vals, paste("GWN Influence -", lname)))
}

########################## gemini atan2 opcion loops o communities

# Required packages
library(igraph)
library(sf)
library(tidyverse)
library(ggplot2)

# Set a seed for reproducibility
set.seed(42)

# ==============================================================================
# 1. NETWORK DEFINITION AND SPATIAL DATA
# ==============================================================================

# Define the base network (Zachary's Karate Club)
g_base <- make_graph("Zachary")
layout_coords <- layout_with_fr(g_base)

positions <- tibble(
  name = as.character(1:vcount(g_base)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)

nodes_sf <- st_as_sf(positions, coords = c("x", "y"))

# Prepare edge data for plotting the base network
edges <- igraph::as_data_frame(g_base, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  ) %>%
  left_join(positions %>% rename(x_from = x, y_from = y), by = c("from" = "name")) %>%
  left_join(positions %>% rename(x_to = x, y_to = y), by = c("to" = "name"))

# ==============================================================================
# 2. CORE FUNCTIONS
# ==============================================================================

# Function to calculate global network metrics
calculate_global_metrics <- function(g) {
  sp <- distances(g, mode = "all")
  finite_sp <- sp[is.finite(sp)]
  
  avg_path_length <- mean(finite_sp)
  efficiency <- 1 / avg_path_length
  avg_betweenness_centrality <- mean(betweenness(g))
  num_components <- components(g)$no
  
  tibble(
    avg_path_length = avg_path_length,
    network_efficiency = efficiency,
    avg_betweenness_centrality = avg_betweenness_centrality,
    num_components = num_components
  )
}

# Function to generate edges from an ordered list of nodes
generate_loop_edges <- function(loop_nodes) {
  num_nodes <- length(loop_nodes)
  from_nodes <- loop_nodes[1:(num_nodes - 1)]
  to_nodes <- loop_nodes[2:num_nodes]
  tibble(from = from_nodes, to = to_nodes)
}

# Function to compute winding number
compute_winding_number <- function(px, py, loop_coords_df) {
  angles <- 0
  n <- nrow(loop_coords_df)
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    angle <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    angles <- angles + angle
  }
  return(angles / (2 * pi))
}

# Function to create an sf polygon from ordered loop nodes
create_loop_polygon <- function(loop_nodes, all_positions) {
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(all_positions, by = "name")
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  st_polygon(list(poly_coords))
}

# Main function to run the full analysis for a given loop and network
run_analysis <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  
  new_edges <- generate_loop_edges(loop_nodes)
  g_enhanced <- add_edges(g, as.vector(t(new_edges[, c("from", "to")])))
  
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(positions, by = "name")
  
  polygon <- create_loop_polygon(loop_nodes, positions)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  strictly_contained_result <- st_within(nodes_sf, st_sfc(polygon))
  on_boundary_result <- st_touches(nodes_sf, st_sfc(polygon))
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
  
  winding_number_values <- sapply(1:nrow(positions), function(i) {
    compute_winding_number(positions$x[i], positions$y[i], loop_coords_df)
  })
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count
  )
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  winding_matrix_col <- tibble(winding_number = winding_number_values)
  colnames(winding_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    global_metrics = calculate_global_metrics(g_enhanced),
    summary = summary_row,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    winding_matrix = winding_matrix_col,
    polygon = polygon
  )
}

# ==============================================================================
# 3. DYNAMICALLY GENERATING LOOPS FROM COMMUNITIES
# ==============================================================================

# Function to get an ordered list of nodes from a community's convex hull
get_ordered_loop_nodes_from_community0 <- function(community_nodes, all_positions) {
  
  community_coords <- all_positions %>%
    filter(name %in% as.character(community_nodes)) %>%
    select(x, y)
    
  hull_coords <- community_coords %>%
    st_as_sf(coords = c("x", "y")) %>%
    st_union() %>%
    st_convex_hull() %>%
    st_cast("LINESTRING") %>%
    st_coordinates() %>%
    as_tibble() %>%
    slice(1:(n() - 1)) %>%
    select(X, Y)
  
  ordered_node_names <- all_positions %>%
    semi_join(hull_coords, by = c("x" = "X", "y" = "Y")) %>%
    pull(name)
  
  return(ordered_node_names)
}
# This function takes the unordered nodes of a community and their positions,
# and returns an ordered list of nodes that form the convex hull (boundary).
get_ordered_loop_nodes_from_community <- function(community_nodes, all_positions) {

  # 1. Filter to get the positions of the community nodes
  community_coords_df <- all_positions %>%
    filter(name %in% as.character(community_nodes)) %>%
    select(name, x, y)
    
  # 2. Convert to an sf object and find the convex hull polygon
  community_sf <- st_as_sf(community_coords_df, coords = c("x", "y"))
  hull_polygon <- st_convex_hull(st_union(community_sf))
  
  # 3. Extract the ordered coordinates of the hull's vertices
  hull_coords_raw <- st_coordinates(hull_polygon)[, 1:2]
  hull_coords_unique <- unique(hull_coords_raw)
  
  # 4. Convert hull coordinates to an sf object for spatial join
  hull_coords_sf <- st_as_sf(as.data.frame(hull_coords_unique), coords = c("X", "Y"))
  
  # 5. Spatially join the community nodes with the hull vertices
  # This matches the nodes that are on the hull's boundary to their ordered position
  # The 'by' argument is not needed here; it's a spatial join.
  nodes_on_hull <- st_join(community_sf, hull_coords_sf, st_equals) %>%
    # Remove any NA values that might result if a node is not on the hull
    drop_na()
  
  # 6. Re-order the nodes based on the hull's sequence
  # This is the tricky part. We need to match the ordered hull coords to the joined nodes.
  # A simple spatial join doesn't preserve order, so we do it manually.
  ordered_names <- c()
  for (i in 1:nrow(hull_coords_unique)) {
    # Find the node that matches the current hull vertex coordinate
    match <- community_coords_df %>%
      filter(x == hull_coords_unique[i, 1] & y == hull_coords_unique[i, 2])
    
    if (nrow(match) > 0) {
      ordered_names <- c(ordered_names, match$name[1])
    }
  }
  
  return(ordered_names)
}

# You can uncomment this section to run the analysis on community-derived loops
#-------------------------------------------------------------------------------
community_flag <-1
if(community_flag ==1){
 communities <- cluster_louvain(g_base)
 community_membership <- membership(communities)
 names(community_membership) <- as.character(1:vcount(g_base))
 
 dynamic_loops <- list()
 for (comm_id in unique(community_membership)) {
   community_nodes <- names(community_membership[community_membership == comm_id])
  if (length(community_nodes) >= 3) {
     ordered_nodes <- get_ordered_loop_nodes_from_community(community_nodes, positions)
# # We close the loop by adding the first node at the end
     dynamic_loops[[paste0("Community_", comm_id)]] <- c(ordered_nodes, ordered_nodes[1])
   }
 }

 loops <- dynamic_loops # Use the dynamically generated loops for the analysis
}
#-------------------------------------------------------------------------------
if(community_flag ==0){
loops <- list(
  Loop_A = c(1, 2, 3, 4, 1),
  Loop_B = c(5, 6, 11, 17, 5),
  Loop_C = c(34, 32, 20, 19, 33, 34)
)
}
loops
# ==============================================================================
# 4. ANALYSIS EXECUTION
# ==============================================================================

all_summary <- tibble()
all_contained <- tibble(node = positions$name)
all_covered <- tibble(node = positions$name)
all_winding <- tibble(node = positions$name)
global_metrics_df <- tibble()
all_polygons_list <- vector("list", length = length(loops))
all_polygons_list_names <- names(loops)

# Scenario 0: Basic network (BN)
cat("--- Scenario 0: Basic Network (BN) ---\n")
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_base) %>% mutate(Scenario = "0: BN"))

# Run analysis for each loop dynamically
for (i in 1:length(loops)) {
  loop_name <- names(loops)[i]
  loop_nodes <- loops[[i]]
  
  cat(paste0("--- Scenario: BN + ", loop_name, " ---\n"))
  results <- run_analysis(g_base, loop_name, loop_nodes, positions, nodes_sf)
  
  all_summary <- bind_rows(all_summary, results$summary)
  all_contained <- bind_cols(all_contained, results$contained_matrix)
  all_covered <- bind_cols(all_covered, results$covered_matrix)
  all_winding <- bind_cols(all_winding, results$winding_matrix)
  global_metrics_df <- bind_rows(global_metrics_df, results$global_metrics %>% mutate(Scenario = paste0(loop_name, ": BN + ", loop_name)))
  all_polygons_list[[i]] <- results$polygon
  all_polygons_list_names[i] <- loop_name
  cat("\n")
}

names(all_polygons_list) <- all_polygons_list_names

# ==============================================================================
# 5. FINAL OUTPUT AND VISUALIZATION
# ==============================================================================

cat("Global Network Metrics Comparison:\n")
print(global_metrics_df)

cat("\nLoop Summary Metrics:\n")
print(all_summary)

cat("\nContained Matrix (strictly inside):\n")
print(all_contained)

cat("\nCovered Matrix (inside or on boundary):\n")
print(as.data.frame(all_covered))

cat("\nWinding Number Matrix (nodes x loops):\n")
print(all_winding)

# Plot the Network and Loops
loop_polygons_df <- st_sf(
  ID = names(all_polygons_list),
  geometry = st_sfc(all_polygons_list)
)

network_plot <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", size = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.3, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "steelblue", size = 3) +
  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(title = "Network Loops and Enhancements",
       subtitle = "Visualizing the Geometric Boundaries of the Loops",
       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID")
plot(network_plot)
ggsave("network_loops_plot.png", plot = network_plot, width = 8, height = 6, dpi = 300)
cat("\nPlot has been saved to 'network_loops_plot.png'\n")





################################################## aqui #################################################
################################################## aqui #################################################
################################################## aqui #################################################
################################################## aqui #################################################


############################################# anterior + influence index
############################################# anterior + influence index
# Required packages
library(igraph)
library(sf)
library(tidyverse)
library(ggplot2)

# Set a seed for reproducibility
set.seed(42)

# ==============================================================================
# 1. NETWORK DEFINITION AND SPATIAL DATA
# ==============================================================================

# Define the base network (Zachary's Karate Club)
g_base <- make_graph("Zachary")
network1="Zachary"

set.seed(42)
layout_coords <- layout_with_fr(g_base)

### aqui desde mapas
network1="Barcelona"
positions <- tibble(
  name = as.character(1:vcount(g_base)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)
positions #verificar si son de mapas o desde file

######### ojo abajo position bologna? cro que no
positions <- tibble(
  name = (V(g_base)),
#  name = as.character(V(g_base)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)

nodes_sf <- st_as_sf(positions, coords = c("x", "y"))

# Prepare edge data for plotting the base network
edges <- igraph::as_data_frame(g_base, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  ) %>%
  left_join(positions %>% rename(x_from = x, y_from = y), by = c("from" = "name")) %>%
  left_join(positions %>% rename(x_to = x, y_to = y), by = c("to" = "name"))

nodes_sf 
head(edges)

# ==============================================================================
# 2. CORE FUNCTIONS
# ==============================================================================

# Function to calculate global network metrics
calculate_global_metrics <- function(g) {
  sp <- distances(g, mode = "all")
  finite_sp <- sp[is.finite(sp)]
  
  avg_path_length <- mean(finite_sp)
  efficiency <- 1 / avg_path_length
  avg_betweenness_centrality <- mean(betweenness(g))
  num_components <- components(g)$no
  
  tibble(
    avg_path_length = avg_path_length,
    network_efficiency = efficiency,
    avg_betweenness_centrality = avg_betweenness_centrality,
    num_components = num_components
  )
}

# Function to generate edges from an ordered list of nodes
generate_loop_edges <- function(loop_nodes) {
  num_nodes <- length(loop_nodes)
  from_nodes <- loop_nodes[1:(num_nodes - 1)]
  to_nodes <- loop_nodes[2:num_nodes]
  tibble(from = from_nodes, to = to_nodes)
}

# Function to compute winding number
compute_winding_number <- function(px, py, loop_coords_df) {
  angles <- 0
  n <- nrow(loop_coords_df)
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    angle <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    angles <- angles + angle
  }
  return(angles / (2 * pi))
}

# Function to compute distance-weighted generalized winding number chatgpt
compute_winding_number <- function(px, py, loop_coords_df) {
  gwn <- 0
  n <- nrow(loop_coords_df)
  
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    
    # Shift coordinates so (px, py) is origin
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    
    # Vector lengths
    r1 <- sqrt(x1^2 + y1^2)
    r2 <- sqrt(x2^2 + y2^2)
    
    # Angle between edges
    theta <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    
    # Distance-weighted contribution
    weight <- 1 / (r1 * r2)
    gwn <- gwn + weight * theta / (2 * pi)
  }
  
  return(gwn)
}

# Function for geometric influence index
compute_geometric_influence <- function(point_x, point_y, polygon) {
  centroid <- st_centroid(polygon)
  centroid_x <- st_coordinates(centroid)[1]
  centroid_y <- st_coordinates(centroid)[2]

  distance <- sqrt((point_x - centroid_x)^2 + (point_y - centroid_y)^2)
  
  if (distance == 0) {
    return(0)
  }
  
  return(1 / distance^2)
}

# Function to create an sf polygon from ordered loop nodes
create_loop_polygon <- function(loop_nodes, all_positions) {
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(all_positions, by = "name")
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  st_polygon(list(poly_coords))
}

# Function to get an ordered list of nodes from a community's convex hull
get_ordered_loop_nodes_from_community <- function(community_nodes, all_positions) {
  
  community_coords_df <- all_positions %>%
    filter(name %in% as.character(community_nodes)) %>%
    select(name, x, y)
    
  community_sf <- st_as_sf(community_coords_df, coords = c("x", "y"))
  hull_polygon <- st_convex_hull(st_union(community_sf))
  
  hull_coords_raw <- st_coordinates(hull_polygon)[, 1:2]
  hull_coords_unique <- unique(hull_coords_raw)
  
  ordered_names <- c()
  for (i in 1:nrow(hull_coords_unique)) {
    match <- community_coords_df %>%
      filter(x == hull_coords_unique[i, 1] & y == hull_coords_unique[i, 2])
    
    if (nrow(match) > 0) {
      ordered_names <- c(ordered_names, match$name[1])
    }
  }
  
  return(ordered_names)
}

# Main function to run the full analysis for a given loop and network VIEJA
run_analysis1 <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  
  new_edges <- generate_loop_edges(loop_nodes)
  g_enhanced <- add_edges(g, as.vector(t(new_edges[, c("from", "to")])))
  
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(positions, by = "name")
  
  polygon <- create_loop_polygon(loop_nodes, positions)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  strictly_contained_result <- st_within(nodes_sf, st_sfc(polygon))
  on_boundary_result <- st_touches(nodes_sf, st_sfc(polygon))
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
#scaled_positions <- scale_coords(positions)
#scaled_loop_coords <- scale_coords(loop_coords_df)
# Now, use the scaled coordinates for the winding number calculation
#winding_number_values <- sapply(1:nrow(scaled_positions), function(i) {
#  compute_winding_number(scaled_positions$x[i], scaled_positions$y[i], scaled_loop_coords)
#})
  
  winding_number_values <- sapply(1:nrow(positions), function(i) {
    compute_winding_number(positions$x[i], positions$y[i], loop_coords_df)
  })

  geometric_influence_values <- sapply(1:nrow(positions), function(i) {
    compute_geometric_influence(positions$x[i], positions$y[i], polygon)
  })
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Mean_Geometric_Influence = mean(geometric_influence_values),
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count
  )

  geometric_matrix_col <- tibble(influence = geometric_influence_values)
  colnames(geometric_matrix_col) <- paste0("Loop_", loop_name)
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  winding_matrix_col <- tibble(winding_number = winding_number_values)
  colnames(winding_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    global_metrics = calculate_global_metrics(g_enhanced),
    summary = summary_row,
    geometric_matrix = geometric_matrix_col,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    winding_matrix = winding_matrix_col,
    polygon = polygon
  )
}

##run analysis nueva fractional_influence_matrix
run_analysis <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  
  new_edges <- generate_loop_edges(loop_nodes)
  g_enhanced <- add_edges(g, as.vector(t(new_edges[, c("from", "to")])))
  
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(positions, by = "name")
  
  polygon <- create_loop_polygon(loop_nodes, positions)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  strictly_contained_result <- st_within(nodes_sf, st_sfc(polygon))
  on_boundary_result <- st_touches(nodes_sf, st_sfc(polygon))
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
  
  # --- New code for Fractional Influence (Distance-based) ---
  # Calculate the distance from each node to the loop's perimeter
  distance_to_loop <- as.numeric(st_distance(nodes_sf, st_cast(polygon, "LINESTRING")))
  
  # Normalize the distances to create the "Fractional Influence" score (0 to 1)
  max_dist <- max(distance_to_loop)
  min_dist <- min(distance_to_loop)
  fractional_influence_values <- 1 - (distance_to_loop - min_dist) / (max_dist - min_dist + 1e-9)
  # --- End of new code ---
  
  geometric_influence_values <- sapply(1:nrow(positions), function(i) {
    compute_geometric_influence(positions$x[i], positions$y[i], polygon)
  })
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Mean_Geometric_Influence = mean(geometric_influence_values),
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count
  )

  geometric_matrix_col <- tibble(influence = geometric_influence_values)
  colnames(geometric_matrix_col) <- paste0("Loop_", loop_name)
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  # New output for the distance-based fractional influence
  fractional_influence_matrix_col <- tibble(fractional_influence = fractional_influence_values)
  colnames(fractional_influence_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    global_metrics = calculate_global_metrics(g_enhanced),
    summary = summary_row,
    geometric_matrix = geometric_matrix_col,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    fractional_influence_matrix = fractional_influence_matrix_col,
    polygon = polygon
  )
}

# ==============================================================================
# 4. ANALYSIS EXECUTION
# ==============================================================================

# Choose one of the following scenarios for your loops.
# To use Community Detection, uncomment SCENARIO B.
#-------------------------------------------------------------------------------
# SCENARIO A: Use hard-coded loops
community_flag <-1
if(community_flag ==0){
loops <- list(
  Loop_A = c(1, 3,2, 4, 1),
  Loop_B = c(5, 6, 11, 17, 5),
#  Loop_C = c(34, 20,13, 19, 33, 34)#,
  Loop_C = c(34, 20,10, 19, 33, 34),
  Loop_D = c(26, 29, 21, 27, 26 ) #zachary
 # Loop_D = c(49,15,28,18,27,48,62,49)
)
}
#-------------------------------------------------------------------------------
# SCENARIO B: Use dynamically generated loops from community detection
#

if(community_flag ==1){
set.seed(42)
 communities <- cluster_louvain(g_base)
 community_membership <- membership(communities)
 names(community_membership) <- V(g_base)#as.character(1:vcount(g_base))
 ################################
# Assuming g_base and community_membership are already defined

# 1. Create a color palette
community_colors <- rainbow(length(unique(community_membership)))

# 2. Map the colors to the nodes based on their membership
# The membership vector's names are the node IDs, so this is a direct mapping.
node_colors <- community_colors[community_membership]

# 3. Plot the graph with the new vertex colors
windows()
plot(g_base, 
     layout = layout_coords,
     vertex.color = node_colors,
     vertex.size = 2,
     vertex.label = NA,
     main = paste(network1," Road Network Communities"))
#legend("topright", 
#       legend = paste("Community", community_ids),
#       pch = 19, # Use a filled circle to match the vertices
#       col = community_colors,
#       title = "Communities")
windows()
plot(communities, g_base, layout=layout_coords, vertex.label=NA, vertex.size=.2,  edge.arrow.size=.2)

################################
 dynamic_loops <- list()
 for (comm_id in unique(community_membership)) {
   community_nodes <- names(community_membership[community_membership == comm_id])
   if (length(community_nodes) >= 3) {
     ordered_nodes <- get_ordered_loop_nodes_from_community(community_nodes, positions)
     dynamic_loops[[paste0("Community_", comm_id)]] <- c(ordered_nodes, ordered_nodes[1])
   }
 }
 
 loops <- dynamic_loops # Uncomment this line to use dynamic loops
}
loops
 loops_orig=loops
##########################################

 
 loops= list(Loop36=loops_orig[[36]],Loop26=loops_orig[[26]],Loop27=loops_orig[[27]],Loop38=loops_orig[[38]])
 loops

#-------------------------------------------------------------------------------

all_summary <- tibble()
all_geometric <- tibble(node = positions$name)
all_contained <- tibble(node = positions$name)
all_covered <- tibble(node = positions$name)
all_winding <- tibble(node = positions$name)
global_metrics_df <- tibble()
all_polygons_list <- vector("list", length = length(loops))
all_polygons_list_names <- names(loops)

# Scenario 0: Basic network (BN)
cat("--- Scenario 0: Basic Network (BN) ---\n")
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_base) %>% mutate(Scenario = "0: BN"))

# Run analysis for each loop dynamically
for (i in 1:length(loops)) {
  loop_name <- names(loops)[i]
  loop_nodes <- loops[[i]]
  
  cat(paste0("--- Scenario: BN + ", loop_name, " ---\n"))
  results <- run_analysis(g_base, loop_name, loop_nodes, positions, nodes_sf)
  
  all_summary <- bind_rows(all_summary, results$summary)
  all_geometric <- bind_cols(all_geometric, results$geometric_matrix)
  all_contained <- bind_cols(all_contained, results$contained_matrix)
  all_covered <- bind_cols(all_covered, results$covered_matrix)
#  all_winding <- bind_cols(all_winding, results$winding_matrix)
##### ojo se usa el nombre de all_winding para fractional ...
  all_winding <- bind_cols(all_winding, results$fractional_influence_matrix)
  global_metrics_df <- bind_rows(global_metrics_df, results$global_metrics %>% mutate(Scenario = paste0(loop_name, ": BN + ", loop_name)))
  all_polygons_list[[i]] <- results$polygon
  all_polygons_list_names[i] <- loop_name
  cat("\n")
}
###############################
#list(
#    global_metrics = calculate_global_metrics(g_enhanced),
#    summary = summary_row,
#    geometric_matrix = geometric_matrix_col,
#    contained_matrix = contained_matrix_col,
#    covered_matrix = covered_matrix_col,
#    fractional_influence_matrix = fractional_influence_matrix_col,
#    polygon = polygon
###########################

names(all_polygons_list) <- all_polygons_list_names

# ==============================================================================
# 5. FINAL OUTPUT AND VISUALIZATION
# ==============================================================================

cat("Global Network Metrics Comparison:\n")
print(global_metrics_df)
as.data.frame(global_metrics_df)
cat("\nLoop Summary Metrics:\n")
print(all_summary)
as.data.frame(global_metrics_df)
as.data.frame(all_summary)

cat("\nGeometric Influence Matrix (nodes x loops):\n")
print(all_geometric)

cat("\nContained Matrix (strictly inside):\n")
print(all_contained)

cat("\nCovered Matrix (inside or on boundary):\n")
print(all_covered)

cat("\nWinding Number Matrix (nodes x loops):\n")
print(all_winding)
###########

all_results_df <-cbind(global_metrics_df[-1,],all_summary)
# 1. Normalize the data
normalized_df <- all_results_df %>%
  # Remove the base network ("BN") row for normalization since it's the baseline
  filter(Loop_ID != "BN") %>%
  mutate(
    # Normalize metrics where HIGHER is better
    network_efficiency = (network_efficiency - min(network_efficiency)) / (max(network_efficiency) - min(network_efficiency)),
    avg_betweenness_centrality = (avg_betweenness_centrality - min(avg_betweenness_centrality)) / (max(avg_betweenness_centrality) - min(avg_betweenness_centrality)),
    Mean_Geometric_Influence = (Mean_Geometric_Influence - min(Mean_Geometric_Influence)) / (max(Mean_Geometric_Influence) - min(Mean_Geometric_Influence)),
    Total_Covered_Nodes = (Total_Covered_Nodes - min(Total_Covered_Nodes)) / (max(Total_Covered_Nodes) - min(Total_Covered_Nodes)),
    # Normalize metrics where LOWER is better
    avg_path_length = 1 - (avg_path_length - min(avg_path_length)) / (max(avg_path_length) - min(avg_path_length)),
    Area = 1 - (Area - min(Area)) / (max(Area) - min(Area)),
    Perimeter = 1 - (Perimeter - min(Perimeter)) / (max(Perimeter) - min(Perimeter))
  )

# 2. Assign weights and calculate the composite index
weighted_df <- normalized_df %>%
  mutate(
    # Apply weights
    weighted_score = (network_efficiency * 0.20) + 
                     (avg_betweenness_centrality * 0.15) +
                     (avg_path_length * 0.15) + 
                     (Mean_Geometric_Influence * 0.15) +
                     (Total_Covered_Nodes * 0.15) +
                     (Area * 0.10) +
                     (Perimeter * 0.10)
  )

# 3. Rank the loops from best to worst
ranked_loops <- weighted_df %>%
  select(Loop_ID, weighted_score) %>%
  arrange(desc(weighted_score))

print("Loop Ranking based on Composite Index:")
print(ranked_loops)
########################################################
weight_scenarios <- list(
  scenario1_balanced = c(
    network_efficiency = 0.20,
    avg_betweenness_centrality = 0.15,
    avg_path_length = 0.15,
    Mean_Geometric_Influence = 0.15,
    Total_Covered_Nodes = 0.15,
    Area = 0.10,
    Perimeter = 0.10
  ),
  scenario2_topological = c(
    network_efficiency = 0.30,
    avg_betweenness_centrality = 0.25,
    avg_path_length = 0.25,
    Mean_Geometric_Influence = 0.05,
    Total_Covered_Nodes = 0.05,
    Area = 0.05,
    Perimeter = 0.05
  ),
  scenario3_cost_focused = c(
    network_efficiency = 0.10,
    avg_betweenness_centrality = 0.10,
    avg_path_length = 0.10,
    Mean_Geometric_Influence = 0.10,
    Total_Covered_Nodes = 0.10,
    Area = 0.25,
    Perimeter = 0.25
  )
)
rank_loops <- function(data, weights) {
  # Calculate the weighted score using the provided weights
  weighted_df <- data %>%
    mutate(
      weighted_score = (network_efficiency * weights["network_efficiency"]) +
                       (avg_betweenness_centrality * weights["avg_betweenness_centrality"]) +
                       (avg_path_length * weights["avg_path_length"]) +
                       (Mean_Geometric_Influence * weights["Mean_Geometric_Influence"]) +
                       (Total_Covered_Nodes * weights["Total_Covered_Nodes"]) +
                       (Area * weights["Area"]) +
                       (Perimeter * weights["Perimeter"])
    )

  # Rank the loops
  ranked_loops <- weighted_df %>%
    select(Loop_ID, weighted_score) %>%
    arrange(desc(weighted_score))
  
  return(ranked_loops)
}
all_rankings <- lapply(weight_scenarios, function(weights) {
  rank_loops(normalized_df, weights)
})

# To view the ranking for a specific scenario, e.g., 'scenario2_topological'
print(all_rankings$scenario2_topological)
#####################################
# Required packages
library(dplyr)
library(ggplot2)
library(tidyr) # For pivot_wider

# --------------------------------------------------------------------------
# 1. DEFINE DATA AND WEIGHT GENERATION FUNCTION
# --------------------------------------------------------------------------

# Replace this hard-coded data frame with your actual normalized_df
normalized_df <- data.frame(
  Loop_ID = c("Loop36", "Loop26", "Loop27", "Loop38"),
  avg_path_length = c(1.0000000, 0.9521042, 0.5614422, 0.0000000),
  network_efficiency = c(1.0000000, 0.9498246, 0.5493695, 0.0000000),
  avg_betweenness_centrality = c(0.00000000, 0.04789579, 0.43855776, 1.00000000),
  Area = c(0.1487161, 1.0000000, 0.9469946, 0.0000000),
  Perimeter = c(0.00000000, 0.82421106, 1.00000000, 0.07289895),
  Mean_Geometric_Influence = c(0.5125821, 0.6115071, 1.0000000, 0.0000000),
  Total_Covered_Nodes = c(1.0000000, 0.1038961, 0.0000000, 0.2077922)
)

# Function to generate constrained random weights
generate_constrained_weights <- function(constraints_df, n_scenarios) {
  # Get number of metrics
  n_metrics <- nrow(constraints_df)
  
  # Initialize a matrix to store the generated weights
  generated_weights <- matrix(NA, nrow = n_scenarios, ncol = n_metrics)
  colnames(generated_weights) <- constraints_df$metric
  
  for (i in 1:n_scenarios) {
    # Calculate the remaining sum to be distributed
    remaining_sum <- 1 - sum(constraints_df$min_w)
    
    # Generate random deltas within the allowed ranges
    deltas <- runif(n_metrics, min = 0, max = (constraints_df$max_w - constraints_df$min_w))
    
    # Scale the deltas to sum to the remaining mass
    scaled_deltas <- deltas * (remaining_sum / sum(deltas))
    
    # Finalize the weights by adding scaled deltas to minimums
    final_weights <- constraints_df$min_w + scaled_deltas
    
    # Store the results
    generated_weights[i, ] <- final_weights
  }
  
  return(as.data.frame(generated_weights))
}

# --------------------------------------------------------------------------
# 2. DEFINE WEIGHT CONSTRAINTS AND RUN THE MONTE CARLO SIMULATION
# --------------------------------------------------------------------------

# Set the number of simulations
n_simulations <- 1000

# Define the constraints for your metrics
 base_weight <- c(0.20, 0.15,0.15,0.15,0.15,0.10,0.10)
porcentaje=0.4
weight_constraints <- data.frame(
  metric = c("avg_path_length", "network_efficiency", "avg_betweenness_centrality",
             "Area", "Perimeter", "Mean_Geometric_Influence", "Total_Covered_Nodes"),
#  min_w = c(0.15, 0.15, 0.10, 0.05, 0.05, 0.10, 0.10),
#  max_w = c(0.25, 0.25, 0.20, 0.15, 0.15, 0.20, 0.20)
min_w = base_weight * (1-porcentaje),max_w=base_weight*(1+porcentaje)
)
porcentaje
weight_constraints

# Generate the constrained weight scenarios
constrained_scenarios <- generate_constrained_weights(weight_constraints, n_simulations)

# Get the list of all metric names from your data frame
metrics <- names(normalized_df)[-1]

# Create an empty data frame to store all simulation results
all_rankings <- data.frame(
  Loop_ID = character(),
  rank = integer(),
  stringsAsFactors = FALSE
)

# Run the simulation loop
for (i in 1:n_simulations) {
  # Get the weights for the current scenario
  weights_vector <- as.numeric(constrained_scenarios[i, ])
  names(weights_vector) <- metrics

  # Calculate the weighted score using matrix multiplication for efficiency
  weighted_scores <- as.matrix(normalized_df[, metrics]) %*% weights_vector
  
  # Create a ranked data frame for this simulation
  current_ranking <- data.frame(
    Loop_ID = normalized_df$Loop_ID,
    weighted_score = as.numeric(weighted_scores)
  ) %>%
    arrange(desc(weighted_score)) %>%
    mutate(rank = row_number())
  
  # Append the results to the main data frame
  all_rankings <- rbind(all_rankings, current_ranking[, c("Loop_ID", "rank")])
}

# --------------------------------------------------------------------------
# 3. AGGREGATE AND VISUALIZE THE RESULTS
# --------------------------------------------------------------------------

# Aggregate the data to count the occurrences of each loop at each rank
rank_counts <- all_rankings %>%
  group_by(Loop_ID, rank) %>%
  summarise(count = n(), .groups = "drop")

# Create the heatmap
heatmap_plot <- ggplot(rank_counts, aes(x = as.factor(rank), y = Loop_ID, fill = count)) +
  geom_tile(color = "white", linewidth = 0.5) +
  scale_fill_gradient(low = "lightblue", high = "navy", name = "Frequency") +
  labs(
    title = "Robustness of Loop Rankings",
    subtitle = paste("Based on", n_simulations, "Monte Carlo Simulations"),
    x = "Rank Position",
    y = "Loop ID"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "bottom"
  )
rank_counts
windows()
print(heatmap_plot)
# Calculate the percentage for each rank/loop combination
total_simulations <- n_simulations
rank_percentages <- rank_counts %>%
  mutate(percentage = (count / total_simulations) * 100)

# Create the heatmap with enhanced aesthetics and text labels
heatmap_plot <- ggplot(rank_percentages, aes(x = as.factor(rank), y = Loop_ID, fill = count)) +
  geom_tile(color = "white", linewidth = 0.5) +
  scale_fill_gradient(low = "lightblue", high = "navy", name = "Frequency") +
  # Add the percentage labels to the cells
  geom_text(aes(label = sprintf("%.1f%%", percentage)), color = "black", size = 4) +
#  labs(
#    title = "Robustness of Loop Rankings",
#    subtitle = paste("Based on", n_simulations, "Monte Carlo Simulations"),
#    x = "Rank Position",
#    y = "Loop ID"
#  ) +
  labs(
    x = "Rank Position",
    y = "Loop ID"
  ) +

  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    legend.position = "bottom",
    # Increase the size of axis titles (labels)
    axis.title = element_text(size = 14,face="bold"),
    # Increase the size of axis values (numbers/text on the axis)
    axis.text = element_text(size = 12,face="bold")
  )
windows()
print(heatmap_plot)
ggsave(paste(network1,"_heat_plot.jpeg"), plot = heatmap_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_heat_plot.pdf"), plot = heatmap_plot, width = 10, height = 8, dpi = 300)












#######################################################
# Assuming your polygon data is in an sf object named 'loop_polygons_df'
# with columns for area and perimeter that are currently in degrees.

# 1. Set the original coordinate reference system (CRS) to WGS 84
# (EPSG: 4326), which is the standard for latitude/longitude.
loop_polygons_wgs84 <- st_set_crs(loop_polygons_df, 4326)

# 2. Reproject the data to a projected CRS.
# For Barcelona, UTM Zone 31N (EPSG: 32631) is a suitable choice.
loop_polygons_reprojected <- st_transform(loop_polygons_wgs84, 32631)

# 3. Recalculate the area and perimeter with the new units (meters)
area_in_sq_m <- st_area(loop_polygons_reprojected)
perimeter_in_m <- st_length(st_cast(loop_polygons_reprojected, "LINESTRING"))

# The results will now be in square meters and meters, which are interpretable
# as real-world values.
print(paste("Area (m²):", area_in_sq_m))
print(paste("Perimeter (m):", perimeter_in_m))


##########
# --- NEW PLOTTING CODE ---
loop_polygons_df <- st_sf(
  ID = names(all_polygons_list),
  geometry = st_sfc(all_polygons_list)
)
windows()
network_plot <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", size = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "steelblue", size = 0.3) +
  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 0.6) +
  labs(title = "Network Loops and Enhancements",
       subtitle = "Visualizing the Geometric Boundaries of the Loops",
       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID")

print(network_plot)

windows()
network_plot1 <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", size = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "steelblue", size = 0.3) +
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(title = "Network Loops and Enhancements",
       subtitle = "Visualizing the Geometric Boundaries of the Loops",
       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID")

print(network_plot1)

ggsave(paste(network1,"_GEInfluloops_plot.jpeg"), plot = network_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEInfluloops_plot.pdf"), plot = network_plot, width = 10, height = 8, dpi = 300)

ggsave(paste(network1,"_GEInfluloops_plotx.jpeg"), plot = network_plot1, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEInfluloops_plotx.pdf"), plot = network_plot1, width = 10, height = 8, dpi = 300)

 ggsave(paste(network1,"_GEInfluloops_plotx.eps"), plot = network_plot1, width = 10, height = 8, dpi = 300)

cat("\nPlot has been saved to 'network_loops_plot.png'\n")



all_winding
all_winding_frame=as.data.frame(all_winding[,-1])
#all_winding_frame
#round(all_winding_frame,3)
head(round(all_winding_frame,3))
distance_winding = matrix(0,length(loops),length(loops))
for (i in 1:length(loops)) {
for (j in 1:length(loops)) {

distance_winding[i,j] <-mean(round(all_winding_frame[as.numeric(loops[[i]]),j],3))
}
}
 colnames(distance_winding) <-names(loops)
 rownames(distance_winding) <-names(loops)
 round(distance_winding,4)

 all_covered_numeric <- all_covered %>% select(-node)
all_winding_numeric <- all_winding %>% select(-node)

 final_score_matrix <- all_covered_numeric * all_winding_numeric

final_score_matrix [which(community_membership==26),2]
which(final_score_matrix [,2]>0.9)

########################### modifica plot
windows()
network_plot_base <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", linewidth = 0.5) +
#  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "black", size = 0.5) + 
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) + 
  labs(x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID") +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14), 
    axis.text = element_text(size = 12, face = "bold"), 
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plot_base)

windows()
network_plot <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", linewidth = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "black", size = .2) + 
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) + 
  labs(x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID") +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14), 
    axis.text = element_text(size = 12, face = "bold"), 
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plot)


ggsave(paste(network1,"_GEInfluloops_plotA.jpeg"), plot = network_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEInfluloops_plotA.pdf"), plot = network_plot, width = 10, height = 8, dpi = 300)


######LLLLL
# Find the min/max of the coordinates to set consistent plot limits
x_min <- min(positions$x)
x_max <- max(positions$x)
y_min <- min(positions$y)
y_max <- max(positions$y)

network_plotx <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", linewidth = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "black", size = .2) +
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID") +
  theme(plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12, face = "bold"),
    legend.position = "bottom"
  ) +
  coord_sf()
print(network_plotx)
ggsave(paste(network1,"_GEInfluloops_plotA.jpeg"), plot = network_plotx, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEInfluloops_plotA.pdf"), plot = network_plotx, width = 10, height = 8, dpi = 300)

# ----------------------------------------
# Plot Original Network
# ----------------------------------------
windows()
original_network_plot <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", linewidth = 0.5) +
  geom_point(data = positions, aes(x = x, y = y), color = "black", size = 0.2) +
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12, face = "bold")
  ) +
  coord_sf()

print(original_network_plot)
ggsave(paste(network1,"_GEInfluloops_plotorg.jpeg"), plot = original_network_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEInfluloops_plotorg.pdf"), plot = original_network_plot, width = 10, height = 8, dpi = 300)

######LLLLLL
######### fin modifica plot
######GGGGGG con escala... detallitos 
# --------------------------------------------------------------------------
# Plotting the Network and Loops
# --------------------------------------------------------------------------

# Define the coordinates for manual axis scaling (adjust these values as needed)
x_min <- min(positions$x)
x_max <- max(positions$x)
y_min <- min(positions$y)
y_max <- max(positions$y)

# Create the first plot (base network only)
windows()
network_plot_base <- ggplot() +
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", linewidth = 0.5) +
  geom_point(data = positions, aes(x = x, y = y), color = "black", size = 0.5) +
#  geom_text(data = positions, aes(x = x, y = y, label = name), hjust = 0, vjust = -1, size = 3) +
  labs(x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID") +
  coord_fixed() + # Forces a 1:1 aspect ratio
  # You can uncomment and adjust the following lines for manual axis control
   xlim(x_min, x_max) +
   ylim(y_min, y_max) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12, face = "bold"),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plot_base)

# Create the second plot (network with loops)
windows()
network_plot <- C
print(network_plot)
###############################################

network_plot <- ggplot() +
  # Existing layers for edges, polygons, and all nodes
  geom_segment(data = edges, aes(x = x_from, y = y_from, xend = x_to, yend = y_to), color = "gray", size = 0.5) +
  geom_sf(data = loop_polygons_df, aes(fill = ID), alpha = 0.8, inherit.aes = FALSE) +
  geom_point(data = positions, aes(x = x, y = y), color = "steelblue", size = 0.5) +
  
  # New layer to plot only the selected nodes in a different color and size
  geom_point(data = selected, aes(x = x, y = y), color = "gray", size = 0.5) +
  
  # Optional: Add labels just for the selected nodes for clarity
#  geom_text(data = selected, aes(x = x, y = y, label = name), 
#            hjust = 0, vjust = -1, size = 1, color = "blue") +
  # You can uncomment and adjust the following lines for manual axis control
   xlim(x_min, x_max) +
   ylim(y_min, y_max) +
  
  # Rest of the plot code
  labs(title = "Network Loops and Enhancements",
       subtitle = "Visualizing the Geometric Boundaries of the Loops",
       x = "X Coordinate", y = "Y Coordinate") +
  theme_minimal() +
  scale_fill_discrete(name = "Loop ID")

print(network_plot)
######################################################################
################################### plot calles seleccionadas
######################################################################
# --------------------------------------------------------------------------
# Plotting the Largest Connected Component (Final, Robust Solution)
# --------------------------------------------------------------------------

# --- 1. Identify the nodes of the largest connected component ---
# Get the names (sequential IDs) of the nodes in your connected igraph object
nodes_in_largest_comp_names <- igraph::as_data_frame(graph_igraph, what = "vertices")$name

# --- 2. Filter the original sfnetwork to the largest connected component ---
# This is the key fix. We activate the nodes first, then filter.
network_connected_sfn <- network_sfn %>%
  sfnetworks::activate("nodes") %>%
  dplyr::filter(name %in% nodes_in_largest_comp_names)

# --- 3. Access the edges of this new, connected network for plotting ---
# This object now contains the geometry and attributes of the connected component only.
network_edges_connected <- network_connected_sfn %>%
  sfnetworks::activate("edges") %>%
  as_tibble()

# --- 4. Define the streets you want to highlight ---
lines_to_highlight <- c("Avinguda de Josep Tarradellas", "Carrer de París", "Avinguda de Sarrià", "Avinguda Diagonal", "Carrer del Comte d'Urgell", "Via Augusta", "Ronda del General Mitre", "Carrer de Muntaner", 
"Gran Via de les Corts Catalanes", "Carrer de Roger de Llúria")


lines_to_highlight <- c("Avinguda de Josep Tarradellas", "Carrer de París", "Avinguda de Sarrià", "Avinguda Diagonal",  "Via Augusta", "Ronda del General Mitre", "Carrer de Muntaner", 
"Gran Via de les Corts Catalanes", "Passeig de Maragall"  )


# --- 5. Prepare the data for plotting with custom colors ---
# We use case_when for a more robust assignment that handles the
# RColorBrewer limit and assigns a custom color for remaining roads.
edges_for_plot <- network_edges_connected %>%
  mutate(
    highlight_group = if_else(name %in% lines_to_highlight, name, "Other Roads")
  )

# --- 6. Create a dynamic color palette that handles the RColorBrewer limit ---
num_highlighted <- length(lines_to_highlight)
max_dark2_colors <- 8

# Check if the number of highlighted streets exceeds the palette limit
if (num_highlighted > max_dark2_colors) {
  # If more than 8 streets, use the full Dark2 palette for the first 8,
  # and a secondary palette for the rest.
  highlight_colors <- c(
    RColorBrewer::brewer.pal(max_dark2_colors, "Dark2"),
    RColorBrewer::brewer.pal(num_highlighted - max_dark2_colors, "Paired")
  )
} else {
  # If 8 or fewer streets, use the Dark2 palette
  highlight_colors <- RColorBrewer::brewer.pal(num_highlighted, "Dark2")
}

# Dynamically create the color vector for the plot
custom_colors <- c(
  setNames(highlight_colors, lines_to_highlight),
  "Other Roads" = "gray70"
)

# --- 7. Create the final plot ---
windows()
network_plotyy <- ggplot() +
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  scale_color_manual(values = custom_colors, name = "Highlighted Streets") +
  guides(color = guide_legend(title = "Highlighted Streets", override.aes = list(linewidth = 2))) +
  labs(title = "Largest Connected Component of the Barcelona Network",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plotyy)

ggsave(paste(network1,"_edges.jpeg"), plot = network_plotyy, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_edges.pdf"), plot = network_plotyy, width = 10, height = 8, dpi = 300)

windows()
# --------------------------------------------------------------------------
# Plotting the Network, Highlighted Streets, and Loops
# --------------------------------------------------------------------------

# --- 1. Identify the nodes of the largest connected component ---
# Get the names (sequential IDs) of the nodes in your connected igraph object
nodes_in_largest_comp_names <- igraph::as_data_frame(graph_igraph, what = "vertices")$name

# --- 2. Filter the original sfnetwork to the largest connected component ---
network_connected_sfn <- network_sfn %>%
  sfnetworks::activate("nodes") %>%
  dplyr::filter(name %in% nodes_in_largest_comp_names)

# --- 3. Access the edges and nodes of this new, connected network for plotting ---
network_edges_connected <- network_connected_sfn %>%
  sfnetworks::activate("edges") %>%
  as_tibble()

# --- 4. Define the streets you want to highlight ---
#lines_to_highlight <- c("Avinguda de Josep Tarradellas", "Carrer de París", "Avinguda de Sarrià", "Avinguda Diagonal", "Carrer del Comte d'Urgell", "Via Augusta", "Ronda del General Mitre", "Carrer de Muntaner", "Gran Via de les Corts Catalanes", "Carrer de Roger de Llúria")

# --- 5. Prepare the data for plotting with custom colors ---
edges_for_plot <- network_edges_connected %>%
  mutate(
    highlight_group = if_else(name %in% lines_to_highlight, name, "Other Roads")
  )

# --- 6. Create a dynamic color palette ---
num_highlighted <- length(lines_to_highlight)
max_dark2_colors <- 8
if (num_highlighted > max_dark2_colors) {
  highlight_colors <- c(RColorBrewer::brewer.pal(max_dark2_colors, "Dark2"),
                        RColorBrewer::brewer.pal(num_highlighted - max_dark2_colors, "Paired"))
} else {
  highlight_colors <- RColorBrewer::brewer.pal(num_highlighted, "Dark2")
}
custom_colors <- c(
  setNames(highlight_colors, lines_to_highlight),
  "Other Roads" = "gray70"
)

# --- 7. CRITICAL FIX: Set the CRS for the loops ---
# Get the CRS from the connected network
network_crs <- st_crs(network_connected_sfn)

# Assign that CRS to the loops object
loop_polygons_df_crs <- st_set_crs(loop_polygons_df, network_crs)

# --- 8. Create the combined plot ---
windows()
combined_plot <- ggplot() +
  # Plot the network edges as the base layer
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  # Add the loops as a transparent fill on top
  geom_sf(data = loop_polygons_df_crs, # Use the object with the CRS
          aes(fill = ID),
          alpha = 0.5,
          inherit.aes = FALSE) +
  scale_color_manual(values = custom_colors, name = "Highlighted Streets") +
  scale_fill_discrete(name = "Loop ID") +
  guides(color = guide_legend(title = "Highlighted Streets", order = 1),
         fill = guide_legend(title = "Loops", order = 2)) +
  labs(title = "Largest Connected Component with Loops",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(combined_plot)
ggsave(paste(network1,"_edgesloop.jpeg"), plot = combined_plot, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_edgeloop.pdf"), plot = combined_plot, width = 10, height = 8, dpi = 300)

windows()
network_plotyy <- ggplot() +
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  scale_color_manual(values = custom_colors, name = "Highlighted Streets") +
  guides(color = guide_legend(title = "Highlighted Streets", override.aes = list(linewidth = 2))) +
  labs(title = "Largest Connected Component of the Barcelona Network",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plotyy)

ggsave(paste(network1,"_edges.jpeg"), plot = network_plotyy, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_edges.pdf"), plot = network_plotyy, width = 10, height = 8, dpi = 300)
#############################################
################ plot con mapa
# --- 1. Load the administrative data (you should have this file) ---
#administrative_boundaries <- sf::st_read("path/to/your/file/barcelona_districts.shp")
administrative_boundaries <- st_read("BCN_Fronterers_Trams_ETRS89_SHP.shx")

# --- 2. Correctly transform the CRS ---
network_crs <- sf::st_crs(network_connected_sfn)
administrative_boundaries_transformed <- sf::st_transform(administrative_boundaries, network_crs)

# --- 3. Create the combined plot ---
windows()
combined_plot_map <- ggplot() +
  # Add the administrative layer with the correct fill column
  geom_sf(data = administrative_boundaries_transformed,
          aes(fill = Limit_A), # <--- REPLACE THIS LINE
          alpha = 0.3,
          inherit.aes = FALSE) +
  # The rest of the plot code remains the same
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  geom_sf(data = loop_polygons_df_crs,
          aes(fill = ID),
          alpha = 0.5,
          inherit.aes = FALSE) +
  scale_color_manual(values = custom_colors, name = "Highlighted Streets") +
  scale_fill_viridis_d(name = "Districts") +
#  guides(color = guide_legend(title = "Highlighted Streets", order = 1, override.aes = list(linewidth = 2)),
#         fill = guide_legend(title = "Administrative Areas", order = 2)) +
  labs(title = "Network and Loops with Administrative Context",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(combined_plot_map)

######## tiene error


ggsave(paste(network1,"_total.jpeg"), plot = combined_plot_map, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_total.pdf"), plot = combined_plot_map, width = 10, height = 8, dpi = 300)

windows()
combined_plot_map1 <- ggplot() +
  geom_sf(data = administrative_boundaries_transformed,
          aes(fill = Limit_A), # <--- REPLACE THIS LINE
          alpha = 0.3,
          inherit.aes = FALSE) +
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  geom_sf(data = loop_polygons_df_crs,
          aes(fill = ID),
          alpha = 0.5,
          inherit.aes = FALSE) +
  scale_color_manual(values = custom_colors) +
  scale_fill_discrete(name = "Loop ID") +
  # This is the key change: set the color guide to FALSE
  guides(color = "none",
         fill = guide_legend(title = "Loops")) +
  labs(       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "right"
  )
####
# --------------------------------------------------------------------------
# Plotting with only the Loop Legend and a Combined Fill Scale
# --------------------------------------------------------------------------

# Assuming the administrative_boundaries_transformed and other objects are already defined.

windows()

print(combined_plot_map1)
ggsave(paste(network1,"_total1.jpeg"), plot = combined_plot_map1, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_total1.pdf"), plot = combined_plot_map1, width = 10, height = 8, dpi = 300)

######### fin plot con mapa
######################################## plot edges selected con red reducida
# --------------------------------------------------------------------------
# Plotting the Largest Connected Component (Final, Robust Solution)
# --------------------------------------------------------------------------

# --- 1. Identify the nodes of the largest connected component ---
# Get the names (sequential IDs) of the nodes in your connected igraph object
nodes_in_largest_comp_names <- igraph::as_data_frame(graph_igraph, what = "vertices")$name

# --- 2. Filter the original sfnetwork to the largest connected component ---
# This is the key fix. We activate the nodes first, then filter.
network_connected_sfn <- network_sfn %>%
  sfnetworks::activate("nodes") %>%
  dplyr::filter(name %in% nodes_in_largest_comp_names)

# --- 3. Access the edges of this new, connected network for plotting ---
network_edges_connected <- network_connected_sfn %>%
  sfnetworks::activate("edges") %>%
  as_tibble()

# --- 4. Define the streets you want to highlight ---
lines_to_highlight <- c("Avinguda de Josep Tarradellas", "Carrer de Martin Luther King", "Avinguda de Sarrià", "Rambla de Prim")
lines_to_highlight <- c("Avinguda de Josep Tarradellas", "Carrer de París", "Avinguda de Sarrià", "Avinguda Diagonal", "Carrer del Comte d'Urgell","Via Augusta", "Ronda del General Mitre","Carrer de Muntaner", 
"Gran Via de les Corts Catalanes",  "Carrer de Roger de Llúria"    )

# --- 5. Prepare the data for plotting with custom colors ---
edges_for_plot <- network_edges_connected %>%
  mutate(
    highlight_group = if_else(name %in% lines_to_highlight, name, "Other Roads")
  )

custom_colors <- c(
  setNames(RColorBrewer::brewer.pal(length(lines_to_highlight), "Dark2"), lines_to_highlight),
  "Other Roads" = "gray70"
)
# Use the Set3 palette which supports up to 12 colors
custom_colors <- c(
  setNames(RColorBrewer::brewer.pal(length(lines_to_highlight), "Set3"), lines_to_highlight),
  "Other Roads" = "gray70"
)

# --- 6. Create the final plot ---
windows()
network_plot <- ggplot() +
  geom_sf(data = edges_for_plot,
          aes(color = highlight_group),
          linewidth = 0.8) +
  scale_color_manual(values = custom_colors, name = "Highlighted Streets") +
  guides(color = guide_legend(title = "Highlighted Streets", override.aes = list(linewidth = 2))) +
  labs(title = "Largest Connected Component of the Barcelona Network",
       x = "Longitude", y = "Latitude") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12),
    legend.title = element_text(size = 12),
    legend.text = element_text(size = 10),
    legend.position = "bottom"
  )

print(network_plot)
##### fin plot edges selected

###############################
# Replace NA/NaN values with 0
all_winding_orig <- all_winding
all_winding[is.na(all_winding)] <- 0

# Convert data to long format for easy plotting with ggplot2
all_geometric_long <- all_geometric %>%
  pivot_longer(
    cols = starts_with("Loop"),
    names_to = "Loop",
    values_to = "Geometric_Influence"
  )

all_winding_long <- all_winding %>%
  pivot_longer(
    cols = starts_with("Loop"),
    names_to = "Loop",
    values_to = "Winding_Number"
  )

# Join the two dataframes
combined_df <- left_join(all_geometric_long, all_winding_long, by = c("node", "Loop"))
windows()
# Create the plot
plotx <- ggplot(combined_df, aes(x = Geometric_Influence, y = Winding_Number)) +
  geom_point(alpha = 0.7, size = 2) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray") +
  # Use the 'node' column for the label aesthetic
  geom_text(aes(x = Geometric_Influence, y = Winding_Number,label = node), check_overlap = FALSE, nudge_y = 0.05, size = 3) +
  labs(
    title = "Generalized Winding Number vs. Geometric Influence",
    subtitle = "Analysis of three loops on a network",
    x = "Geometric Influence (1 / distance^2)",
    y = "Generalized Winding Number"
  ) +
  theme_minimal() +
  facet_wrap(~Loop, scales = "free")

# To display the plot, run:
windows()
print(plotx)

ggsave(paste(network1,"_GEinfvswind_plot.jpeg"), plot = plotx, width = 10, height = 8, dpi = 300)
 ggsave(paste(network1,"_GEinfvswind_plot.pdf"), plot = plotx, width = 10, height = 8, dpi = 300)


########
# una sintesis
#leo bologna, creo red, algo centrality y plot (el png es mejor?)
# --- 0. Install and Load Necessary Packages ---
# You only need to run install.packages() once per package on your system.
# install.packages("osmdata")
# install.packages("sf")
# install.packages("ggplot2")
# install.packages("dplyr")
# install.packages("sfnetworks")
# install.packages("tidygraph")
setwd("D:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
library(osmdata)
library(sf)
library(ggplot2)
library(dplyr)
library(sfnetworks)
library(tidygraph)
library(igraph)

library(tidyverse)

# --- 1. Efficient Data Acquisition for Bologna (Expanded Area) ---

message("Step 1: Acquiring OSM data for Bologna (expanded area)...")

# Define a much wider bounding box for Greater Bologna, encompassing the Tangenziale
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65) # [min_lon, min_lat, max_lon, max_lat]

bbox_barcelona <- getbb("Barcelona, Spain")
bologna_wider_bbox <-bbox_barcelona

names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")


# Query for all major roads within this wider area, including motorway types and links
message("  Querying for major roads in Wider Bologna area...")
##############################
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

all_roads_osm_org <- all_roads_osm
##################################################
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("primary", "secondary")) %>%
  osmdata_sf()

# Use these combined lines directly for network creation
all_network_lines <- all_roads_osm$osm_lines %>%
  filter(sf::st_is_valid(.)) 

message(paste("  Downloaded", nrow(all_network_lines), "road segments from the wider Bologna area."))


# --- 2. Convert sf Data to an sfnetwork object ---

message("Step 2: Converting sf data to sfnetwork object...")

# Create an sfnetwork object. This object IS your graph.
network_sfn <- sfnetworks::as_sfnetwork(all_network_lines, directed = FALSE)

# Add sequential 'name' attribute to nodes - NOW KEPT AS INTEGER
network_sfn <- network_sfn %>%
  tidygraph::activate("nodes") %>%
  dplyr::mutate(name = dplyr::row_number()) # Removed as.character()

message(paste("  Created sfnetwork object with", network_sfn %>% tidygraph::activate("nodes") %>% nrow(), "nodes and", network_sfn %>% tidygraph::activate("edges") %>% nrow(), "edges."))
message("  Nodes in the sfnetwork object now have 'name' (sequential ID) and 'x', 'y' coordinates.")
windows()
plot(network_sfn)

# --- 3. Manually Define Specific Cycles (Tangenziale Example) ---

message("Step 3: Defining specific cycles (COORDINATE-BASED S/T node selection and two shortest paths method)...")

# --- CRITICAL FIX: Convert sfnetwork to an igraph object for direct igraph functions ---

tempo_cord=st_coordinates(network_sfn)
graph_igraph_original <- as.igraph(network_sfn)

head(tempo_cord)
V(graph_igraph_original)$X <-tempo_cord[,1]
V(graph_igraph_original)$Y <-tempo_cord[,2]

# --- Identify and work with the largest connected component ---
message("\n  Checking graph connectivity and identifying largest component...")
if (!igraph::is_connected(graph_igraph_original)) {
  message("  Graph is disconnected. Extracting the largest connected component for pathfinding...")
  components_result <- igraph::components(graph_igraph_original)
  largest_comp_id <- which.max(components_result$csize)
  # Get the internal igraph vertex indices of nodes that belong to the largest component
  nodes_in_largest_comp_indices <- which(components_result$membership == largest_comp_id)
  # Create a subgraph containing only the largest component
  graph_igraph <- igraph::induced_subgraph(graph_igraph_original, vids = nodes_in_largest_comp_indices)
  message(paste0("  Original graph has ", igraph::vcount(graph_igraph_original), " nodes and ", components_result$no, " components."))
  message(paste0("  Largest component has ", igraph::vcount(graph_igraph), " nodes (", 
                 round(igraph::vcount(graph_igraph) / igraph::vcount(graph_igraph_original) * 100, 2), "% of original nodes)."))
} else {
  message("  Graph is connected. Proceeding with the full graph.")
  graph_igraph <- graph_igraph_original # Use the original if it's connected
}
coord_igraph <- cbind(V(graph_igraph)$X,V(graph_igraph)$Y)
is_connected(graph_igraph)
head(coord_igraph)
windows()
plot(graph_igraph,layout=coord_igraph,vertex.size=1,vertex.label=NA)

g_base <- graph_igraph
layout_coords <- coord_igraph
network1 <-'Bologna'
network1 <-'Barcelona'
####
 edge_graph <-get.edgelist(graph_igraph,name=FALSE)
head(edge_graph)
gg=graph_from_edgelist(edge_graph,directed=FALSE)
gg
head(V(gg))
head(E(gg))
windows()
plot(gg,vertex.size=1,vertex.label=NA)
g_base <- gg
set.seed(42)
layout_coords <- layout_with_fr(g_base)
positions <- tibble(
  name = as.character(1:vcount(g_base)),
  x = layout_coords[, 1],
  y = layout_coords[, 2]
)
windows()
plot(gg,layout=layout_coords,vertex.size=1,vertex.label=NA)
layout_coords <-coord_igraph
windows()
plot(gg,layout=layout_coords,vertex.size=1,vertex.label=NA)
nodes_sf <- st_as_sf(positions, coords = c("x", "y"))
# Prepare edge data for plotting the base network
edges <- igraph::as_data_frame(g_base, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  ) %>%
  left_join(positions %>% rename(x_from = x, y_from = y), by = c("from" = "name")) %>%
  left_join(positions %>% rename(x_to = x, y_to = y), by = c("to" = "name"))
nodes_sf
positions
#edges
head(edges)
layout_orig <-layout_coords



###############################################################################
# Set a seed for reproducibility
set.seed(42)

# ==============================================================================
# 1. NETWORK DEFINITION AND SPATIAL DATA
# ==============================================================================


# Correctly create positions from the igraph object
positions <- tibble(
  name = as.character(V(graph_igraph)),
  x = V(graph_igraph)$X,
  y = V(graph_igraph)$Y
)

# Create the sf object for nodes from the positions data frame
nodes_sf <- st_as_sf(positions, coords = c("x", "y"), crs = st_crs(network_sfn))

# Prepare edge data for plotting the base network
edges <- igraph::as_data_frame(g_base, what = "edges") %>%
  mutate(
    from = as.character(from),
    to = as.character(to)
  ) %>%
  left_join(positions %>% rename(x_from = x, y_from = y), by = c("from" = "name")) %>%
  left_join(positions %>% rename(x_to = x, y_to = y), by = c("to" = "name"))

# ==============================================================================
# 2. CORE FUNCTIONS
# ==============================================================================

# Function to calculate global network metrics
calculate_global_metrics <- function(g) {
  sp <- distances(g, mode = "all")
  finite_sp <- sp[is.finite(sp)]
  
  avg_path_length <- mean(finite_sp)
  efficiency <- 1 / avg_path_length
  avg_betweenness_centrality <- mean(betweenness(g))
  num_components <- components(g)$no
  
  tibble(
    avg_path_length = avg_path_length,
    network_efficiency = efficiency,
    avg_betweenness_centrality = avg_betweenness_centrality,
    num_components = num_components
  )
}

# Function to generate edges from an ordered list of nodes
generate_loop_edges <- function(loop_nodes) {
  num_nodes <- length(loop_nodes)
  from_nodes <- loop_nodes[1:(num_nodes - 1)]
  to_nodes <- loop_nodes[2:num_nodes]
  tibble(from = from_nodes, to = to_nodes)
}

# Function to compute winding number
compute_winding_number <- function(px, py, loop_coords_df) {
  angles <- 0
  n <- nrow(loop_coords_df)
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    angle <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    angles <- angles + angle
  }
  return(angles / (2 * pi))
}

# Function to compute distance-weighted generalized winding number chatgpt
compute_winding_number <- function(px, py, loop_coords_df) {
  gwn <- 0
  n <- nrow(loop_coords_df)
  
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    
    # Shift coordinates so (px, py) is origin
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    
    # Vector lengths
    r1 <- sqrt(x1^2 + y1^2)
    r2 <- sqrt(x2^2 + y2^2)
    
    # Angle between edges
    theta <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    
    # Distance-weighted contribution
    weight <- 1 / (r1 * r2)
    gwn <- gwn + weight * theta / (2 * pi)
  }
  
  return(gwn)
}

# Function for geometric influence index
compute_geometric_influence <- function(point_x, point_y, polygon) {
  centroid <- st_centroid(polygon)
  centroid_x <- st_coordinates(centroid)[1]
  centroid_y <- st_coordinates(centroid)[2]

  distance <- sqrt((point_x - centroid_x)^2 + (point_y - centroid_y)^2)
  
  if (distance == 0) {
    return(0)
  }
  
  return(1 / distance^2)
}

# Function to create an sf polygon from ordered loop nodes
create_loop_polygon <- function(loop_nodes, all_positions) {
  loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
    left_join(all_positions, by = "name")
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  st_polygon(list(poly_coords))
}

# Function to get an ordered list of nodes from a community's convex hull
get_ordered_loop_nodes_from_community <- function(community_nodes, all_positions) {
  
  community_coords_df <- all_positions %>%
    filter(name %in% as.character(community_nodes)) %>%
    select(name, x, y)
    
  community_sf <- st_as_sf(community_coords_df, coords = c("x", "y"))
  hull_polygon <- st_convex_hull(st_union(community_sf))
  
  hull_coords_raw <- st_coordinates(hull_polygon)[, 1:2]
  hull_coords_unique <- unique(hull_coords_raw)
  
  ordered_names <- c()
  for (i in 1:nrow(hull_coords_unique)) {
    match <- community_coords_df %>%
      filter(x == hull_coords_unique[i, 1] & y == hull_coords_unique[i, 2])
    
    if (nrow(match) > 0) {
      ordered_names <- c(ordered_names, match$name[1])
    }
  }
  
  return(ordered_names)
}

# Main function to run the full analysis for a given loop and network


#####################################################
# Main function to run the full analysis for a given loop and network

run_analysis <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  
  # Ensure nodes_sf has a CRS of NA
  # This is crucial because it was created with a CRS,
  # but the polygon is created without one.
  nodes_sf_no_crs <- st_set_crs(nodes_sf, NA)

  new_edges <- generate_loop_edges(loop_nodes)
  g_enhanced <- add_edges(g, as.vector(t(new_edges[, c("from", "to")])))
  
#####
# OLD LINE: This is where the error occurs
# loop_coords_df <- tibble(name = as.character(loop_nodes)) %>%
#   left_join(positions, by = "name")

# NEW, CORRECTED LINE:
loop_coords_df <- tibble(
    name = V(g_base)$name[loop_nodes],  # Get original IDs from g_base using integer indices
    x = V(g_base)$X[loop_nodes],
    y = V(g_base)$Y[loop_nodes]
)
#### 
  polygon <- create_loop_polygon(loop_nodes, positions)
  
  # Ensure the polygon also has a CRS of NA
  polygon_no_crs <- st_set_crs(polygon, NA)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  # Now these functions will work without crashing
  strictly_contained_result <- st_within(nodes_sf_no_crs, polygon_no_crs)
  on_boundary_result <- st_touches(nodes_sf_no_crs, polygon_no_crs)
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
  
  # The rest of the function remains the same
  winding_number_values <- sapply(1:nrow(positions), function(i) {
    compute_winding_number(positions$x[i], positions$y[i], loop_coords_df)
  })

  geometric_influence_values <- sapply(1:nrow(positions), function(i) {
    compute_geometric_influence(positions$x[i], positions$y[i], polygon)
  })
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Mean_Geometric_Influence = mean(geometric_influence_values),
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count
  )

  geometric_matrix_col <- tibble(influence = geometric_influence_values)
  colnames(geometric_matrix_col) <- paste0("Loop_", loop_name)
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  winding_matrix_col <- tibble(winding_number = winding_number_values)
  colnames(winding_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    global_metrics = calculate_global_metrics(g_enhanced),
    summary = summary_row,
    geometric_matrix = geometric_matrix_col,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    winding_matrix = winding_matrix_col,
    polygon = polygon
  )
}

# ==============================================================================
# 4. ANALYSIS EXECUTION
# ==============================================================================

# Choose one of the following scenarios for your loops.
# To use Community Detection, uncomment SCENARIO B.
#-------------------------------------------------------------------------------
# SCENARIO A: Use hard-coded loops
community_flag <-1
if(community_flag ==0){
loops <- list(
  Loop_A = c(1, 2, 3, 4, 1),
  Loop_B = c(5, 6, 11, 17, 5),
#  Loop_C = c(34, 13, 20, 19, 33, 34)
  Loop_C = c(49,15,28,18,27,48,62,49)
)
}
#-------------------------------------------------------------------------------

# SCENARIO B: Use dynamically generated loops from community detection
community_flag <- 1
if(community_flag == 1){
  # Run community detection on the final, connected graph
  communities <- cluster_louvain(graph_igraph)
  community_membership <- membership(communities)
  names(community_membership) <- as.character(V(graph_igraph)$name)
  
  # The rest of your community-based loop generation code should now work correctly
  dynamic_loops <- list()
  for (comm_id in unique(community_membership)) {
    community_nodes <- names(community_membership[community_membership == comm_id])
    if (length(community_nodes) >= 3) {
      ordered_nodes <- get_ordered_loop_nodes_from_community(community_nodes, positions)
      dynamic_loops[[paste0("Community_", comm_id)]] <- c(ordered_nodes, ordered_nodes[1])
    }
  }
  loops <- dynamic_loops
}
loops1 <-loops

# Translate loop node IDs to the new graph's internal indexing
translated_loops <- lapply(loops, function(loop_nodes) {
  # V(graph_igraph) returns the internal integer IDs of the nodes.
  # V(graph_igraph)$name returns the original OSM IDs that are attached to those new nodes.
  # We use match() to find the position (new integer ID) of each original ID in the new graph.
  match(loop_nodes, V(graph_igraph)$name)
})

# Reassign loops to the translated version
loops <- translated_loops

# The rest of your code remains the same.
# The 'for' loop will now use the correct integer IDs
# which correspond to the new graph's indexing.

#-------------------------------------------------------------------------------

all_summary <- tibble()
all_geometric <- tibble(node = positions$name)
all_contained <- tibble(node = positions$name)
all_covered <- tibble(node = positions$name)
all_winding <- tibble(node = positions$name)
global_metrics_df <- tibble()
all_polygons_list <- vector("list", length = length(loops))
all_polygons_list_names <- names(loops)

# Scenario 0: Basic network (BN)
cat("--- Scenario 0: Basic Network (BN) ---\n")
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_base) %>% mutate(Scenario = "0: BN"))

# Run analysis for each loop dynamically
for (i in 1:length(loops)) {
  loop_name <- names(loops)[i]
  loop_nodes <- loops[[i]]
  
  cat(paste0("--- Scenario: BN + ", loop_name, " ---\n"))
  results <- run_analysis(g_base, loop_name, loop_nodes, positions, nodes_sf)
  
  all_summary <- bind_rows(all_summary, results$summary)
  all_geometric <- bind_cols(all_geometric, results$geometric_matrix)
  all_contained <- bind_cols(all_contained, results$contained_matrix)
  all_covered <- bind_cols(all_covered, results$covered_matrix)
  all_winding <- bind_cols(all_winding, results$winding_matrix)
  global_metrics_df <- bind_rows(global_metrics_df, results$global_metrics %>% mutate(Scenario = paste0(loop_name, ": BN + ", loop_name)))
  all_polygons_list[[i]] <- results$polygon
  all_polygons_list_names[i] <- loop_name
  cat("\n")
}

names(all_polygons_list) <- all_polygons_list_names



###################
#######################
##########################
setwd("D:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")
library(osmdata)
library(sf)
library(ggplot2)
library(dplyr)
library(sfnetworks)
library(tidygraph)
library(igraph)
library(tidyverse)

# --- 1. Data Acquisition and Network Creation ---

message("Step 1: Acquiring OSM data for Bologna (expanded area)...")
bologna_wider_bbox <- c(11.15, 44.35, 11.55, 44.65)
names(bologna_wider_bbox) <- c("left", "bottom", "right", "top")

message("  Querying for major roads in Wider Bologna area...")
all_roads_osm <- bologna_wider_bbox %>%
  opq() %>%
  add_osm_feature(key = "highway", 
                  value = c("motorway", "trunk", "primary", "secondary", "tertiary",
                            "motorway_link", "trunk_link", "primary_link", "secondary_link", "tertiary_link")) %>%
  osmdata_sf()

all_network_lines <- all_roads_osm$osm_lines %>%
  filter(sf::st_is_valid(.))

message(paste("  Downloaded", nrow(all_network_lines), "road segments."))

# --- 2. Convert to sfnetwork and igraph ---

message("Step 2: Converting to sfnetwork and igraph objects...")


# NEW, CORRECTED CODE
# Correct: Let sfnetwork and igraph handle node IDs from OSM
network_sfn <- sfnetworks::as_sfnetwork(all_network_lines, directed = FALSE) %>%
tidygraph::activate("nodes") %>%
mutate(name = tryCatch(as.character(osm_id), error = function(e) as.character(row_number())))

# Get original OSM IDs and coordinates from sfnetwork
nodes_df_with_coords <- as_tibble(network_sfn, "nodes") %>%
st_coordinates() %>%
as_tibble() %>%
bind_cols(name = as.character(network_sfn %>% activate("nodes") %>% pull(name))) %>%
select(name, x = X, y = Y)
##
graph_igraph_original <- as.igraph(network_sfn)

V(graph_igraph_original)$X <- nodes_df_with_coords$x
V(graph_igraph_original)$Y <- nodes_df_with_coords$y
V(graph_igraph_original)$name <- as.character(nodes_df_with_coords$name)

# --- 3. Identify and work with the largest connected component ---
message("\n  Checking graph connectivity and identifying largest component...")
if (!igraph::is_connected(graph_igraph_original)) {
  message("  Graph is disconnected. Extracting the largest connected component...")
  components_result <- igraph::components(graph_igraph_original)
  largest_comp_id <- which.max(components_result$csize)
  nodes_in_largest_comp_indices <- which(components_result$membership == largest_comp_id)
  graph_igraph <- igraph::induced_subgraph(graph_igraph_original, vids = nodes_in_largest_comp_indices)
  message(paste0("  Original graph has ", igraph::vcount(graph_igraph_original), " nodes and ", components_result$no, " components."))
  message(paste0("  Largest component has ", igraph::vcount(graph_igraph), " nodes (", round(igraph::vcount(graph_igraph) / igraph::vcount(graph_igraph_original) * 100, 2), "% of original nodes)."))
} else {
  message("  Graph is connected. Proceeding with the full graph.")
  graph_igraph <- graph_igraph_original
}

g_base <- graph_igraph

# ==============================================================================
# 1. NETWORK DEFINITION AND SPATIAL DATA
# ==============================================================================

positions <- tibble(
  name = as.character(V(g_base)$name),
  x = V(g_base)$X,
  y = V(g_base)$Y
)

# Create the sf object for nodes
nodes_sf <- st_as_sf(positions, coords = c("x", "y"), crs = st_crs(network_sfn))

# ==============================================================================
# 2. CORE FUNCTIONS
# ==============================================================================

calculate_global_metrics <- function(g) {
  sp <- distances(g, mode = "all")
  finite_sp <- sp[is.finite(sp)]
  avg_path_length <- mean(finite_sp)
  efficiency <- 1 / avg_path_length
  avg_betweenness_centrality <- mean(betweenness(g))
  num_components <- components(g)$no
  tibble(
    avg_path_length = avg_path_length,
    network_efficiency = efficiency,
    avg_betweenness_centrality = avg_betweenness_centrality,
    num_components = num_components
  )
}

generate_loop_edges <- function(loop_nodes) {
  num_nodes <- length(loop_nodes)
  from_nodes <- loop_nodes[1:(num_nodes - 1)]
  to_nodes <- loop_nodes[2:num_nodes]
  tibble(from = from_nodes, to = to_nodes)
}

compute_winding_number <- function(px, py, loop_coords_df) {
  gwn <- 0
  n <- nrow(loop_coords_df)
  for (i in 1:n) {
    p1 <- loop_coords_df[i, ]
    p2 <- loop_coords_df[(i %% n) + 1, ]
    x1 <- p1$x - px
    y1 <- p1$y - py
    x2 <- p2$x - px
    y2 <- p2$y - py
    r1 <- sqrt(x1^2 + y1^2)
    r2 <- sqrt(x2^2 + y2^2)
    theta <- atan2(x1 * y2 - y1 * x2, x1 * x2 + y1 * y2)
    weight <- 1 / (r1 * r2)
    gwn <- gwn + weight * theta / (2 * pi)
  }
  return(gwn)
}

compute_geometric_influence <- function(point_x, point_y, polygon) {
  centroid <- st_centroid(polygon)
  centroid_x <- st_coordinates(centroid)[1]
  centroid_y <- st_coordinates(centroid)[2]
  distance <- sqrt((point_x - centroid_x)^2 + (point_y - centroid_y)^2)
  if (distance == 0) {
    return(0)
  }
  return(1 / distance^2)
}

get_ordered_loop_nodes_from_community <- function(community_nodes, all_positions) {
  community_coords_df <- all_positions %>%
    filter(name %in% as.character(community_nodes)) %>%
    select(name, x, y)
  community_sf <- st_as_sf(community_coords_df, coords = c("x", "y"))
  hull_polygon <- st_convex_hull(st_union(community_sf))
  hull_coords_raw <- st_coordinates(hull_polygon)[, 1:2]
  hull_coords_unique <- unique(hull_coords_raw)
  ordered_names <- c()
  for (i in 1:nrow(hull_coords_unique)) {
    match <- community_coords_df %>%
      filter(x == hull_coords_unique[i, 1] & y == hull_coords_unique[i, 2])
    if (nrow(match) > 0) {
      ordered_names <- c(ordered_names, match$name[1])
    }
  }
  return(ordered_names)
}

run_analysis <- function(g, loop_name, loop_nodes, positions, nodes_sf) {
  nodes_sf_no_crs <- st_set_crs(nodes_sf, NA)

  new_edges <- generate_loop_edges(loop_nodes)
  g_enhanced <- add_edges(g, as.vector(t(new_edges[, c("from", "to")])))
  
  # CORRECTED: Use the 'positions' data frame directly to get loop coordinates
  loop_coords_df <- positions %>%
      filter(name %in% V(g_base)$name[loop_nodes]) %>%
      select(x, y)
  
  poly_coords <- matrix(c(loop_coords_df$x, loop_coords_df$y), ncol = 2)
  poly_coords <- rbind(poly_coords, poly_coords[1, ])
  polygon <- st_polygon(list(poly_coords))
  
  polygon_no_crs <- st_set_crs(polygon, NA)
  
  area <- as.numeric(st_area(polygon))
  perimeter <- as.numeric(st_length(st_cast(polygon, "LINESTRING")))
  
  strictly_contained_result <- st_within(nodes_sf_no_crs, polygon_no_crs)
  on_boundary_result <- st_touches(nodes_sf_no_crs, polygon_no_crs)
  
  contained_count <- sum(lengths(strictly_contained_result) > 0)
  on_boundary_count <- sum(lengths(on_boundary_result) > 0)
  total_covered_count <- contained_count + on_boundary_count
  
  winding_number_values <- sapply(1:nrow(positions), function(i) {
    compute_winding_number(positions$x[i], positions$y[i], loop_coords_df)
  })

  geometric_influence_values <- sapply(1:nrow(positions), function(i) {
    compute_geometric_influence(positions$x[i], positions$y[i], polygon)
  })
  
  summary_row <- tibble(
    Loop_ID = loop_name,
    Area = area,
    Perimeter = perimeter,
    Mean_Geometric_Influence = mean(geometric_influence_values),
    Total_Contained_Nodes = contained_count,
    Total_Covered_Nodes = total_covered_count
  )

  geometric_matrix_col <- tibble(influence = geometric_influence_values)
  colnames(geometric_matrix_col) <- paste0("Loop_", loop_name)
  
  contained_matrix_col <- tibble(contained = as.numeric(lengths(strictly_contained_result) > 0))
  colnames(contained_matrix_col) <- paste0("Loop_", loop_name)
  
  covered_matrix_col <- tibble(covered = as.numeric(lengths(strictly_contained_result) > 0 | lengths(on_boundary_result) > 0))
  colnames(covered_matrix_col) <- paste0("Loop_", loop_name)
  
  winding_matrix_col <- tibble(winding_number = winding_number_values)
  colnames(winding_matrix_col) <- paste0("Loop_", loop_name)
  
  list(
    global_metrics = calculate_global_metrics(g_enhanced),
    summary = summary_row,
    geometric_matrix = geometric_matrix_col,
    contained_matrix = contained_matrix_col,
    covered_matrix = covered_matrix_col,
    winding_matrix = winding_matrix_col,
    polygon = polygon
  )
}

# ==============================================================================
# 4. ANALYSIS EXECUTION
# ==============================================================================

# Choose one of the following scenarios for your loops.
community_flag <- 1
if(community_flag == 0){
  loops <- list(
    Loop_A = c("3178", "2967", "4545", "1433", "2099", "2085", "550", "549", "5066", "3178"),
    Loop_B = c("2325", "2159", "3225", "1102", "1549", "1535", "477", "476", "3603", "2325")
  )
} else {
  communities <- cluster_louvain(g_base)
  community_membership <- membership(communities)
  names(community_membership) <- as.character(V(g_base)$name)
  
  dynamic_loops <- list()
  for (comm_id in unique(community_membership)) {
    community_nodes <- names(community_membership[community_membership == comm_id])
    if (length(community_nodes) >= 3) {
      ordered_nodes <- get_ordered_loop_nodes_from_community(community_nodes, positions)
      dynamic_loops[[paste0("Community_", comm_id)]] <- c(ordered_nodes, ordered_nodes[1])
    }
  }
  loops <- dynamic_loops
}

translated_loops <- lapply(loops, function(loop_nodes) {
  match(loop_nodes, V(g_base)$name)
})

loops <- translated_loops

# Filter out loops that contain NAs (nodes not in the component)
loops <- loops[!sapply(loops, anyNA)]

# Remove empty loop lists
loops <- loops[sapply(loops, function(x) length(x) > 0)]

all_summary <- tibble()
all_geometric <- tibble(node = positions$name)
all_contained <- tibble(node = positions$name)
all_covered <- tibble(node = positions$name)
all_winding <- tibble(node = positions$name)
global_metrics_df <- tibble()
all_polygons_list <- vector("list", length = length(loops))
all_polygons_list_names <- names(loops)

cat("--- Scenario 0: Basic Network (BN) ---\n")
global_metrics_df <- bind_rows(global_metrics_df, 
                               calculate_global_metrics(g_base) %>% mutate(Scenario = "0: BN"))

for (i in 1:length(loops)) {
  loop_name <- names(loops)[i]
  loop_nodes <- loops[[i]]
  
  cat(paste0("--- Scenario: BN + ", loop_name, " ---\n"))
  results <- run_analysis(g_base, loop_name, loop_nodes, positions, nodes_sf)
  
  all_summary <- bind_rows(all_summary, results$summary)
  all_geometric <- bind_cols(all_geometric, results$geometric_matrix)
  all_contained <- bind_cols(all_contained, results$contained_matrix)
  all_covered <- bind_cols(all_covered, results$covered_matrix)
  all_winding <- bind_cols(all_winding, results$winding_matrix)
  global_metrics_df <- bind_rows(global_metrics_df, results$global_metrics %>% mutate(Scenario = paste0(loop_name, ": BN + ", loop_name)))
  all_polygons_list[[i]] <- results$polygon
  all_polygons_list_names[i] <- loop_name
  cat("\n")
}

names(all_polygons_list) <- all_polygons_list_names




>   new_edges_df <- new_edges_df %>%
+     mutate(geometry = st_sfc(rep(NA, n()), crs = st_crs(g_sfn))) %>%
+     relocate(geometry, .after = to)
Error in `mutate()`:
ℹ In argument: `geometry = st_sfc(rep(NA, n()), crs = st_crs(g_sfn))`.
Caused by error:
! object(s) should be of class 'sfg'


####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################










####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################
####################################################################


library(igraph)
library(ggplot2)
library(dplyr)
library(tidyr)
library(gridExtra)
#library(clue)  # for solve_LSAP
#library(infotheo)
#library(mclust)
#library(vegan)
#library(scales)
setwd("D:/AC_2025/docucr_25/HASSE diagram/Moronta/anticlu")

network1 <-'base'
network1 <- 'ieee39 line.txt'
network1 <- 'ieee57 line.txt'
network1 <- 'ieee39 line.txt'
network1 <- 'it_bas.txt'
#network1 <- 'ieee118.txt'
#network1 <- 'power494.txt'
#network1 <- 'Germany.txt'
#network1 <- 'mafia1.txt'
#network1 <- 'jazz.txt'
#network1 <- 'Spain.txt'
#network1 <- 'it_bas.txt'
#network1 <- 'SENTRONCAL_mod2.txt'
#network1 <- 'euroroad.txt'
#network1 <- 'usaairpotx.txt'
network1 <- 'ieee57 line.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'it_bas.txt'
network1 <- 'ieee118.txt'
network1 <- 'Germany.txt'
#network1 <- "Crime_Gcc1.txt"
network1 <- "barcelona.txt"
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'Germany.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- "barcelona.txt"
network1 <- 'it_bas.txt'
network1 <- 'SENTRONCAL_mod2.txt'
network1 <- 'Germany.txt'
network1 <- 'mafia1.txt'
network1 <- 'it_bas.txt'
network1 <- 'SENTRONCAL_mod2.txt'


ff<-scan(network1, what=list(0,0,0))

# Define network data
f1<-ff[[1]] #Nodo desde
f2<-ff[[2]] #Nodo hast
f3<-ff[[3]] #reactance
g <-graph(t(cbind(f1,f2)), directed=FALSE)
#if(network1 == 'mafia1.txt'){
g <- simplify(delete_vertices(g, which(components(g)$membership!=which.max(components(g)$csize))))
#}

# Load graph and layout
##########
g <- make_graph("Zachary")
network1 <- 'Zachary1K1'

V(g)$name =1:vcount(g)

set.seed(42)
layout_orig <- layout_with_kk(g) #layout_with_fr(g)

#g <- make_graph("Zachary")
#original_coords <- layout_orig #layout_with_kk(g)
# Normalization function
normalize_coords <- function(coords) {
  x_range <- range(coords[, 1])
  y_range <- range(coords[, 2])
  coords[, 1] <- (coords[, 1] - x_range[1]) / (x_range[2] - x_range[1])
  coords[, 2] <- (coords[, 2] - y_range[1]) / (y_range[2] - y_range[1])
  return(coords)
}

windows()

set.seed(42)
plot(g,layout=layout_orig,main="base",vertex.size=1)

#layout_norm <- layout_orig / max(layout_orig)
layout_norm <- normalize_coords(layout_orig)#(layout_orig- min(layout_orig))/ (max(layout_orig)-min(layout_orig))
original_coords <- layout_norm  
windows()
coords_orig <-layout_norm 
plot(g,layout=layout_orig,main="base norm",vertex.size=1)

jpeg(filename= paste(network1,"base.jpeg"), units = "px", res= 600, height= 3000, width= 3000) 

 plot_network(G1, tempo1[nrow(tempo1),], layout_G1, paste("G1 at end",keval))
if(jpeg==1)dev.off()

center_orig <- colMeans(layout_norm)
dist_center_orig <- sqrt(rowSums((layout_norm - center_orig)^2))

noise= 0.1
angle = pi / 3
 scale = 1.3
shift = 1
opcion = 1 # 1 considera arnold+shift

# Compute topological features
features <- list(
  Degree = degree(g),
  Betweenness = betweenness(g),
  Closeness = closeness(g),
  Eigenvector = eigen_centrality(g)$vector
)

# Arnold Map functions
arnold_map <- function(x, y, mod = 1) {
  x_new <- (x + y) %% mod
  y_new <- (x + 2 * y) %% mod
  return(c(x_new, y_new))
}

apply_arnold <- function(coords, iterations = 1, mod = 1) {
  coords_out <- coords
  for (i in 1:iterations) {
    coords_out <- t(apply(coords_out, 1, function(p) arnold_map(p[1], p[2], mod)))
  }
  return(coords_out)
}

# Other distortions
apply_jitter <- function(coords, noise = noise) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}


apply_jitter <- function(coords) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}

apply_affine <- function(coords, angle =angle, scale = scale) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

apply_affine <- function(coords) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

apply_circular_swap <- function(coords) {
  center <- colMeans(coords)
  radius <- sqrt(rowSums((coords - center)^2))
  angle <- atan2(coords[,2] - center[2], coords[,1] - center[1])
  angle <- sample(angle)
  cbind(radius * cos(angle), radius * sin(angle)) + center
}

apply_shift <- function(coords ) {
  n <- nrow(coords)
  coords[((1:n - shift - 1) %% n) + 1, ]
}
############################################################### non reversibility

obfuscate_jitter <- function(layout, scale = 0.05) {
  layout + cbind(
    rnorm(nrow(layout), mean = 0, sd = scale),
    rnorm(nrow(layout), mean = 0, sd = scale)
  )
}

obfuscate_permutation <- function(layout) {
  perm <- sample(1:nrow(layout))
  layout[perm, ]
}
rlaplace <- function(n, mu = 0, b = 1) {
  u <- runif(n, -0.5, 0.5)
  mu - b * sign(u) * log(1 - 2 * abs(u))
}

obfuscate_dp <- function(layout, scale = 0.05) {
  layout + cbind(
    rlaplace(nrow(layout), b = scale),
    rlaplace(nrow(layout), b = scale)
  )
}

obfuscate_random <- function(layout) {
  cbind(runif(nrow(layout), min = min(layout[,1]), max = max(layout[,1])),
        runif(nrow(layout), min = min(layout[,2]), max = max(layout[,2])))
}

obfuscate_hybrid <- function(layout, scale = 0.05) {
  perm <- sample(1:nrow(layout))
  permuted <- layout[perm, ]
  permuted + cbind(
    rnorm(nrow(layout), mean = 0, sd = scale),
    rnorm(nrow(layout), mean = 0, sd = scale)
  )
}

# Suppose layout_original is an N x 2 matrix of node coordinates
set.seed(123)
#layout_jitter <- obfuscate_jitter(layout_original, scale = 0.1)

layout_perm   <- obfuscate_permutation(layout_norm)
layout_dp     <- obfuscate_dp(layout_norm, scale = 0.1)
layout_rand   <- obfuscate_random(layout_norm)
layout_hybrid <- obfuscate_hybrid(layout_norm, scale = 0.1)


##################################################################################
# Compute robustness
compute_robustness <- function(method_name, layout_dist, feature_list, dist_center_orig, iteration = NA) {
  center_dist <- colMeans(layout_dist)
  dist_center_dist <- sqrt(rowSums((layout_dist - center_dist)^2))

  df <- lapply(names(feature_list), function(name) {
    x <- feature_list[[name]]
    cor_orig <- cor(x, -dist_center_orig)
    cor_dist <- cor(x, -dist_center_dist)
    robustness <- 1 - abs(cor_orig - cor_dist)
    data.frame(
      Method = method_name,
      Iteration = iteration,
      Feature = name,
      Cor_Original = round(cor_orig, 4),
      Cor_Distorted = round(cor_dist, 4),
      Robustness = round(robustness, 4)
    )
  })

  do.call(rbind, df)
}

# Parameters
n_iterations <- 10#5
chosen_iter <- 5#3


# Distorted layouts for fixed methods
distorted_layouts <- list(
  Jitter = apply_jitter(layout_norm),
  Affine = apply_affine(layout_norm),
  Circular = apply_circular_swap(layout_norm)
)

distorted_layouts <- list(
  Jitter = apply_jitter(layout_norm),
  Affine = apply_affine(layout_norm),
  Circular = apply_circular_swap(layout_norm),
  Shift = apply_shift(layout_norm)  # You can change shift value here
)


Arnold <- apply_arnold(layout_norm, iterations = chosen_iter)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift
)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift,
Perm = layout_perm  ,
Dp =layout_dp    ,
Rand =layout_rand   ,
Hybrid = layout_hybrid 

)

distorted_layouts <- list(
  Jitter = apply_jitter(layout_norm),
  Affine = apply_affine(layout_norm),
  Circular = apply_circular_swap(layout_norm),
  Shift = apply_shift(layout_norm),  # You can change shift value here
Perm =layout_perm  ,
Dp =layout_dp    ,
Rand =layout_rand   ,
Hybrid = layout_hybrid 

)

layout_list_base <- layout_list 


# Rename Arnold to include iteration number
#names(layout_list)[2] <- paste0("Arnold-", #chosen_iter)

res_mean <-numeric(n_iterations )

# Store all results
robustness_results <- list()

# Arnold iterations
for (i in 1:n_iterations) {
  layout_arnold <- apply_arnold(layout_norm, iterations = i)
  res <- compute_robustness("Arnold", layout_arnold, features, dist_center_orig, iteration = i)
  robustness_results[[paste0("Arnold_", i)]] <- res
res_mean [i] <- mean(abs(res$Robustness))

}
res_mean
chosen_iter <- which.min(res_mean)
chosen_iter

# Other methods (without iterations)
for (method in names(distorted_layouts)) {
  res <- compute_robustness(method, distorted_layouts[[method]], features, dist_center_orig)
  robustness_results[[method]] <- res
}

# Combine all results
robustness_df <- do.call(rbind, robustness_results)

# === 1. Plot: Robustness across Arnold iterations ===
windows()
arnold_it <- ggplot(filter(robustness_df, Method == "Arnold"), aes(x = Iteration, y = Robustness, color = Feature)) +
  geom_line(size = 1.1) +
  geom_point(size = 2.5) +
  ylim(-0.5, 1) +
  labs(title = paste(network1,"Robustness of Features Across Arnold Iterations"),
       x = "Arnold Iteration", y = "Robustness Score") +
  theme_minimal()
 print(arnold_it)

###############
ggsave(
  filename = paste(network1,"arnold.pdf"),
  plot = arnold_it,
  device = "pdf",
  width = 8,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 6,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"arnold.jpeg"),
  plot = arnold_it,
  device = "jpeg",
  width = 8,  # Adjust width
  height = 6,  # Adjust height
  dpi = 600,
  quality = 90 # Optional: JPEG quality
)
#################
# === 2. Plot: Barplot of all methods (select 1 Arnold iteration) ===
robust_subset <- filter(robustness_df, is.na(Iteration) | Iteration == chosen_iter)
robust_subset$Method <- ifelse(is.na(robust_subset$Iteration),
                               robust_subset$Method,
                               paste0("Arnold-", robust_subset$Iteration))
windows()
barplot <- ggplot(robust_subset, aes(x = Method, y = Robustness, fill = Feature)) +
  geom_bar(stat = "identity", position = "dodge") +
  ylim(-0.5, 1) +
  labs(title = paste(network1,"Robustness by Method (Arnold Iteration =", chosen_iter, ")"),
       y = "Robustness Score", x = "Distortion Method") +
  theme_minimal()
print(barplot)
###############
ggsave(
  filename = paste(network1,"barplot.pdf"),
  plot = barplot,
  device = "pdf",
  width = 8,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 6,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"barplot"), "\n")

ggsave(
  filename = paste(network1,"barplot.jpeg"),
  plot = barplot,
  device = "jpeg",
  width = 8,  # Adjust width
  height = 6,  # Adjust height
  dpi = 600,
  quality = 90 # Optional: JPEG quality
)

#################
# === 3. Plot: Network layouts with edges ===
plot_network_layout <- function(layout_matrix, title) {
  layout_df <- as.data.frame(layout_matrix)
  colnames(layout_df) <- c("x", "y")
  layout_df$node <- as.character(V(g))

  # Edges
  edges <- get.edgelist(g)
  edge_df <- data.frame(
    x = layout_matrix[edges[,1], 1],
    y = layout_matrix[edges[,1], 2],
    xend = layout_matrix[edges[,2], 1],
    yend = layout_matrix[edges[,2], 2]
  )
#windows()
  ggplot() +
    geom_segment(data = edge_df, aes(x = x, y = y, xend = xend, yend = yend), color = "gray80") +
    geom_point(data = layout_df, aes(x = x, y = y), color = "steelblue", size = 3) +
    geom_text(data = layout_df, aes(x = x, y = y, label = node), size = 2.8, vjust = -0.7) +
    theme_void() +
    labs(title = title)
}

# Layouts to display


Arnold <- apply_arnold(layout_norm, iterations = chosen_iter)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular
)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift
)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift,
Perm = layout_perm  ,
Dp =layout_dp    ,
Rand =layout_rand   ,
Hybrid = layout_hybrid 

)

# Rename Arnold to include iteration number
names(layout_list)[2] <- paste0("Arnold-", chosen_iter)

layout_plots <- lapply(names(layout_list), function(name) {
  plot_network_layout(layout_list[[name]], name)
})

windows()
combined_plot<-do.call(grid.arrange, c(layout_plots, ncol = 2))
#print(combined_plot)

ggsave(
  filename = paste(network1,"combined.pdf"),
  plot = combined_plot,
  device = "pdf",
  width = 12,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 9,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"combined.jpeg"),
  plot = combined_plot,
  device = "jpeg",
  width = 12,  # Adjust width
  height = 9,  # Adjust height
  dpi = 300,
  quality = 90 # Optional: JPEG quality
)
cat("Saved combined JPEG:", paste(network1,"combined.jpeg"), "\n")

# === 4. Print full robustness table ===
print(robustness_df)

######################################## parte nueva con variaciones de parametros

# Arnold Map functions (as provided)
arnold_map <- function(x, y, mod = 1) {
  x_new <- (x + y) %% mod
  y_new <- (x + 2 * y) %% mod
  return(c(x_new, y_new))
}

apply_arnold <- function(coords, iterations = 1, mod = 1) {
  coords_out <- coords
  for (i in 1:iterations) {
    coords_out <- t(apply(coords_out, 1, function(p) arnold_map(p[1], p[2], mod)))
  }
  return(coords_out)
}

# Other distortion functions with parameters
apply_jitter <- function(coords, noise) {
  coords + matrix(runif(length(coords), -noise, noise), ncol = 2)
}

apply_affine <- function(coords, angle, scale) {
  rot_matrix <- matrix(c(cos(angle), -sin(angle), sin(angle), cos(angle)), 2, 2)
  t(apply(coords, 1, function(p) scale * (rot_matrix %*% p)))
}

apply_circular_swap <- function(coords) {
  center <- colMeans(coords)
  radius <- sqrt(rowSums((coords - center)^2))
  angle <- atan2(coords[,2] - center[2], coords[,1] - center[1])
  angle <- sample(angle)
  cbind(radius * cos(angle), radius * sin(angle)) + center
}

apply_shift <- function(coords) {
  n <- nrow(coords)
  coords[((1:n - 1 - floor(n/2)) %% n) + 1, ]
}

# Revised function with a shift parameter
apply_shift <- function(coords, shift = 1) {
  n <- nrow(coords)
  coords[((1:n - shift - 1) %% n) + 1, ]
}

# Normalization function
normalize_coords <- function(coords) {
  x_range <- range(coords[, 1])
  y_range <- range(coords[, 2])
  coords[, 1] <- (coords[, 1] - x_range[1]) / (x_range[2] - x_range[1])
  coords[, 2] <- (coords[, 2] - y_range[1]) / (y_range[2] - y_range[1])
  return(coords)
}

# User's compute_robustness function (as provided)
compute_robustness <- function(method_name, layout_dist, feature_list, dist_center_orig, iteration = NA) {
  center_dist <- colMeans(layout_dist)
  dist_center_dist <- sqrt(rowSums((layout_dist - center_dist)^2))

  df <- lapply(names(feature_list), function(name) {
    x <- feature_list[[name]]
    cor_orig <- cor(x, -dist_center_orig)
    cor_dist <- cor(x, -dist_center_dist)
    robustness <- 1 - abs(cor_orig - cor_dist)
    data.frame(
      Method = method_name,
      Iteration = iteration,
      Feature = name,
      Cor_Original = round(cor_orig, 4),
      Cor_Distorted = round(cor_dist, 4),
      Robustness = round(robustness, 4)
    )
  })
  do.call(rbind, df)
}

# Set up parameter ranges and lists
n_iterations <- 20 # Arnold
noise_values <- seq(0.1, 0.5, by = 0.1)#noise#seq(0.1, 1, by = 0.1) # Jitter
angle_values <- seq(0, 2*pi, length.out = 10) #angle#seq(0, 2*pi, length.out = 10) # Affine Angle
scale_values <- seq(0.5, 2, by = 0.25) # scale#seq(0.5, 2, by = 0.25) # Affine Scale
shift_values <- seq(1, 5, by = 1) #shift#seq(1, 5, by = 1) # shift Scale
dp_values <- seq(0.03, 0.06, by = 0.01) #dp
# Storage for the best layouts and parameters
best_params <- list()
distorted_layouts <- list()

# 1. Find best Arnold iteration
res_mean_arnold <- numeric(n_iterations)
for (i in 1:n_iterations) {
  layout_arnold <- normalize_coords(apply_arnold(layout_norm, iterations = i))
  # Temporarily compute robustness to find best iteration
  res <- compute_robustness("Arnold", layout_arnold, features, dist_center_orig, iteration = i)
  res_mean_arnold[i] <- mean(abs(res$Robustness))
}
chosen_iter <- which.min(res_mean_arnold)
best_params$Arnold <- chosen_iter
distorted_layouts$Arnold <- normalize_coords(apply_arnold(layout_norm, iterations = chosen_iter))


# 2. Find best Jitter noise
res_mean_jitter <- numeric(length(noise_values))
for (i in 1:length(noise_values)) {
  layout_jitter <- normalize_coords(apply_jitter(layout_norm, noise = noise_values[i]))
  # Temporarily compute robustness to find best noise
  res <- compute_robustness("Jitter", layout_jitter, features, dist_center_orig)
  res_mean_jitter[i] <- mean(abs(res$Robustness))
}
chosen_noise <- noise_values[which.min(res_mean_jitter)]
best_params$Jitter <- chosen_noise
distorted_layouts$Jitter <- normalize_coords(apply_jitter(layout_norm, noise = chosen_noise))


# 3. Find best Affine angle and scale
res_mean_affine <- matrix(NA, nrow = length(angle_values), ncol = length(scale_values))
for (i in 1:length(angle_values)) {
  for (j in 1:length(scale_values)) {
    layout_affine <- normalize_coords(apply_affine(layout_norm, angle = angle_values[i], scale = scale_values[j]))
    # Temporarily compute robustness to find best parameters
    res <- compute_robustness("Affine", layout_affine, features, dist_center_orig)
    res_mean_affine[i, j] <- mean(abs(res$Robustness))
  }
}
chosen_indices_affine <- which(res_mean_affine == min(res_mean_affine, na.rm = TRUE), arr.ind = TRUE)
chosen_angle <- angle_values[chosen_indices_affine[1,1]]
chosen_scale <- scale_values[chosen_indices_affine[1,2]]
best_params$Affine <- list(angle = chosen_angle, scale = chosen_scale)
distorted_layouts$Affine <- normalize_coords(apply_affine(layout_norm, angle = chosen_angle, scale = chosen_scale))


# 4. Process methods without parameter variations
distorted_layouts$Circular <- normalize_coords(apply_circular_swap(layout_norm))

distorted_layouts$Dp =layout_dp 
distorted_layouts$Rand =layout_rand  
distorted_layouts$Perm =layout_perm

# 1. Find best shift iteration
res_mean_shift <- numeric(length(shift_values))
for (i in 1:length(shift_values)) {
  layout_shift <- normalize_coords(apply_shift(layout_norm, shift = shift_values[i]))
  # Temporarily compute robustness to find best noise
  res <- compute_robustness("Shift", layout_shift, features, dist_center_orig)
  res_mean_shift[i] <- mean(abs(res$Robustness))
}
chosen_shift <- shift_values[which.min(res_mean_shift)]
best_params$Shift <- chosen_shift
distorted_layouts$Shift <- normalize_coords(apply_shift(layout_norm, shift= chosen_shift))
###########################################

set.seed(123)

# x. Find best dpiteration
res_mean_dp <- numeric(length(dp_values))
for (i in 1:length(dp_values)) {
  layout_dp <- normalize_coords(obfuscate_dp(layout_norm, scale = dp_values[i]))
  # Temporarily compute robustness to find best noise

  res <- compute_robustness("Dp", layout_dp, features, dist_center_orig)
  res_mean_dp[i] <- mean(abs(res$Robustness))
}
chosen_dp <- dp_values[which.min(res_mean_dp)]
best_params$Dp <- chosen_dp
distorted_layouts$Dp <- normalize_coords(obfuscate_hybrid(layout_norm, scale = chosen_dp))
#

# x. Find hybrid dpiteration
res_mean_hybrid <- numeric(length(dp_values))
for (i in 1:length(dp_values)) {
  layout_hybrid <- normalize_coords(obfuscate_hybrid(layout_norm, scale = dp_values[i]))
  # Temporarily compute robustness to find best noise

  res <- compute_robustness("Hybrid", layout_hybrid, features, dist_center_orig)
  res_mean_hybrid[i] <- mean(abs(res$Robustness))
}
chosen_hybrid <- dp_values[which.min(res_mean_hybrid)]
best_params$hybrid <- chosen_hybrid
distorted_layouts$Hybrid <- normalize_coords(obfuscate_hybrid(layout_norm, scale = chosen_hybrid))
#
best_params
names(distorted_layouts)

###########################################
# 5. Final Calculation: Use the original `compute_robustness` function on the best layouts
# and combine all results into a single data frame
robustness_results <- list()
for (method in names(distorted_layouts)) {
  layout_to_evaluate <- distorted_layouts[[method]]
  res <- compute_robustness(method, layout_to_evaluate, features, dist_center_orig, iteration = best_params$Arnold)
  robustness_results[[method]] <- res
}

robustness_df <- do.call(rbind, robustness_results)

# Print best parameters for review
print("Best parameters found:")
print(best_params)

chosen_iter <-best_params$Arnold

 noise <-best_params$Jitter

 angle<-best_params$Affine$angle
 scale <-best_params$Affine$scale
 shift <-best_params$Shift
dpx <- best_params$Dp
hyx <-best_params$Hybrid

windows()
robust_subset<-robustness_df
barplot1 <- ggplot(robust_subset, aes(x = Method, y = Robustness, fill = Feature)) +
  geom_bar(stat = "identity", position = "dodge") +
  ylim(-0.5, 1) +
  labs(title = paste(network1,"Robustness by Method (Arnold Iteration =", chosen_iter, ")"),
       y = "Robustness Score", x = "Distortion Method") +
  theme_minimal()
print(barplot1)
###############
ggsave(
  filename = paste(network1,"optbarplot1.pdf"),
  plot = barplot1,
  device = "pdf",
  width = 8,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 6,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"barplot"), "\n")

ggsave(
  filename = paste(network1,"optbarplot1.jpeg"),
  plot = barplot1,
  device = "jpeg",
  width = 8,  # Adjust width
  height = 6,  # Adjust height
  dpi = 600,
  quality = 90 # Optional: JPEG quality
)



# === 3. Plot: Network layouts with edges ===
plot_network_layout <- function(layout_matrix, title) {
  # Convert the layout to a data frame with proper column names
  layout_df <- as.data.frame(layout_matrix)
  colnames(layout_df) <- c("x", "y")
  
  # Ensure the node IDs are correctly associated with the layout coordinates
  # by assuming the original graph's vertex names are '1', '2', '3', etc.
  layout_df$node <- V(g)$name
  
  # Get the edge list using the modern function
  edges <- as_edgelist(g)
  
  # Create a data frame for edges by matching node IDs to coordinates
  edge_df <- data.frame(
    x = layout_df$x[match(edges[,1], layout_df$node)],
    y = layout_df$y[match(edges[,1], layout_df$node)],
    xend = layout_df$x[match(edges[,2], layout_df$node)],
    yend = layout_df$y[match(edges[,2], layout_df$node)]
  )
  
  ggplot() +
    geom_segment(data = edge_df, aes(x = x, y = y, xend = xend, yend = yend), color = "gray80") +
    geom_point(data = layout_df, aes(x = x, y = y), color = "steelblue", size = 3) +
    geom_text(data = layout_df, aes(x = x, y = y, label = node), size = 2.8, vjust = -0.7) +
    theme_void() +
    labs(title = paste("*",title))
}

# The layouts list remains the same
layout_list <- list(
  Original = layout_norm,
  Arnold = distorted_layouts$Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
  Shift = distorted_layouts$Shift
)

layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift,
Perm = distorted_layouts$Perm,
Dp =distorted_layouts$Dp,
Rand =distorted_layouts$Rand,
Hybrid = distorted_layouts$Hybrid 

)

# Plotting loop
layout_plots <- lapply(names(layout_list), function(name) {
  plot_network_layout(layout_list[[name]], name)
})

# Example of how to display the plots
 library(gridExtra)
windows()
# grid.arrange(grobs = layout_plots, ncol = 2)
combined_plot1<-do.call(grid.arrange, c(layout_plots, ncol = 2))
#print(combined_plot1)
ggsave(
  filename = paste(network1,"combined1.pdf"),
  plot = combined_plot1,
  device = "pdf",
  width = 12,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 9,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"combined1.jpeg"),
  plot = combined_plot1,
  device = "jpeg",
  width = 12,  # Adjust width
  height = 9,  # Adjust height
  dpi = 300,
  quality = 90 # Optional: JPEG quality
)
cat("Saved combined JPEG:", paste(network1,"combined.jpeg"), "\n")
##################################################################################### inclusion anold+shift

# New combined method: Arnold-Shift
# First, apply the best-performing Arnold iteration
layout_arnold <- normalize_coords(apply_arnold(layout_norm, iterations = best_params$Arnold))

# Second, apply the Shift permutation to the result
layout_arnold_shifted <- apply_shift(layout_arnold, shift= shift)

# You would then add this new layout to your list for evaluation
distorted_layouts$ArnoldShift <- layout_arnold_shifted

layout_list_orig <-layout_list

# The layouts list remains the same
layout_list <- list(
  Original = layout_norm,
  Arnold = distorted_layouts$Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
  Shift = distorted_layouts$Shift,
  ArnoldShift = distorted_layouts$ArnoldShift )
layout_list <- list(
  Original = layout_norm,
  Arnold = Arnold,
  Jitter = distorted_layouts$Jitter,
  Affine = distorted_layouts$Affine,
  Circular = distorted_layouts$Circular,
Shift = distorted_layouts$Shift,
Perm = distorted_layouts$Perm,
Dp =distorted_layouts$Dp,
Rand =distorted_layouts$Rand,
Hybrid = distorted_layouts$Hybrid ,
  ArnoldShift = distorted_layouts$ArnoldShift )


# Plotting loop
layout_plots <- lapply(names(layout_list), function(name) {
  plot_network_layout(layout_list[[name]], name)
})

# Example of how to display the plots
 library(gridExtra)
windows()
# grid.arrange(grobs = layout_plots, ncol = 2)
combined_plot1x<-do.call(grid.arrange, c(layout_plots, ncol = 2))
#print(combined_plot1)
ggsave(
  filename = paste(network1,"combined1x.pdf"),
  plot = combined_plot1x,
  device = "pdf",
  width = 12,  # Adjust width for multiple plots (e.g., 2 columns, so wider)
  height = 9,  # Adjust height as needed
  dpi = 300
)
cat("Saved combined PDF:", paste(network1,"combined"), "\n")

ggsave(
  filename = paste(network1,"combined1x.jpeg"),
  plot = combined_plot1x,
  device = "jpeg",
  width = 12,  # Adjust width
  height = 9,  # Adjust height
  dpi = 300,
  quality = 90 # Optional: JPEG quality
)
cat("Saved combined JPEG:", paste(network1,"combined.jpeg"), "\n")
print(best_params)


########################################################## 
########## ojo seleccionar pdf o jpeg
kuno=0
if(kuno==1){
# Loop through each layout and save it as a separate PDF
lapply(names(layout_list), function(name) {
  # Generate the plot object
  p_plot <- plot_network_layout(layout_list[[name]], name)

  # Construct the filename
  file_name <- paste(network1,tolower(name),".pdf")#file.path(output_dir, paste0(gsub(" ", "_", tolower(name)), "_layout.pdf")) # Cleans name for filename
  file_name <- paste(network1,tolower(name),".jpeg")#file.path(output_dir, paste0(gsub(" ", "_", tolower(name)), "_layout.pdf")) # Cleans name for filename

  # Save the plot to PDF with high resolution
  ggsave(
    filename = file_name,
    plot = p_plot,
#    device = "pdf",
    device = "jpeg",
    width = 8,  # Adjust width as needed (in inches)
    height = 6, # Adjust height as needed (in inches)
#    dpi = 300   # High resolution (dots per inch)
    dpi = 600,   # High resolution (dots per inch)
quality=90
  )

  cat("Saved:", file_name, "\n") # Print message for confirmation
})

}

############################################################# aqui hacia abajo
vertex_names <- if (!is.null(V(g)$name)) {
  V(g)$name
} else {
  as.character(1:vcount(g))
}

nodes_df <- data.frame(
  name = vertex_names,
  x = original_coords[, 1],
  y = original_coords[, 2]
)

# 4. Calculate topological importance (The Adversary's "Ground Truth")
nodes_df <- nodes_df %>%
  mutate(
    degree = degree(g),
    betweenness = betweenness(g, normalized = TRUE),
    eigenvector = evcent(g)$vector
  ) %>%
  mutate(
    rank_degree = rank(-degree, ties.method = "min"),
    rank_betweenness = rank(-betweenness, ties.method = "min"),
    rank_eigenvector = rank(-eigenvector, ties.method = "min")
  )

# 5. Measure Visual Prominence (The Adversary's "Inferred Importance")
calculate_visual_prominence <- function(coords, dist_threshold = 2.5) {
  num_nodes <- nrow(coords)
  prominence <- numeric(num_nodes)
  for (i in 1:num_nodes) {
    distances <- apply(coords, 1, function(p) sqrt((coords[i, 1] - p[1])^2 + (coords[i, 2] - p[2])^2))
    prominence[i] <- sum(distances < dist_threshold) - 1
  }
  return(prominence)
}

# 6. Evaluate the Attack's Success for a specific method
evaluate_attack <- function(data, k, centrality_type) {
  top_k_topo_nodes <- data %>%
    filter(!!as.symbol(paste0("rank_", centrality_type)) <= k) %>%
    pull(name)
  top_k_visual_nodes <- data %>%
    filter(rank_visual <= k) %>%
    pull(name)
  correct_guesses <- length(intersect(top_k_topo_nodes, top_k_visual_nodes))
  success_rate <- (correct_guesses / k) * 100
  return(success_rate)
}

# 7. Run the full experiment for each obfuscation method and print results
#opcion = 0
#> methods 
# [1] "Arnold"      "Jitter"      "Affine"      "Circular"    "Shift"      
# [6] "Perm"        "Dp"          "Rand"        "Hybrid"      "ArnoldShift"
#>
if(opcion==0) methods <- c("jitter", "affine", "circular_swap", "shift","arnold")
if(opcion==1) methods <- c("jitter", "affine", "circular_swap", "shift", "arnold","arnoldshift","perm","dp","rand","hybrid")
if(opcion==0)layout_list <- layout_list_orig 
results_list <- list()
dist_threshold = .2#2.
dist_threshold 
###
kkk=0
tempo_success <- matrix(NA,length(methods),3)
for (method in methods) {
  cat(paste0("\n--- Results for ", method, " obfuscation ---\n"))
kkk=kkk+1  
#  obfuscated_coords <- switch(method,
#    "jitter" = apply_jitter(original_coords),
#    "affine" = apply_affine(original_coords),
#    "circular_swap" = apply_circular_swap(original_coords),
#    "shift" = apply_shift(original_coords),
#    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
##    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
#  )
 



  obfuscated_coords <- switch(method,
    "jitter" = apply_jitter(original_coords,noise),
    "affine" = apply_affine(original_coords,angle, scale),
    "circular_swap" = apply_circular_swap(original_coords),
    "shift" = apply_shift(original_coords,shift),
    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
#    "arnold" = apply_arnold(original_coords, iterations = chosen_iter,mod = 1)
  )

  obfuscated_coords <- switch(method,
    "jitter" = distorted_layouts$Jitter,
    "affine" = distorted_layouts$Affine,
    "circular_swap" = distorted_layouts$Circular,
    "shift" = distorted_layouts$Shift,
    "arnold" = distorted_layouts$Arnold)


if(opcion ==1){
  obfuscated_coords <- switch(method,
    "jitter" = distorted_layouts$Jitter,
    "affine" = distorted_layouts$Affine,
    "circular_swap" = distorted_layouts$Circular,
    "shift" = distorted_layouts$Shift,
    "arnold" = distorted_layouts$Arnold,
    "arnoldshift" = distorted_layouts$ArnoldShift,
"perm"= distorted_layouts$Perm,
"dp"= distorted_layouts$Dp,
"rand"= distorted_layouts$Rand,
"hybrid"= distorted_layouts$Hybrid
)
}

  obfuscated_df <- nodes_df %>%
    mutate(
      x_obfuscated = obfuscated_coords[, 1],
      y_obfuscated = obfuscated_coords[, 2],
      visual_prominence = calculate_visual_prominence(obfuscated_coords, dist_threshold = dist_threshold )
    ) %>%
    mutate(rank_visual = rank(-visual_prominence, ties.method = "min"))
 kj=0   
  for (k in c(5, 10, 15)) {
kj=kj+1
    success_rate <- evaluate_attack(obfuscated_df, k, "degree")
tempo_success [kkk,kj]=success_rate
    cat(paste0("  Top-", k, " success rate (by Degree): ", round(success_rate, 2), "%\n"))
  }
}
rownames(tempo_success) <- methods

tempo_success
###############################################

#chat# ================================
# Batch Evaluation of Multiple Obfuscation Methods
# ================================

library(igraph)
library(ggplot2)
library(gridExtra)
library(infotheo)
library(mclust)

# ---- Utility: entropy function ----
shannon_entropy <- function(x) {
  p <- prop.table(table(x))
  -sum(p * log(p))
}

# ---- Core evaluation (same as before) ----
evaluate_obfuscation <- function(g, coords_orig, coords_obf, method_name="") {
  # Node IDs
  if(is.null(V(g)$name)) V(g)$name <- as.character(1:vcount(g))
  coords_orig$node <- V(g)$name
  coords_obf$node  <- V(g)$name
  
  # Displacement
  disp <- sqrt((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  avg_disp <- mean(disp)
  max_disp <- max(disp)
  disp_entropy <- shannon_entropy(round(disp, 2))
  nde <- mean((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  
  # Centrality recovery
  centroid_obf  <- colMeans(coords_obf[,c("x","y")])
  spatial_cent_obf <- sqrt((coords_obf$x - centroid_obf[1])^2 + (coords_obf$y - centroid_obf[2])^2)
  true_cent <- betweenness(g)
  rank_corr_obf <- suppressWarnings(cor(rank(true_cent), rank(spatial_cent_obf), method="kendall"))
  disc_true  <- infotheo::discretize(true_cent, disc="equalfreq", nbins=5)
  disc_spat  <- infotheo::discretize(spatial_cent_obf, disc="equalfreq", nbins=5)
  mi_obf     <- mutinformation(disc_true, disc_spat)
  
  # Mapping entropy
  perm <- match(paste(coords_obf$x, coords_obf$y), paste(coords_orig$x, coords_orig$y))
  mapping_entropy <- shannon_entropy(perm)
  
  # Community leakage
  comm_true <- cluster_louvain(g)$membership
  k <- length(unique(comm_true))
  comm_spatial <- kmeans(coords_obf[,c("x","y")], centers=k, nstart=10)$cluster
  comm_ARI <- adjustedRandIndex(comm_true, comm_spatial)
  
  return(data.frame(
    Method = method_name,
    AvgDisplacement = avg_disp,
    MaxDisplacement = max_disp,
    DispEntropy = disp_entropy,
    NDE = nde,
    RankCorr = rank_corr_obf,
    MI = mi_obf,
    MappingEntropy = mapping_entropy,
    CommunityARI = comm_ARI
  ))
}
###############################################################3

######################################## aqui si hacia abajo



evaluate_from_list <- function(g, layout_list) {
  results <- data.frame()
  
  # Extract original
  coords_orig <- as.data.frame(layout_list$Original)
  colnames(coords_orig) <- c("x","y")
  
  # Loop over all obfuscated versions
  for (name in names(layout_list)) {
    if (name == "Original") next  # skip original
    coords_obf <- as.data.frame(layout_list[[name]])
    colnames(coords_obf) <- c("x","y")
    
    # Evaluate
    res <- evaluate_obfuscation(g, coords_orig, coords_obf, method_name=name)
    results <- rbind(results, res)
  }
  
  return(results)
}

# Example run
#library(igraph)
#g <- make_graph("Zachary")
#V(g)$name <- as.character(1:vcount(g))

results_from_list <- evaluate_from_list(g, layout_list)
print(results_from_list)

# A simple Shannon Entropy function (if you don't have one)
shannon_entropy <- function(p) {
  p_norm <- p / sum(p) # normalize if not already a probability distribution
  p_norm <- p_norm[p_norm > 0] # remove zero probabilities
  -sum(p_norm * log2(p_norm))
}

# --- New Functions that return Permutation and Coordinates ---

get_shifted_layout_and_perm <- function(coords, shift) {
  n <- nrow(coords)
  perm <- ((1:n - shift - 1) %% n) + 1
  return(list(coords = coords[perm, ], perm = perm))
}

get_circular_layout_and_perm <- function(coords) {
  center <- colMeans(coords)
  radius <- sqrt(rowSums((coords - center)^2))
  angle <- atan2(coords[,2] - center[2], coords[,1] - center[1])
  
  # The permutation is based on the randomized angle order
  perm <- order(sample(angle))
  
  coords_out <- cbind(radius * cos(angle[perm]), radius * sin(angle[perm])) + center
  
  return(list(coords = coords_out, perm = perm))
}

get_arnold_shifted_layout_and_perm <- function(coords, iterations = 1) {
  # Step 1: Apply Arnold map (in-place transformation)
  coords_arnold <- coords
  for (i in 1:iterations) {
    coords_arnold <- t(apply(coords_arnold, 1, function(p) arnold_map(p[1], p[2], mod = 1)))
  }
  
  # Step 2: Apply Shift permutation to the Arnold output
  n <- nrow(coords)
  perm <- ((1:n - shift - 1) %% n) + 1
  coords_arnold_shifted <- coords_arnold[perm, ]
  
  return(list(coords = coords_arnold_shifted, perm = perm))
}

permutations <- list() 

shift_results <- get_shifted_layout_and_perm(layout_norm, shift = shift)
#distorted_layouts$Shift <- shift_results$coords
permutations$Shift <- shift_results$perm # Now this will work

circular_results <- get_circular_layout_and_perm(layout_norm)
#distorted_layouts$Circular <- circular_results$coords
permutations$Circular <- circular_results$perm

if(opcion==1){
# Example for ArnoldShift
arnold_shift_results <- get_arnold_shifted_layout_and_perm(layout_norm, iterations = best_params$Arnold)
#distorted_layouts$ArnoldShift <- arnold_shift_results$coords
permutations$ArnoldShift <- arnold_shift_results$perm
}

# For methods without a permutation, just add a placeholder
permutations$Arnold <- 1:nrow(layout_norm)
permutations$Jitter <- 1:nrow(layout_norm)
permutations$Affine <- 1:nrow(layout_norm)
permutations$Perm <- 1:nrow(layout_norm)
permutations$Rand <- 1:nrow(layout_norm)
permutations$Dp <- 1:nrow(layout_norm)
permutations$Hybrid <- 1:nrow(layout_norm)

# A better function to calculate normalized Kendall Tau distance
kendall_tau_distance1 <- function(perm_original, perm_obfuscated) {
  # Calculate Kendall's Tau correlation coefficient
  # The value ranges from -1 (perfectly inverted) to 1 (perfectly correlated)
  tau_cor <- cor(perm_original, perm_obfuscated, method = "kendall")
  
  # The Kendall's Tau DISTANCE is a measure of dissimilarity,
  # so we convert the correlation to a distance.
  # The formula is (1 - tau_cor) / 2, which normalizes the result
  # to a score from 0 (perfect correlation) to 1 (perfect inversion).
  tau_distance <- (1 - tau_cor) / 2
  
  return(tau_distance)
}


evaluate_obfuscation1 <- function(g, coords_orig, coords_obf, method_name) {
  # Node IDs
  if(is.null(V(g)$name)) V(g)$name <- as.character(1:vcount(g))
  coords_orig$node <- V(g)$name
  coords_obf$node  <- V(g)$name
print("**********************")
print(method_name)
print(permutations[[method_name]])  
  # Displacement
  disp <- sqrt((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  avg_disp <- mean(disp)
  max_disp <- max(disp)
  disp_entropy <- shannon_entropy(round(disp, 2))
  nde <- mean((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  
  # Centrality recovery
  centroid_obf  <- colMeans(coords_obf[,c("x","y")])
  spatial_cent_obf <- sqrt((coords_obf$x - centroid_obf[1])^2 + (coords_obf$y - centroid_obf[2])^2)
  true_cent <- betweenness(g)
  rank_corr_obf <- suppressWarnings(cor(rank(true_cent), rank(spatial_cent_obf), method="kendall"))
  disc_true  <- infotheo::discretize(true_cent, disc="equalfreq", nbins=5)
  disc_spat  <- infotheo::discretize(spatial_cent_obf, disc="equalfreq", nbins=5)
  mi_obf     <- mutinformation(disc_true, disc_spat)
  
  # Mapping entropy
#  perm <- match(paste(coords_obf$x, coords_obf$y), paste(coords_orig$x, coords_orig$y))
original_perm <- 1:vcount(g)
print(original_perm)
  mapping_entropy <- kendall_tau_distance1(original_perm,permutations[[method_name]])
  
  # Community leakage
  comm_true <- cluster_louvain(g)$membership
  k <- length(unique(comm_true))
  comm_spatial <- kmeans(coords_obf[,c("x","y")], centers=k, nstart=10)$cluster
  comm_ARI <- adjustedRandIndex(comm_true, comm_spatial)
  
  return(data.frame(
    Method = method_name,
    AvgDisplacement = avg_disp,
    MaxDisplacement = max_disp,
    DispEntropy = disp_entropy,
    NDE = nde,
    RankCorr = rank_corr_obf,
    MI = mi_obf,
    MappingEntropy = mapping_entropy,
    CommunityARI = comm_ARI
  ))
}
evaluate_from_list1 <- function(g, layout_list) {
  results <- data.frame()
  
  # Extract original
  coords_orig <- as.data.frame(layout_list$Original)
  colnames(coords_orig) <- c("x","y")
  
  # Loop over all obfuscated versions
  for (name in names(layout_list)) {
    if (name == "Original") next  # skip original
    coords_obf <- as.data.frame(layout_list[[name]])
    colnames(coords_obf) <- c("x","y")
    
    # Evaluate
    res <- evaluate_obfuscation1(g, coords_orig, coords_obf, method_name=name)
    results <- rbind(results, res)
  }
  
  return(results)
}
results_from_list1 <- evaluate_from_list1(g, layout_list)
print(results_from_list1)

results_from_list0  <-results_from_list
results_from_list  <-results_from_list1
##############################################################3



###################################
#g <- make_graph("Zachary")
#V(g)$name <- as.character(1:vcount(g))
#coords_orig <- 
#results_all <- apply_obfuscations(g)
#print(results_all)

results <-results_from_list


# ================================
# Radar Chart of Privacy Effectiveness
# ================================

# Install package if needed
# install.packages("fmsb")

library(fmsb)

# ---- Data: your results ----
resultsxx <- data.frame(
  Method = c("Jitter", "Affine", "Circular", "Permutation", "Arnold"),
  AvgDisplacement = c(0.7027344, 3.1636854, 4.0545331, 4.2355644, 4.4078964),
  MaxDisplacement = c(1.749123, 6.633358, 8.172122, 11.121577, 8.991237),
  DispEntropy     = c(3.322494, 3.485587, 3.485587, 3.485587, 3.526361),
  NDE             = c(0.5989409, 12.1442519, 19.5850286, 24.7566402, 24.0728461),
  RankCorr        = c(-0.36851640, -0.38371296, -0.04991428, -0.19375605, -0.15576467),
  MI              = c(0.4630779, 0.5907936, 0.1385731, 0.2825589, 0.2338274),
  MappingEntropy  = c(0.0, 0.0, 0.0, 3.526361, 0.0),
  CommunityARI    = c(0.32723243, 0.55666494, 0.16094483, 0.02275605, 0.05308465)
)

# ---- Normalize to 0–1 scale ----
metrics <- colnames(results)[-1]
df_norm <- results

for (col in metrics) {
  if (col %in% c("RankCorr","MI","CommunityARI")) {
    # invert: lower leakage = better privacy
    df_norm[[col]] <- 1 - (results[[col]] - min(results[[col]], na.rm=TRUE)) /
                            (max(results[[col]], na.rm=TRUE) - min(results[[col]], na.rm=TRUE))
  } else {
    # normal: higher displacement/entropy = stronger privacy
    df_norm[[col]] <- (results[[col]] - min(results[[col]], na.rm=TRUE)) /
                      (max(results[[col]], na.rm=TRUE) - min(results[[col]], na.rm=TRUE))
  }
}

# ---- Radar chart needs max + min rows ----
df_radar <- rbind(
  rep(1, length(metrics)),  # max
  rep(0, length(metrics)),  # min
  df_norm[, metrics]
)
rownames(df_radar) <- c("Max","Min",results$Method)

# ---- Plot ----
colors <- c("blue","red","green","purple","orange","cyan")
windows()
radarchart(df_radar, axistype=1,
           # grid
           cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
           # polygons
           pcol=colors, plwd=2, plty=1,
           pfcol=scales::alpha(colors,0.1),
           # labels
           vlcex=0.8)

legend("bottomleft", legend=results$Method, col=colors, lty=1, lwd=2, bty="n", cex=0.8)


metrics <- colnames(results)[-1]  # exclude Method

# Ranking function: always return consistent column names
get_rankings <- function(df, metric, invert=FALSE) {
  if (invert) {
    ranked <- df %>% arrange(!!sym(metric))
  } else {
    ranked <- df %>% arrange(desc(!!sym(metric)))
  }
  ranked <- ranked %>%
    mutate(Rank = row_number(),
           Metric = metric) %>%
    select(Metric, Method, Value = !!sym(metric), Rank)
  return(ranked)
}

# Apply to all metrics
rankings_list <- list()
for (m in metrics) {
  if (m %in% c("RankCorr","MI","CommunityARI")) {
    rankings_list[[m]] <- get_rankings(results, m, invert=TRUE)
  } else {
    rankings_list[[m]] <- get_rankings(results, m, invert=FALSE)
  }
}

# Combine safely
rankings <- bind_rows(rankings_list)

# Best method per metric
best_methods <- rankings %>% filter(Rank == 1) %>% select(Metric, BestMethod = Method)
rankings_list
print(rankings)
print(best_methods)

 results_norm <- df_norm

# --- Define user weights (example: emphasize MappingEntropy & CommunityARI) ---
weights <- c(
  AvgDisplacement = 0.1,
  MaxDisplacement = 0.1,
  DispEntropy     = 0.1,
  NDE             = 0.1,
  RankCorr        = 0.2,
  MI              = 0.2,
  MappingEntropy  = 0.1,
  CommunityARI    = 0.1
)

# --- Compute global score ---
results_norm$GlobalScore <- as.numeric(as.matrix(results_norm[,-1]) %*% weights)

# --- Sort by best privacy score ---
results_ranked <- results_norm %>% arrange(desc(GlobalScore))

print(results_ranked[,c("Method","GlobalScore")])
######
# A script to quantify the visual similarity between original and obfuscated layouts.

# This script requires the 'vegan' package for Procrustes analysis.
# If you don't have it, uncomment the line below and run it once:
# install.packages("vegan")

library(vegan)

# --- Define the Similarity Analysis Function ---
# This function takes a list of named layouts, where the first one is the original.
# It returns a data frame with the similarity metrics for each obfuscated layout.
calculate_layout_similarity <- function(layout_list) {
  
  # 1. Get the original layout and its name
  original_name <- names(layout_list)[1]
  original_coords <- layout_list[[1]]

  # 2. Calculate the pairwise Euclidean distances for the original layout
  original_dist_matrix <- as.matrix(dist(original_coords, method = "euclidean"))
  
  # Prepare a data frame to store the results
  results_df <- data.frame(
    Method = character(),
    Procrustes_Distance = numeric(),
    Stress_Measure = numeric(),
    Distance_Correlation = numeric(),
    stringsAsFactors = FALSE
  )
  
  # 3. Loop through the obfuscated layouts and perform the analysis
  for (i in 2:length(layout_list)) {
    obfuscated_name <- names(layout_list)[i]
    obfuscated_coords <- layout_list[[i]]
    
    # Calculate the pairwise Euclidean distances for the obfuscated layout
    obfuscated_dist_matrix <- as.matrix(dist(obfuscated_coords, method = "euclidean"))
    
    # -- 4. Quantify the Similarity Metrics --
    
    # A) Procrustes Distance
    # This measures the minimum distance between the two layouts after optimal rotation and scaling.
    # It requires the same number of rows (nodes) in both data sets.
    proc_result <- procrustes(original_coords, obfuscated_coords, symmetric = TRUE)
    procrustes_dist <- proc_result$ss
    
    # B) Stress Measure (Sum of Squared Differences of Distances)
    # This directly compares the two distance matrices. A lower value indicates less distortion.
    stress_val <- sum((original_dist_matrix - obfuscated_dist_matrix)^2)
    
    # C) Correlation of Inter-node Euclidean Distances
    # This measures the linear correlation between the two sets of distances. A high correlation
    # indicates that the relative distances between nodes were preserved.
    distance_cor <- cor(as.vector(original_dist_matrix), as.vector(obfuscated_dist_matrix))
    
    # Add the results to the data frame
    results_df <- rbind(results_df, data.frame(
      Method = obfuscated_name,
      Procrustes_Distance = procrustes_dist,
      Stress_Measure = stress_val,
      Distance_Correlation = distance_cor
    ))
  }
  
  return(results_df)
}

# --- Example of how to use the function ---

# Create dummy layouts for demonstration (replace with your actual layouts)
# Make sure they are data frames or matrices with two columns (x and y)
dummy_original <- data.frame(x = 1:5, y = 1:5)
dummy_jitter <- data.frame(x = 1:5 + rnorm(5, 0, 0.1), y = 1:5 + rnorm(5, 0, 0.1))
dummy_scrambled <- data.frame(x = sample(1:5), y = sample(1:5))

# Create a named list of your layouts
my_layouts <- list(
  "Original" = dummy_original,
  "Jitter" = dummy_jitter,
  "Scrambled" = dummy_scrambled
)

# Run the analysis

similarity_results <- calculate_layout_similarity(layout_list)
print(similarity_results)

# --- Interpretation of the Results ---

#The three metrics you've chosen provide a robust, quantitative way to measure the visual similarity that can lead to a "sense of familiarity" and a false sense of security in an observer.
#
#1.  **Procrustes Distance:** This metric quantifies the overall **shape similarity** between the two layouts after they have been optimally aligned (rotated, scaled, and translated). A low Procrustes distance indicates that the obfuscated layout retains the overall "shape" of the original. Methods like your "Shift" approach, which preserve the spatial footprint, would be expected to have a much lower Procrustes distance than a chaotic method like the Arnold Transform.
#
#2.  **Stress or Distortion Measure:** This metric directly measures the **distortion of internal relationships** by comparing the distances between all pairs of nodes. A low stress value means that the obfuscation did not significantly alter the relative proximities of the nodes. This is a critical metric for a geographical layout, as it indicates whether local neighborhoods and clusters have been preserved.
#
#3.  **Correlation of Inter-node Distances:** This is a simple but powerful measure of how well the obfuscation method **breaks the overall spatial structure**. A high correlation would show that nodes that were close in the original layout are still relatively close in the obfuscated one. This would suggest the obfuscation is a less robust defense against a savvy adversary.

######### chapt
library(igraph)
library(vegan)     # for Procrustes, Mantel
library(infotheo)  # for MI
library(cluster)   # for ARI
library(entropy)   # for Shannon entropy

# --- helper: Shannon entropy ---
shannon_entropy <- function(x) {
  x <- table(x)
  p <- x / sum(x)
  -sum(p * log(p + 1e-12))
}

# --- evaluation function for one obfuscation ---
evaluate_obfuscation_full <- function(g, coords_orig, coords_obf, method_name="") {
  # --- Node IDs ---
  if (is.null(V(g)$name)) V(g)$name <- as.character(1:vcount(g))
  coords_orig$node <- V(g)$name
  coords_obf$node  <- V(g)$name
  
  # --- Displacement metrics ---
  disp <- sqrt((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  avg_disp <- mean(disp)
  max_disp <- max(disp)
  disp_entropy <- shannon_entropy(round(disp, 2))
  nde <- mean((coords_orig$x - coords_obf$x)^2 + (coords_orig$y - coords_obf$y)^2)
  
  # --- Layout similarity metrics ---
  proc <- procrustes(coords_orig[,c("x","y")], coords_obf[,c("x","y")], scale=TRUE)
  proc_dist <- proc$ss
  
  d_orig <- dist(coords_orig[,c("x","y")])
  d_obf  <- dist(coords_obf[,c("x","y")])
  mantel_res <- suppressWarnings(mantel(d_orig, d_obf))
  mantel_corr <- mantel_res$statistic
  
  # --- Spatial centrality (obfuscated) ---
  centroid_obf <- colMeans(coords_obf[,c("x","y")])
  spatial_cent_obf <- sqrt((coords_obf$x - centroid_obf[1])^2 +
                           (coords_obf$y - centroid_obf[2])^2)
  
  # --- Topological centralities ---
  centralities <- list(
    Degree      = degree(g),
    Closeness   = closeness(g, normalized=TRUE),
    Betweenness = betweenness(g),
    Eigenvector = eigen_centrality(g)$vector
  )
  
  rankcorr_list <- c()
  mi_list <- c()
  
  for (cname in names(centralities)) {
    true_cent <- centralities[[cname]]
    
    rank_corr <- suppressWarnings(cor(rank(true_cent), rank(spatial_cent_obf), method="kendall"))

disc_true <- infotheo::discretize(true_cent, disc="equalfreq", nbins=5)
disc_spat <- infotheo::discretize(spatial_cent_obf, disc="equalfreq", nbins=5)
mi <- mutinformation(disc_true, disc_spat)    
    rankcorr_list[paste0("RankCorr_", cname)] <- rank_corr
    mi_list[paste0("MI_", cname)] <- mi
  }
  
  # --- Mapping entropy ---
  perm <- match(paste(coords_obf$x, coords_obf$y), paste(coords_orig$x, coords_orig$y))
  mapping_entropy <- shannon_entropy(perm)
  
  # --- Community leakage ---
  comm_true <- cluster_louvain(g)$membership
  k <- length(unique(comm_true))
  comm_spatial <- kmeans(coords_obf[,c("x","y")], centers=k, nstart=10)$cluster
  comm_ARI <- adjustedRandIndex(comm_true, comm_spatial)
  
  # --- Assemble result ---
  res <- data.frame(
    Method = method_name,
    AvgDisplacement = avg_disp,
    MaxDisplacement = max_disp,
    DispEntropy = disp_entropy,
    NDE = nde,
    ProcrustesSS = proc_dist,
    MantelCorr = mantel_corr,
    MappingEntropy = mapping_entropy,
    CommunityARI = comm_ARI,
    t(rankcorr_list),
    t(mi_list),
    row.names=NULL
  )
  return(res)
}

# --- apply to all obfuscation methods in layout_list ---
evaluate_all_methods <- function(g, layout_list) {
  results <- data.frame()
  
  coords_orig <- as.data.frame(layout_list$Original)
  colnames(coords_orig) <- c("x","y")
  
  for (name in names(layout_list)) {
    if (name == "Original") next
    coords_obf <- as.data.frame(layout_list[[name]])
    colnames(coords_obf) <- c("x","y")
    
    res <- evaluate_obfuscation_full(g, coords_orig, coords_obf, method_name=name)
    results <- rbind(results, res)
  }
  return(results)
}

# --- Example run with Zachary karate club ---
#g <- make_graph("Zachary")
V(g)$name <- as.character(1:vcount(g))

# Assume you have layout_list already
 results_all <- evaluate_all_methods(g, layout_list)
results_all$MappingEntropy <- results_from_list1$MappingEntropy

 print(results_all)

library(dplyr)
library(ggplot2)
library(scales)

# Assume results_all already computed
df <- results_all

# ---- Construct composite axes ----
# Visual scrambling = high ProcrustesSS + low MantelCorr
df <- df %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  )

# Normalize to 0–1
df <- df %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore)
  )

# ---- Plot ----
windows()
biplot_0 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")
print(biplot_0)
ggsave(paste(network1,"biplot_0.pdf"), plot = biplot_0, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_0.jpeg"), plot = biplot_0, width = 10, height = 8, dpi = 300)

library(dplyr)
library(ggplot2)

library(scales)

# Assume results_all already computed
df <- results_all

# ---- Composite scores ----
df <- df %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore)
  )

# ---- Plot with quadrants ----
windows()
biplot_1 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(biplot_1)
ggsave(paste(network1,"biplot_1.pdf"), plot = biplot_1, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_1.jpeg"), plot = biplot_1, width = 10, height = 8, dpi = 300)

df1 <-df
################### aqui weighted
library(dplyr)
library(ggplot2)
library(scales)

# ---- Composite scores ----
df <- results_all %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore),
    GlobalScore = (ScramblingScore + PrivacyScore) / 2   # weighted sum, equal weights
  )

# ---- Plot with quadrants and global ranking ----
windows()
biplot_2 <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = GlobalScore)) +
  geom_point(size = 6) +
  geom_text(vjust = -1, size = 4, color = "black") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
#  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
#  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
#  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
#  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  scale_color_gradient(low = "orange", high = "darkgreen", name = "Global Score") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Scrambling vs Privacy + Global Ranking") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right")
print(biplot_2)
ggsave(paste(network1,"biplot_2.pdf"), plot = biplot_2, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_2.jpeg"), plot = biplot_2, width = 10, height = 8, dpi = 300)
df
dfbase=df

# ---- Composite scores ----
df <- results_all %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore),
    GlobalScore = (ScramblingScore*0.8 + PrivacyScore*0.2)    # weighted sum, equal weights
  )

# ---- Plot with quadrants and global ranking ----
windows()
biplot_a <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = GlobalScore)) +
  geom_point(size = 6) +
  geom_text(vjust = -1, size = 4, color = "black") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
#  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
#  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
#  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
#  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  scale_color_gradient(low = "orange", high = "darkgreen", name = "Global Score") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Scrambling*0.8 vs Privacy*0.2 + Global Ranking") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right")
print(biplot_a)
ggsave(paste(network1,"biplot_a.pdf"), plot = biplot_a, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_a.jpeg"), plot = biplot_a, width = 10, height = 8, dpi = 300)
df
dfa=df

# ---- Composite scores ----
df <- results_all %>%
  mutate(
    ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
    PrivacyScore = rescale(MappingEntropy) +
                   rescale(1 - abs(CommunityARI)) +
                   rescale(1 - abs(RankCorr_Degree)) +
                   rescale(1 - abs(RankCorr_Closeness)) +
                   rescale(1 - abs(RankCorr_Betweenness)) +
                   rescale(1 - abs(RankCorr_Eigenvector)) +
                   rescale(1 - MI_Degree) +
                   rescale(1 - MI_Closeness) +
                   rescale(1 - MI_Betweenness) +
                   rescale(1 - MI_Eigenvector)
  ) %>%
  mutate(
    ScramblingScore = rescale(ScramblingScore),
    PrivacyScore = rescale(PrivacyScore),
    GlobalScore = (ScramblingScore*0.2 + PrivacyScore*0.8)    # weighted sum, equal weights
  )

# ---- Plot with quadrants and global ranking ----
windows()
biplot_b <-ggplot(df, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = GlobalScore)) +
  geom_point(size = 6) +
  geom_text(vjust = -1, size = 4, color = "black") +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
#  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
#  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
#  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
#  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  scale_color_gradient(low = "orange", high = "darkgreen", name = "Global Score") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Scrambling*0.3 vs Privacy*0.7 + Global Ranking") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "right")
print(biplot_b)
ggsave(paste(network1,"biplot_b.pdf"), plot = biplot_b, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot_b.jpeg"), plot = biplot_b, width = 10, height = 8, dpi = 300)
df
dfb=df

todo <- cbind(dfbase$GlobalScore,dfa$GlobalScore,dfb$GlobalScore)
todo
nsamples <- 100  # Number of random weight scenarios to simulate

iviejo=0
if(iviejo==1){

############################################################################### sim original, creo errada
# --- Simulation Setup ---


# Dataframe to store simulation results
sim_results <- data.frame(
    Method = character(),
    Scenario = numeric(),
    GlobalScore = numeric(),
    Rank = numeric()
)

# --- Simulation Loop ---
set.seed(123)
for (i in 1:nsamples) {
    # Generate random weights for Scrambling and Privacy
    # (Keep this part as is if you want them to be in [0,1])
    weights <- runif(2, min = 0, max = 1)
    weight_scrambling <- weights[1] / sum(weights)
    weight_privacy <- weights[2] / sum(weights)
    
    # Generate random weights for PrivacyScore components in specified ranges
    weight_mapping_entropy <- runif(1, min = 0.01, max = 0.04)
    weight_community_ari <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_degree <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_eigenvector <- runif(1, min = 0.01, max = 0.1)
    weight_mi_degree <- runif(1, min = 0.01, max = 0.1)
    weight_mi_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_eigenvector <- runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the weights
    all_weights <- c(
      weight_mapping_entropy,
      weight_community_ari,
      weight_rank_corr_degree,
      weight_rank_corr_closeness,
      weight_rank_corr_betweenness,
      weight_rank_corr_eigenvector,
      weight_mi_degree,
      weight_mi_closeness,
      weight_mi_betweenness,
      weight_mi_eigenvector
    )
    privacy_weights_rand <- all_weights / sum(all_weights)
    
    # Now, calculate scores with these normalized random weights
    df_sim <- results_all %>%
        mutate(
            ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
            PrivacyScore = (
                rescale(MappingEntropy) * privacy_weights_rand[1] +
                rescale(1 - abs(CommunityARI)) * privacy_weights_rand[2] +
                rescale(1 - abs(RankCorr_Degree)) * privacy_weights_rand[3] +
                rescale(1 - abs(RankCorr_Closeness)) * privacy_weights_rand[4] +
                rescale(1 - abs(RankCorr_Betweenness)) * privacy_weights_rand[5] +
                rescale(1 - abs(RankCorr_Eigenvector)) * privacy_weights_rand[6] +
                rescale(1 - MI_Degree) * privacy_weights_rand[7] +
                rescale(1 - MI_Closeness) * privacy_weights_rand[8] +
                rescale(1 - MI_Betweenness) * privacy_weights_rand[9] +
                rescale(1 - MI_Eigenvector) * privacy_weights_rand[10]
            )
        ) %>%
        mutate(
            ScramblingScore = rescale(ScramblingScore),
            PrivacyScore = rescale(PrivacyScore),
            GlobalScore = (ScramblingScore * weight_scrambling + PrivacyScore * weight_privacy)
        )
    
    # Calculate and store the ranks
    df_sim <- df_sim %>%
        arrange(desc(GlobalScore)) %>%
        mutate(Rank = row_number())
        
    sim_results <- rbind(sim_results, data.frame(
        Method = df_sim$Method,
        Scenario = i,
        GlobalScore = df_sim$GlobalScore,
        Rank = df_sim$Rank
    ))
}
}

####################################################################### fin simulacion anterior creo que errada
################################################## nueva simulacion
##############################33
# Simulation Loop ---
nsamples <- 1000  # Number of random weight scenarios to simulate
set.seed(123)
# Dataframe to store simulation results
sim_results <- data.frame(
    Method = character(),
    Scenario = numeric(),
    GlobalScore = numeric(),
        ScramblingScore = numeric(),
        PrivacyScore = numeric(),
    Rank = numeric()
)

for (i in 1:nsamples) {
    # Generate random weights for Scrambling and Privacy
######################
if(i==1){
    weights <- runif(2, min = 0, max = 1)
    weight_scrambling <- 0.5#weights[1] / sum(weights)
    weight_privacy <- 0.5#weights[2] / sum(weights)
    
    # Generate random weights for ScramblingScore components in specified ranges
    weight_procrustes <- 0.02#runif(1, min = 0.01, max = 0.04)
    weight_avg_disp <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_disp_entropy <- 0.05#runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the scrambling weights
    scrambling_all_weights <- c(
      weight_procrustes,
      weight_avg_disp,
      weight_disp_entropy
    )
    scrambling_weights_rand <- scrambling_all_weights / sum(scrambling_all_weights)
    
    # Generate random weights for PrivacyScore components in specified ranges
    weight_mapping_entropy <- 0.02#runif(1, min = 0.01, max = 0.04)
    weight_community_ari <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_degree <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_closeness <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_betweenness <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_eigenvector <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_mi_degree <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_mi_closeness <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_mi_betweenness <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_mi_eigenvector <- 0.05#runif(1, min = 0.01, max = 0.1)
    weight_mantel_corr <- 0.05#runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the privacy weights
    privacy_all_weights <- c(
      weight_mapping_entropy,
      weight_community_ari,
      weight_rank_corr_degree,
      weight_rank_corr_closeness,
      weight_rank_corr_betweenness,
      weight_rank_corr_eigenvector,
      weight_mi_degree,
      weight_mi_closeness,
      weight_mi_betweenness,
      weight_mi_eigenvector,
      weight_mantel_corr
    )
    privacy_weights_rand <- privacy_all_weights / sum(privacy_all_weights)
    
}
if(i>1){
    weights <- runif(2, min = 0, max = 1)
    weight_scrambling <- weights[1] / sum(weights)
    weight_privacy <- weights[2] / sum(weights)
    
    # Generate random weights for ScramblingScore components in specified ranges
    weight_procrustes <- runif(1, min = 0.01, max = 0.04)
    weight_avg_disp <- runif(1, min = 0.01, max = 0.1)
    weight_disp_entropy <- runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the scrambling weights
    scrambling_all_weights <- c(
      weight_procrustes,
      weight_avg_disp,
      weight_disp_entropy
    )
    scrambling_weights_rand <- scrambling_all_weights / sum(scrambling_all_weights)
    
    # Generate random weights for PrivacyScore components in specified ranges
    weight_mapping_entropy <- runif(1, min = 0.01, max = 0.04)
    weight_community_ari <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_degree <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_eigenvector <- runif(1, min = 0.01, max = 0.1)
    weight_mi_degree <- runif(1, min = 0.01, max = 0.1)
    weight_mi_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_eigenvector <- runif(1, min = 0.01, max = 0.1)
    weight_mantel_corr <- runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the privacy weights
    privacy_all_weights <- c(
      weight_mapping_entropy,
      weight_community_ari,
      weight_rank_corr_degree,
      weight_rank_corr_closeness,
      weight_rank_corr_betweenness,
      weight_rank_corr_eigenvector,
      weight_mi_degree,
      weight_mi_closeness,
      weight_mi_betweenness,
      weight_mi_eigenvector,
      weight_mantel_corr
    )
    privacy_weights_rand <- privacy_all_weights / sum(privacy_all_weights)
 }   
    # Now, calculate scores with these normalized random weights
    df_sim <- results_all %>%
        mutate(
            ScramblingScore = (
                rescale(ProcrustesSS) * scrambling_weights_rand[1] +
                rescale(AvgDisplacement) * scrambling_weights_rand[2] +
                rescale(DispEntropy) * scrambling_weights_rand[3]
            ),
            PrivacyScore = (
                rescale(MappingEntropy) * privacy_weights_rand[1] +
                rescale(1 - abs(CommunityARI)) * privacy_weights_rand[2] +
                rescale(1 - abs(RankCorr_Degree)) * privacy_weights_rand[3] +
                rescale(1 - abs(RankCorr_Closeness)) * privacy_weights_rand[4] +
                rescale(1 - abs(RankCorr_Betweenness)) * privacy_weights_rand[5] +
                rescale(1 - abs(RankCorr_Eigenvector)) * privacy_weights_rand[6] +
                rescale(1 - MI_Degree) * privacy_weights_rand[7] +
                rescale(1 - MI_Closeness) * privacy_weights_rand[8] +
                rescale(1 - MI_Betweenness) * privacy_weights_rand[9] +
                rescale(1 - MI_Eigenvector) * privacy_weights_rand[10] +
                rescale(1 - MantelCorr) * privacy_weights_rand[11]
            )
        ) %>%
        mutate(
            ScramblingScore = rescale(ScramblingScore),
            PrivacyScore = rescale(PrivacyScore),
            GlobalScore = (ScramblingScore * weight_scrambling + PrivacyScore * weight_privacy)
        )
    
    # Calculate and store the ranks
    df_sim <- df_sim %>%
        arrange(desc(GlobalScore)) %>%
        mutate(Rank = row_number())
if(i==1){
    sim_results <- rbind(sim_results, data.frame(
        Method = df_sim$Method,
        Scenario = i,
        GlobalScore = df_sim$GlobalScore,
        ScramblingScore = df_sim$ScramblingScore,
        PrivacyScore = df_sim$PrivacyScore,
        Rank = df_sim$Rank
    ))
sim_results1 <-sim_results
}
if(i>1){        
    sim_results <- rbind(sim_results, data.frame(
        Method = df_sim$Method,
        Scenario = i,
        GlobalScore = df_sim$GlobalScore,
        ScramblingScore = df_sim$ScramblingScore,
        PrivacyScore = df_sim$PrivacyScore,
        Rank = df_sim$Rank
    ))
}
}

# Load necessary libraries
library(ggplot2)
library(car)
library(dplyr)

# Assuming 'sim_results' is the data frame from your simulation
# Create the base plot with all 
#    x = "Scrambling Score",
#    y = "Privacy Score"

p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Ellipses indicate the spread of scores; crosses mark the mean of each method.",
    x = "Scrambling Score",
    y = "Privacy Score"  ) +
  theme_minimal() +
  scale_color_discrete(name = "Obfuscation Method") +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add ellipses only for methods with variability
unique_methods <- unique(sim_results$Method)
for (method in unique_methods) {
  method_data <- sim_results %>% filter(Method == method)
  
  # Check for sufficient variance to draw an ellipse
  if (sd(method_data$ScramblingScore) > 1e-6 & sd(method_data$PrivacyScore) > 1e-6) {
    p <- p + stat_ellipse(data = method_data, aes(group = Method),
                          type = "norm", level = 0.95, geom = "polygon",
                          alpha = 0.1, fill = "white", color = "black")
  }
}

# Add mean points for all methods
p <- p + geom_point(data = sim_results %>% group_by(Method) %>% summarize(ScramblingScore = mean(ScramblingScore), PrivacyScore = mean(PrivacyScore)),
             aes(x = ScramblingScore, y = PrivacyScore),
             shape = 4, size = 4, stroke = 2)

# Print the final plot
windows()
print(p)

p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Ellipses indicate the spread of scores; crosses mark the mean of each method.",
    x = "Scrambling Score",
    y = "Privacy Score"

  ) +
  theme_minimal() +
  scale_color_discrete(name = "Obfuscation Method") +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add ellipses only for methods with variability
unique_methods <- unique(sim_results$Method)
for (method in unique_methods) {
  method_data <- sim_results %>% filter(Method == method)
  
  # Check for sufficient variance to draw an ellipse
  if (sd(method_data$ScramblingScore) > 1e-6 & sd(method_data$PrivacyScore) > 1e-6) {
    p <- p + stat_ellipse(data = method_data, aes(group = Method),
                          type = "norm", level = 0.95, geom = "polygon",
                          alpha = 0.1, fill = "white", color = "black")
  }
}

# Add mean points for all methods
p <- p + geom_point(data = sim_results %>% group_by(Method) %>% summarize(ScramblingScore = mean(ScramblingScore), PrivacyScore = mean(PrivacyScore)),
             aes(x = ScramblingScore, y = PrivacyScore),
             shape = 4, size = 4, stroke = 2)

# Print the final plot
windows()
print(p)
p0<-p
ggsave(paste(network1,"p0.pdf"), plot = p0, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"p0.jpeg"), plot = p0, width = 10, height = 8, dpi = 300)


# Create a summary data frame with the mean scores for each method
centroids <- sim_results %>%
  group_by(Method) %>%
  summarize(
    ScramblingScore = mean(ScramblingScore),
    PrivacyScore = mean(PrivacyScore),
    .groups = 'drop'
  )

# Create the base plot with points and no legend
#  xlab("Visual Scrambling (Aesthetic Anonymization)") +
#  ylab("Privacy Protection (True Anonymization)") +

p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Ellipses indicate the spread of scores; filled points mark the mean of each method.",
    x = "Scrambling Score-Aesthetic Anonymization",
    y = "Privacy Score-True Anonymization"
  ) +
  theme_minimal() +
  scale_color_discrete(guide = "none") + # Hide the default legend
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add ellipses only for methods with variability
unique_methods <- unique(sim_results$Method)
for (method in unique_methods) {
  method_data <- sim_results %>% filter(Method == method)
  
  # Check for sufficient variance to draw an ellipse
  if (sd(method_data$ScramblingScore) > 1e-6 & sd(method_data$PrivacyScore) > 1e-6) {
    p <- p + stat_ellipse(data = method_data, aes(group = Method),
                          type = "norm", level = 0.95, geom = "polygon",
                          alpha = 0.1, fill = "white", color = "black")
  }
}

# Add text labels for each centroid using the Method name
p <- p + geom_text(
  data = centroids,
  aes(label = Method),
  color = "black", # Use black for readability
  size = 4,
  fontface = "bold",
  nudge_x = 0.0,
  nudge_y = -0.05
) +
  # Add a filled point for the centroid
  geom_point(data = centroids,
             aes(shape = Method),
             size = 2, stroke = 1.2, fill = "white", color = "black") + # Using shape=21 creates a filled circle with a black border
  # Use a separate legend for the shapes if desired (not recommended for this plot)
  scale_shape_manual(values = rep(21, nrow(centroids))) +
  guides(shape = "none")


# Print the final plot
windows()
print(p)
p1<-p
ggsave(paste(network1,"p1.pdf"), plot = p1, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"p1.jpeg"), plot = p1, width = 10, height = 8, dpi = 300)



################################################# fin nueva simulacion
# --- Rank Distribution Heatmap (Percentages + Labels) ---

# Count how many times each Method got each Rank
rank_counts <- sim_results %>%
  group_by(Method, Rank) %>%
  summarise(Count = n(), .groups = "drop")

# Normalize counts to percentages
rank_counts <- rank_counts %>%
  group_by(Method) %>%
  mutate(Percent = Count / sum(Count) * 100)

# Plot heatmap of percentages with labels
windows()
library(scales)

heat0 <- ggplot(rank_counts, aes(x = Rank, y = reorder(Method, -Rank), fill = Percent)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.1f%%", Percent)),
            size = 4.5, fontface = "bold", color = "black") +
  scale_fill_gradient(low = "white", high = "darkblue", name = "% of Scenarios") +
  scale_x_continuous(breaks = seq(min(rank_counts$Rank), max(rank_counts$Rank), by = 1)) +  # integer ticks
  theme_minimal(base_size = 14) +
  labs(
    title = "Percentage Distribution of Ranks for Each Obfuscation Method",
    x = "Rank Position",
    y = "Obfuscation Method"
  ) +
  theme(
    plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
    axis.title.x = element_text(size = 16, face = "bold", angle = 0),
    axis.title.y = element_text(size = 16, face = "bold", angle = 90),
    axis.text.x = element_text(size = 14, angle = 0, hjust = 0.5, face = "bold"),
    axis.text.y = element_text(size = 14, face = "bold"),
    legend.title = element_text(size = 14, face = "bold"),
    legend.text = element_text(size = 12),
    panel.grid.major = element_line(color = "gray80", size = 0.5),
    panel.grid.minor = element_line(color = "gray90", size = 0.25)
  )

print(heat0)

ggsave(paste(network1,"heat0.pdf"), plot = heat0, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"heat0.jpeg"), plot = heat0, width = 10, height = 8, dpi = 300)
write_xlsx(sim_results1, paste(network1,"sim1.xlsx"))
########################################################
#############################################
# given layout_original (n x 2) and layout_obf (n x 2)
library(clue)  # for solve_LSAP


compute_recovery_rate <- function(layout_original, layout_obf, topk = NULL) {
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  n <- nrow(layout_original)
  cost <- dmat[1:n, (n+1):(2*n)]
  # assignment (Hungarian)
  assign <- solve_LSAP(cost)        # returns a permutation mapping original -> obf index
  # if you have true node IDs preserved as row order, success = identity
  success <- sum(assign == seq_len(n)) / n
  if (!is.null(topk)) {
    # topk strategy: compare top-k nodes by degree recovered
    # topk recovery placeholder — compute separately
  }
  return(as.numeric(success))
}

#recovery_rate <- compute_recovery_rate(layout_original, layout_obf)

# Remove "Original" from evaluation
methods <- setdiff(names(layout_list), "Original")

rev_results <- lapply(methods, function(m) {
  res <- compute_recovery_rate(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res

  )
})

rev_df1 <- do.call(rbind, rev_results)
print(rev_df1)

library(clue)  # for solve_LSAP

compute_recovery_rate <- function(layout_original, layout_obf, 
                                  topk = NULL, centrality = NULL, graph = NULL) {
  # layout_original, layout_obf : n x 2 coordinate matrices
  # topk : if not NULL, test recovery of top-k nodes by centrality
  # centrality : vector of centrality scores for original graph (length n)
  # graph : igraph object (only needed if you want to recompute centralities inside)
  
  n <- nrow(layout_original)
  
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  cost <- dmat[1:n, (n+1):(2*n)]
  
  # Hungarian assignment
  assign <- solve_LSAP(cost)  # permutation mapping original -> obf
  
  if (is.null(topk)) {
    # ---- Case 1: Exact ID match ----
    success <- sum(assign == seq_len(n)) / n
    return(success)
  } else {
    # ---- Case 2: Top-k recovery ----
    if (is.null(centrality)) {
      if (is.null(graph)) stop("Need either centrality vector or graph object")
      stop("Please compute centrality outside and pass as vector for reproducibility.")
    }
    
    # Identify top-k original nodes
    topk_orig <- order(centrality, decreasing = TRUE)[1:topk]
    
    # Get matched obfuscated indices
    matched_ids <- as.integer(assign)
    
    # Map original top-k to their obfuscated partners
    recovered <- matched_ids[topk_orig]
    
    # Check overlap with top-k nodes in obfuscation by same centrality
    # (if we have centrality scores for obfuscated graph)
    if (!is.null(graph)) {
      stop("Option to recompute obfuscated centrality not implemented here.")
    }
    
    # For simplicity: measure fraction of top-k original nodes correctly mapped
    success <- sum(recovered %in% topk_orig) / topk
    return(success)
  }
}

rev_results1 <- lapply(methods, function(m) {
  res <- compute_recovery_rate(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res

  )
})

rev_df11 <- do.call(rbind, rev_results1)
print(rev_df11)

library(clue)  # for solve_LSAP

compute_recovery_info <- function(layout_original, layout_obf) {
  n <- nrow(layout_original)
  
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  cost <- dmat[1:n, (n+1):(2*n)]
  
  # Hungarian assignment (minimum-cost matching)
  assign <- solve_LSAP(cost)   # permutation mapping original -> obf
  
  # Identify correctly recovered nodes
  recovered_ids <- which(assign == seq_len(n))
  
  # Proportion of correctly recovered nodes
  mean_recover <- length(recovered_ids) / n
  
  list(
    MeanRecover = mean_recover,
    RecoveredNodes = recovered_ids
  )
}

methods <- setdiff(names(layout_list), "Original")

rev_results <- lapply(methods, function(m) {
  res <- compute_recovery_info(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res$MeanRecover,
    RecoveredNodes = paste(res$RecoveredNodes, collapse = ",")
  )
})

rev_df <- do.call(rbind, rev_results)
print(rev_df)

dfbase$MeanRecover <- rev_df$MeanRecover

dfbase

write_xlsx(dfbase, paste(network1,"dbase.xlsx"))

write_xlsx(sim_results1, paste(network1,"sim1.xlsx"))

####
windows()
biplot_1a <-ggplot(sim_results1, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(biplot_1a)
ggsave(paste(network1,"biplot1a.pdf"), plot = biplot_1a, width = 10, height = 8, dpi = 300)
ggsave(paste(network1,"biplot1a.jpeg"), plot = biplot_1a, width = 10, height = 8, dpi = 300)



#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################












#############################
# A function to generate random weights that respect min/max constraints and sum to 1
generate_constrained_weights <- function(constraints_df) {
  # Get number of metrics and ranges
  n_metrics <- nrow(constraints_df)
  min_w <- constraints_df$min_w
  max_w <- constraints_df$max_w
  
  # Calculate the remaining mass after allocating the minimums
  remaining_mass <- 1 - sum(min_w)
  
  # Calculate the range of each weight
  weight_ranges <- max_w - min_w
  
  # Generate a set of random numbers (deltas) in proportion to each weight's range
  # We use a single 'runif' call for efficiency
  deltas <- runif(n_metrics, min = 0, max = weight_ranges)
  
  # Scale the deltas so their sum equals the remaining mass
  scaled_deltas <- deltas * (remaining_mass / sum(deltas))
  
  # Finalize the weights
  final_weights <- min_w + scaled_deltas
  
  # Return the weights as a named list or vector
  setNames(final_weights, constraints_df$metric)
}

# --------------------------------------------------------------------------
# 1. DEFINE WEIGHT CONSTRAINTS AND RUN THE MONTE CARLO SIMULATION
# --------------------------------------------------------------------------

# Define Weight Constraints for PrivacyScore Metrics
weight_constraints <- data.frame(
  metric = c(
    "MappingEntropy", "CommunityARI", "RankCorr_Degree", "RankCorr_Closeness",
    "RankCorr_Betweenness", "RankCorr_Eigenvector", "MI_Degree", "MI_Closeness",
    "MI_Betweenness", "MI_Eigenvector"
  ),
  # Define your weight intervals as requested
  min_w = c(0.01, rep(0.01, 9)),
  max_w = c(0.04, rep(0.1, 9))
)

weight_constraints

# Set the number of simulations
n_simulations <- 1000

# Dataframe to store simulation results
sim_results <- data.frame(
  Method = character(),
  Scenario = numeric(),
  GlobalScore = numeric(),
  Rank = numeric()
)

# --- Simulation Loop ---
for (i in 1:n_simulations) {
  # Generate a single set of constrained weights for this scenario
  privacy_weights_rand <- generate_constrained_weights(weight_constraints)
  
  # Generate weights for GlobalScore (Scrambling and Privacy) from a uniform distribution
  weights_global <- runif(2)
  weight_scrambling <- weights_global[1] / sum(weights_global)
  weight_privacy <- weights_global[2] / sum(weights_global)
  
  # Calculate composite scores with the new weights
  df_sim <- results_all %>%
    mutate(
      ScramblingScore = rescale(ProcrustesSS) + rescale(1 - MantelCorr),
      PrivacyScore = (
        rescale(MappingEntropy) * privacy_weights_rand["MappingEntropy"] +
        rescale(1 - abs(CommunityARI)) * privacy_weights_rand["CommunityARI"] +
        rescale(1 - abs(RankCorr_Degree)) * privacy_weights_rand["RankCorr_Degree"] +
        rescale(1 - abs(RankCorr_Closeness)) * privacy_weights_rand["RankCorr_Closeness"] +
        rescale(1 - abs(RankCorr_Betweenness)) * privacy_weights_rand["RankCorr_Betweenness"] +
        rescale(1 - abs(RankCorr_Eigenvector)) * privacy_weights_rand["RankCorr_Eigenvector"] +
        rescale(1 - MI_Degree) * privacy_weights_rand["MI_Degree"] +
        rescale(1 - MI_Closeness) * privacy_weights_rand["MI_Closeness"] +
        rescale(1 - MI_Betweenness) * privacy_weights_rand["MI_Betweenness"] +
        rescale(1 - MI_Eigenvector) * privacy_weights_rand["MI_Eigenvector"]
      )
    ) %>%
    mutate(
      ScramblingScore = rescale(ScramblingScore),
      PrivacyScore = rescale(PrivacyScore),
      GlobalScore = (ScramblingScore * weight_scrambling + PrivacyScore * weight_privacy)
    )
  
  # Calculate and store the ranks
  df_sim <- df_sim %>%
    arrange(desc(GlobalScore)) %>%
    mutate(Rank = row_number())
  
  sim_results <- rbind(sim_results, data.frame(
    Method = df_sim$Method,
    Scenario = i,
    GlobalScore = df_sim$GlobalScore,
    Rank = df_sim$Rank
  ))
}

# --- 2. HEATMAP VISUALIZATION ---
# Calculate average rank for each method
avg_ranks <- sim_results %>%
  group_by(Method) %>%
  summarise(AvgRank = mean(Rank)) %>%
  arrange(AvgRank)

# Plot the heatmap
ggplot(sim_results, aes(x = Scenario, y = reorder(Method, Rank), fill = Rank)) +
    geom_tile() +
    scale_fill_gradient(low = "darkgreen", high = "orange", name = "Global Rank") +
    theme_minimal() +
    labs(
        title = "Ranking of Obfuscation Methods Under Random Weight Scenarios",
        x = "Simulation Scenario",
        y = "Obfuscation Method"
    ) +
    theme(
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        legend.position = "right"
    )

# Count how many times each Method got each Rank
rank_counts <- sim_results %>%
  group_by(Method, Rank) %>%
  summarise(Count = n(), .groups = "drop")

# Normalize counts to percentages
rank_counts <- rank_counts %>%
  group_by(Method) %>%
  mutate(Percent = Count / sum(Count) * 100)

# Plot heatmap of percentages with labels
windows()
heat1 <-ggplot(rank_counts, aes(x = Rank, y = reorder(Method, -Rank), fill = Percent)) +
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.1f%%", Percent)), size = 3, color = "black") +
  scale_fill_gradient(low = "white", high = "darkblue", name = "% of Scenarios") +
  theme_minimal() +
  labs(
    title = "Percentage Distribution of Ranks for Each Obfuscation Method",
    x = "Rank Position",
    y = "Obfuscation Method"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )
print(heat1)

##############################################


layout_original <-layout_list$Original
layout_obf <- layout_list$Arnold
reps = 50

#############################################
# given layout_original (n x 2) and layout_obf (n x 2)
library(clue)  # for solve_LSAP

compute_recovery_rate <- function(layout_original, layout_obf, topk = NULL) {
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  n <- nrow(layout_original)
  cost <- dmat[1:n, (n+1):(2*n)]
  # assignment (Hungarian)
  assign <- solve_LSAP(cost)        # returns a permutation mapping original -> obf index
  # if you have true node IDs preserved as row order, success = identity
  success <- sum(assign == seq_len(n)) / n
  if (!is.null(topk)) {
    # topk strategy: compare top-k nodes by degree recovered
    # topk recovery placeholder — compute separately
  }
  return(as.numeric(success))
}

#recovery_rate <- compute_recovery_rate(layout_original, layout_obf)

# Remove "Original" from evaluation
methods <- setdiff(names(layout_list), "Original")

rev_results <- lapply(methods, function(m) {
  res <- compute_recovery_rate(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res

  )
})

rev_df1 <- do.call(rbind, rev_results)
print(rev_df1)

library(clue)  # for solve_LSAP

compute_recovery_rate <- function(layout_original, layout_obf, 
                                  topk = NULL, centrality = NULL, graph = NULL) {
  # layout_original, layout_obf : n x 2 coordinate matrices
  # topk : if not NULL, test recovery of top-k nodes by centrality
  # centrality : vector of centrality scores for original graph (length n)
  # graph : igraph object (only needed if you want to recompute centralities inside)
  
  n <- nrow(layout_original)
  
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  cost <- dmat[1:n, (n+1):(2*n)]
  
  # Hungarian assignment
  assign <- solve_LSAP(cost)  # permutation mapping original -> obf
  
  if (is.null(topk)) {
    # ---- Case 1: Exact ID match ----
    success <- sum(assign == seq_len(n)) / n
    return(success)
  } else {
    # ---- Case 2: Top-k recovery ----
    if (is.null(centrality)) {
      if (is.null(graph)) stop("Need either centrality vector or graph object")
      stop("Please compute centrality outside and pass as vector for reproducibility.")
    }
    
    # Identify top-k original nodes
    topk_orig <- order(centrality, decreasing = TRUE)[1:topk]
    
    # Get matched obfuscated indices
    matched_ids <- as.integer(assign)
    
    # Map original top-k to their obfuscated partners
    recovered <- matched_ids[topk_orig]
    
    # Check overlap with top-k nodes in obfuscation by same centrality
    # (if we have centrality scores for obfuscated graph)
    if (!is.null(graph)) {
      stop("Option to recompute obfuscated centrality not implemented here.")
    }
    
    # For simplicity: measure fraction of top-k original nodes correctly mapped
    success <- sum(recovered %in% topk_orig) / topk
    return(success)
  }
}

rev_results1 <- lapply(methods, function(m) {
  res <- compute_recovery_rate(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res

  )
})

rev_df11 <- do.call(rbind, rev_results1)
print(rev_df11)

library(clue)  # for solve_LSAP

compute_recovery_info <- function(layout_original, layout_obf) {
  n <- nrow(layout_original)
  
  # cost matrix = Euclidean distances
  dmat <- as.matrix(dist(rbind(layout_original, layout_obf)))
  cost <- dmat[1:n, (n+1):(2*n)]
  
  # Hungarian assignment (minimum-cost matching)
  assign <- solve_LSAP(cost)   # permutation mapping original -> obf
  
  # Identify correctly recovered nodes
  recovered_ids <- which(assign == seq_len(n))
  
  # Proportion of correctly recovered nodes
  mean_recover <- length(recovered_ids) / n
  
  list(
    MeanRecover = mean_recover,
    RecoveredNodes = recovered_ids
  )
}

methods <- setdiff(names(layout_list), "Original")

rev_results <- lapply(methods, function(m) {
  res <- compute_recovery_info(layout_list$Original, layout_list[[m]])
  data.frame(
    Method = m,
    MeanRecover = res$MeanRecover,
    RecoveredNodes = paste(res$RecoveredNodes, collapse = ",")
  )
})

rev_df <- do.call(rbind, rev_results)
print(rev_df)


##########################################
library(entropy)
library(clue)   # for Hungarian algorithm (if you want recovery checks later)

compute_reversibility <- function(layout_original, layout_obf, reps = 50) {
  n <- nrow(layout_original)
  
  # Initialize mapping counts
  mapping_counts <- matrix(0, nrow = n, ncol = n)
  
  for (r in 1:reps) {
    # Add a tiny jitter for stability (avoids ties)
    layout_obf_rep <- layout_obf + matrix(rnorm(n*2, sd = 1e-6), ncol = 2)
    
    # Pairwise distances
    dmat <- as.matrix(dist(rbind(layout_original, layout_obf_rep)))
    dmat <- dmat[1:n, (n+1):(2*n)]
    
    # Nearest neighbour mapping
    nearest <- apply(dmat, 1, which.min)
    
    for (i in 1:n) {
      mapping_counts[i, nearest[i]] <- mapping_counts[i, nearest[i]] + 1
    }
  }
  
  # Node-level entropies
  entropies <- apply(mapping_counts, 1, function(row) {
    if (sum(row) == 0) return(0)
    probs <- row / sum(row)
    entropy::entropy(probs, unit = "log2")
  })
  
  mean_entropy <- mean(entropies)
  
  # Normalized Reversibility Score [0,1]
  RS <- 1 - (mean_entropy / log2(n))
  
  list(
    mean_entropy = mean_entropy,
    entropies = entropies,
    reversibility_score = RS
  )
}


##############################33
# Simulation Loop ---
set.seed(123)
# Dataframe to store simulation results
sim_results <- data.frame(
    Method = character(),
    Scenario = numeric(),
    GlobalScore = numeric(),
        ScramblingScore = numeric(),
        PrivacyScore = numeric(),
    Rank = numeric()
)

for (i in 1:nsamples) {
    # Generate random weights for Scrambling and Privacy
    weights <- runif(2, min = 0, max = 1)
    weight_scrambling <- weights[1] / sum(weights)
    weight_privacy <- weights[2] / sum(weights)
    
    # Generate random weights for ScramblingScore components in specified ranges
    weight_procrustes <- runif(1, min = 0.01, max = 0.04)
    weight_avg_disp <- runif(1, min = 0.01, max = 0.1)
    weight_disp_entropy <- runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the scrambling weights
    scrambling_all_weights <- c(
      weight_procrustes,
      weight_avg_disp,
      weight_disp_entropy
    )
    scrambling_weights_rand <- scrambling_all_weights / sum(scrambling_all_weights)
    
    # Generate random weights for PrivacyScore components in specified ranges
    weight_mapping_entropy <- runif(1, min = 0.01, max = 0.04)
    weight_community_ari <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_degree <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_rank_corr_eigenvector <- runif(1, min = 0.01, max = 0.1)
    weight_mi_degree <- runif(1, min = 0.01, max = 0.1)
    weight_mi_closeness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_betweenness <- runif(1, min = 0.01, max = 0.1)
    weight_mi_eigenvector <- runif(1, min = 0.01, max = 0.1)
    weight_mantel_corr <- runif(1, min = 0.01, max = 0.1)
    
    # Combine and normalize the privacy weights
    privacy_all_weights <- c(
      weight_mapping_entropy,
      weight_community_ari,
      weight_rank_corr_degree,
      weight_rank_corr_closeness,
      weight_rank_corr_betweenness,
      weight_rank_corr_eigenvector,
      weight_mi_degree,
      weight_mi_closeness,
      weight_mi_betweenness,
      weight_mi_eigenvector,
      weight_mantel_corr
    )
    privacy_weights_rand <- privacy_all_weights / sum(privacy_all_weights)
    
    # Now, calculate scores with these normalized random weights
    df_sim <- results_all %>%
        mutate(
            ScramblingScore = (
                rescale(ProcrustesSS) * scrambling_weights_rand[1] +
                rescale(AvgDisplacement) * scrambling_weights_rand[2] +
                rescale(DispEntropy) * scrambling_weights_rand[3]
            ),
            PrivacyScore = (
                rescale(MappingEntropy) * privacy_weights_rand[1] +
                rescale(1 - abs(CommunityARI)) * privacy_weights_rand[2] +
                rescale(1 - abs(RankCorr_Degree)) * privacy_weights_rand[3] +
                rescale(1 - abs(RankCorr_Closeness)) * privacy_weights_rand[4] +
                rescale(1 - abs(RankCorr_Betweenness)) * privacy_weights_rand[5] +
                rescale(1 - abs(RankCorr_Eigenvector)) * privacy_weights_rand[6] +
                rescale(1 - MI_Degree) * privacy_weights_rand[7] +
                rescale(1 - MI_Closeness) * privacy_weights_rand[8] +
                rescale(1 - MI_Betweenness) * privacy_weights_rand[9] +
                rescale(1 - MI_Eigenvector) * privacy_weights_rand[10] +
                rescale(1 - MantelCorr) * privacy_weights_rand[11]
            )
        ) %>%
        mutate(
            ScramblingScore = rescale(ScramblingScore),
            PrivacyScore = rescale(PrivacyScore),
            GlobalScore = (ScramblingScore * weight_scrambling + PrivacyScore * weight_privacy)
        )
    
    # Calculate and store the ranks
    df_sim <- df_sim %>%
        arrange(desc(GlobalScore)) %>%
        mutate(Rank = row_number())
        
    sim_results <- rbind(sim_results, data.frame(
        Method = df_sim$Method,
        Scenario = i,
        GlobalScore = df_sim$GlobalScore,
        ScramblingScore = df_sim$ScramblingScore,
        PrivacyScore = df_sim$PrivacyScore,
        Rank = df_sim$Rank
    ))
}
The error message `the leading minor of order 2 is not positive` from `stat_ellipse()` is a common issue that occurs when the data you're trying to fit an ellipse to has zero variance in one of its dimensions.

In this specific case, it means that for at least one of your obfuscation methods, all of the simulated `ScramblingScore` or `PrivacyScore` values are exactly the same. This can happen with deterministic methods that produce a single, consistent result regardless of the random weights. Because there is no variability, R's statistical functions cannot calculate the necessary variance-covariance matrix to draw the ellipse.

The solution is to add a small check to the code that will only attempt to draw the ellipse for methods with enough variability in their scores. This prevents the plot from failing for the deterministic methods while still showing the ellipses for the non-deterministic ones.

Here is the corrected R script that handles this issue:

```r
# Load necessary libraries
library(ggplot2)
library(car)
library(dplyr)

# Assuming 'sim_results' is the data frame from your simulation
# Create the base plot with all points
p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Ellipses indicate the spread of scores; crosses mark the mean of each method.",
    x = "Scrambling Score",
    y = "Privacy Score"
  ) +
  theme_minimal() +
  scale_color_discrete(name = "Obfuscation Method") +
  theme(
    legend.position = "right",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add ellipses only for methods with variability
unique_methods <- unique(sim_results$Method)
for (method in unique_methods) {
  method_data <- sim_results %>% filter(Method == method)
  
  # Check for sufficient variance to draw an ellipse
  if (sd(method_data$ScramblingScore) > 1e-6 & sd(method_data$PrivacyScore) > 1e-6) {
    p <- p + stat_ellipse(data = method_data, aes(group = Method),
                          type = "norm", level = 0.95, geom = "polygon",
                          alpha = 0.1, fill = "white", color = "black")
  }
}

# Add mean points for all methods
p <- p + geom_point(data = sim_results %>% group_by(Method) %>% summarize(ScramblingScore = mean(ScramblingScore), PrivacyScore = mean(PrivacyScore)),
             aes(x = ScramblingScore, y = PrivacyScore),
             shape = 4, size = 4, stroke = 2)

# Print the final plot
print(p)

#######################
# Load necessary libraries
library(ggplot2)
library(dplyr)

# Create a summary data frame with the mean scores for each method
centroids <- sim_results %>%
  group_by(Method) %>%
  summarize(
    ScramblingScore = mean(ScramblingScore),
    PrivacyScore = mean(PrivacyScore),
    .groups = 'drop'
  )

# Create the base plot with points
p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Method names directly label the centroid of each method.",
    x = "Scrambling Score",
    y = "Privacy Score"
  ) +
  theme_minimal() +
  scale_color_discrete(guide = "none") + # Hide the default legend
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add a text label for each centroid using the Method name
p <- p + geom_text(
  data = centroids,
  aes(label = Method),
  color = "black", # Use black for readability
  size = 4,
  fontface = "bold",
  nudge_x = 0.0,
  nudge_y = -0.05
) +
  # Add the cross marker for the centroid
  geom_point(data = centroids,
             aes(shape = Method),
             size = 4, stroke = 2, color = "black") +
  # Use a separate legend for the shapes if desired (not recommended for this plot)
  scale_shape_manual(values = rep(4, nrow(centroids))) +
  guides(shape = "none")

# Print the final plot
print(p)

####
# Load necessary libraries
library(ggplot2)
library(car)
library(dplyr)

# Create a summary data frame with the mean scores for each method
centroids <- sim_results %>%
  group_by(Method) %>%
  summarize(
    ScramblingScore = mean(ScramblingScore),
    PrivacyScore = mean(PrivacyScore),
    .groups = 'drop'
  )

# Create the base plot with points and no legend
p <- ggplot(sim_results, aes(x = ScramblingScore, y = PrivacyScore, color = Method)) +
  geom_point(alpha = 0.2, size = 1.5) + # Plot individual points with transparency
  labs(
    title = "Scrambling Score vs. Privacy Score in Simulated Scenarios",
    subtitle = "Ellipses indicate the spread of scores; filled points mark the mean of each method.",
    x = "Scrambling Score",
    y = "Privacy Score"
  ) +
  theme_minimal() +
  scale_color_discrete(guide = "none") + # Hide the default legend
  theme(
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 10)
  )

# Add ellipses only for methods with variability
unique_methods <- unique(sim_results$Method)
for (method in unique_methods) {
  method_data <- sim_results %>% filter(Method == method)
  
  # Check for sufficient variance to draw an ellipse
  if (sd(method_data$ScramblingScore) > 1e-6 & sd(method_data$PrivacyScore) > 1e-6) {
    p <- p + stat_ellipse(data = method_data, aes(group = Method),
                          type = "norm", level = 0.95, geom = "polygon",
                          alpha = 0.1, fill = "white", color = "black")
  }
}

# Add text labels for each centroid using the Method name
p <- p + geom_text(
  data = centroids,
  aes(label = Method),
  color = "black", # Use black for readability
  size = 4,
  fontface = "bold",
  nudge_x = 0.0,
  nudge_y = -0.05
) +
  # Add a filled point for the centroid
  geom_point(data = centroids,
             aes(shape = Method),
             size = 2, stroke = 1.2, fill = "white", color = "black") + # Using shape=21 creates a filled circle with a black border
  # Use a separate legend for the shapes if desired (not recommended for this plot)
  scale_shape_manual(values = rep(21, nrow(centroids))) +
  guides(shape = "none")


# Print the final plot
print(p)

####
windows()
biplot_1 <-ggplot(sim_results1, aes(x = ScramblingScore, y = PrivacyScore,
               label = Method, color = Method)) +
  geom_point(size = 5) +
  geom_text(vjust = -1, size = 4) +
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey50") +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "grey50") +
  annotate("text", x = 0.75, y = 0.9, label = "Scrambled + Private", size = 5, color = "darkgreen") +
  annotate("text", x = 0.75, y = 0.1, label = "Scrambled but Not Private", size = 5, color = "red") +
  annotate("text", x = 0.25, y = 0.9, label = "Private but Low Scrambling", size = 5, color = "blue") +
  annotate("text", x = 0.25, y = 0.1, label = "Neither Scrambled nor Private", size = 5, color = "black") +
  xlab("Visual Scrambling (Aesthetic Anonymization)") +
  ylab("Privacy Protection (True Anonymization)") +
  ggtitle("Trade-off: Visual Scrambling vs Privacy Protection") +
  theme_minimal(base_size = 14) +
  theme(legend.position = "none")

print(biplot_1)
